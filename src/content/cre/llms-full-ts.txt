# Chainlink Runtime Environment (CRE)
Source: https://docs.chain.link/cre
Last Updated: 2025-11-04

## What is CRE?

**Chainlink Runtime Environment (CRE)** is the all-in-one orchestration layer unlocking institutional-grade smart contracts—data-connected, compliance-ready, privacy-preserving, and interoperable across blockchains and existing systems.

Using the **CRE SDK** (available in Go and TypeScript), you build **Workflows**. Using the CRE CLI, you compile them into binaries and deploy them to production, where CRE runs them across a Decentralized Oracle Network (DON).

- Each workflow is orchestrated by a **Workflow DON (Decentralized Oracle Network)** that monitors for triggers and coordinates execution.
- The workflow can then invoke specialized **Capability DONs**—for example, one that fetches offchain data or one that writes to a chain.
- During execution, each node in a DON performs the requested task independently.
  - Their results are then cryptographically verified and aggregated via a Byzantine Fault Tolerant (BFT) consensus protocol. This guarantees a single, correct, and consistent outcome.

## What you can do today

### Build and simulate (available now)

You can start building and [simulating](/cre/guides/operations/simulating-workflows) CRE workflows immediately, without any approval:

- **Create an account** at [cre.chain.link](https://cre.chain.link) to access the platform
- **Install the CRE CLI** on your machine
- **Build workflows** using the Go or TypeScript SDKs
- **Simulate workflows** to test and debug before deployment

Simulation compiles your workflows into [WebAssembly (WASM)](https://webassembly.org/) and runs them on your machine—but makes **real calls** to live APIs and public EVM blockchains. This gives you confidence your workflow will work as expected when deployed to a DON.

### Deploy your workflows (Early Access)


<Aside type="caution" title="Early Access: Disclaimer">
  Chainlink Runtime Environment (CRE) deployment is in the "Early Access" stage of development, which means that CRE currently has functionality which is under development and may be changed in later versions. By using CRE, you expressly acknowledge and agree to accept the Chainlink <a href="https://chain.link/terms" target="_blank" rel="noopener noreferrer">Terms of Service</a>, which provides important information and disclosures.
</Aside>

Early Access to workflow deployment includes:

- **Deploy and run workflows** on a Chainlink DON
- **Workflow lifecycle management**: Deploy, activate, pause, update, and delete workflows through the CLI
- **Monitoring and debugging**: Access detailed logs, events, and performance metrics in the CRE UI

To <a href="https://cre.chain.link/request-access" target="_blank">request Early Access</a>, please share details about your project and use case—this helps us provide better support as you build with CRE.

## How CRE runs your workflows

Now that you understand what CRE is, let's explore how it executes your workflows.

### The trigger-and-callback model

Workflows use a **trigger-and-callback model** to provide a code-first developer experience. This model is the primary architectural pattern you will use in your workflows. It consists of three simple parts:

1. **A Trigger**: An event source that starts a workflow execution (e.g., `cron.Trigger`). This is the "when" of your workflow.
2. **A Callback**: A function that contains your business logic. It is inside this function that you will use the SDK's clients to invoke capabilities. This is the "what" of your workflow.
3. **The `cre.handler()`**: The glue that connects a single trigger to a single callback.

You can define multiple trigger and callback combinations in your workflow. You can also attach the same callback to multiple triggers for reusability.

Here's what the trigger-and-callback pattern looks like:

```ts
cre.handler(
  cronTrigger.trigger({ schedule: "0 */10 * * * *" }), // trigger fires every 10 minutes
  onCronTrigger // your callback function
)

function onCronTrigger(runtime: Runtime<Config>): Record<string, never> {
  // Create SDK clients and call capabilities
  return {}
}
```


<Aside type="note" title="Each trigger fire = independent execution">
  Each time a trigger fires, it starts a **fresh, independent execution** of your callback function. Callbacks are **stateless**—there is no persistent state between executions. The value you return from a callback represents the result of that specific execution, not a stored workflow state.
</Aside>

### Execution lifecycle

When a trigger fires, the Workflow DON orchestrates the execution of your callback function on every node in the network. **Each execution is independent and stateless**—your callback runs, performs its work, returns a result, and completes. Inside your callback, you create SDK clients and invoke capabilities.

Each capability call is an **asynchronous operation** that returns a `Promise`—a placeholder for a future result. This allows you to pipeline multiple capability calls and run them in parallel.

Your callback typically follows this pattern:

1. Invoke multiple capabilities in parallel (each returns a `Promise` immediately)
2. Await the consensus-verified results
3. Use the trusted results in your business logic
4. Optionally perform final actions like writing back to a blockchain

For every capability you invoke, CRE handles the underlying process of having a dedicated DON execute the task, reach consensus, and return the verified result.

### Built-in consensus for every operation

One of CRE's most powerful features is that **every capability execution automatically includes consensus**. When your workflow invokes a capability (like fetching data from an API or reading from a blockchain), multiple independent nodes perform the operation. Their results are then validated and aggregated through a Byzantine Fault Tolerant (BFT) consensus protocol, ensuring a single, verified outcome.

This means your entire workflow—not just the onchain parts—benefits from the same security and reliability guarantees as blockchain transactions. Unlike traditional applications that rely on a single API provider or RPC endpoint, CRE eliminates single points of failure by having multiple nodes independently verify every operation.

Learn more about [Consensus Computing in CRE](/cre/concepts/consensus-computing).

## Glossary: Building blocks

| Concept            | One-liner                                                         |
| ------------------ | ----------------------------------------------------------------- |
| **Workflow**       | Compiled WebAssembly (WASM) binary.                               |
| **Handler**        | `cre.handler(trigger, callback)` pair; the atom of execution.     |
| **Trigger**        | Event that starts an execution (cron, HTTP, EVM log, …).          |
| **Callback**       | Function that runs when its trigger fires; contains your logic.   |
| **Runtime**        | Object passed to a callback; used to invoke capabilities.         |
| **Capability**     | Decentralized microservice (chain read/write, HTTP Fetch, ...).   |
| **Workflow DON**   | Watches triggers and coordinates the workflow.                    |
| **Capability DON** | Executes a specific capability.                                   |
| **Consensus**      | BFT protocol that merges node results into one verifiable report. |

Full definitions live on **[Key Terms and Concepts](/cre/key-terms)**.

## Why build on CRE?

- **Unified cross-domain orchestration**: Seamlessly combine onchain and offchain operations in a single workflow. Read from multiple blockchains, call authenticated APIs, perform computations, and write results back onchain or offchain—all orchestrated by CRE.

- **Institutional-grade security by default**: Every operation—API calls, blockchain reads, computations—runs across multiple independent nodes with Byzantine Fault Tolerant consensus. Your workflows inherit the same security guarantees as blockchain transactions.

- **One platform, any chain**: Build your logic once and connect to any supported blockchain. No need to deploy separate infrastructure for each chain you support.

- **Code-first developer experience**: Write workflows in Go or TypeScript using familiar patterns. The SDK abstracts away the complexity of distributed systems, letting you focus on your business logic.

## Where to go next?

### New to CRE?

Start here:

1. **[Create Your Account](/cre/account/creating-account)** - Set up your CRE account (required for all CLI commands)
2. **[Install the CLI](/cre/getting-started/cli-installation)** - Download and install the `cre` command-line tool

Then choose your path:

- **Learn by building:** [Getting Started Guide](/cre/getting-started/overview) - Step-by-step guide where you build your first workflow, learning core concepts along the way
- **Quick start:** [Run the Custom Data Feed Demo](/cre/templates/running-demo-workflow) - See a production-ready workflow in action. Just follow the steps to run a complete, pre-built example

### Already familiar?

Jump to what you need:

- **[Workflow Guides](/cre/guides/workflow/using-triggers/overview)** - Learn how to use triggers, make API calls, and interact with blockchains
- **[Workflow Operations](/cre/guides/operations/simulating-workflows)** - Simulate, deploy, and manage your workflows
- **[SDK Reference](/cre/reference/sdk)** - Detailed API documentation for Go and TypeScript SDKs

---

# Key Terms and Concepts
Source: https://docs.chain.link/cre/key-terms
Last Updated: 2025-11-04

This page defines the fundamental terms and concepts for the Chainlink Runtime Environment (CRE).

## High-level concepts

### Chainlink Runtime Environment (CRE)

The all-in-one orchestration layer unlocking institutional-grade smart contracts—data-connected, compliance-ready, privacy-preserving, and interoperable across blockchains and existing systems

### Decentralized Oracle Network (DON)

A decentralized, peer-to-peer network of independent nodes that work together to execute a specific task. In CRE, there are two primary types of DONs: **Workflow DONs** that orchestrates the workflow, and specialized **Capability DONs** that execute specific tasks like blockchain interactions.

## Workflow architecture

### Workflow

A workflow uses the CRE SDK (Go or TypeScript) and comprises one or more [handlers](/cre/key-terms#handler), which define the logic that executes when events ([triggers](/cre/key-terms#trigger)) occur. CRE compiles the workflow to a WASM binary and runs it on a Workflow DON.

### Handler

The basic building block of a workflow, created using the `cre.Handler` function. It connects a single **Trigger** event to a single **Callback** function.

### Trigger

An event source that initiates the execution of a handler's callback function. Examples include Cron trigger, HTTP trigger, and EVM Log trigger. Learn more in the [Trigger capability page](/cre/capabilities/triggers).

### Callback

A function that contains your core logic. It is executed by the Workflow DON every time its corresponding trigger fires.

## The developer's toolkit: The CRE SDK

### `Runtime` & `NodeRuntime`

Short-lived objects passed to your callback function during a specific execution. The key difference between `Runtime` and `NodeRuntime` is who is responsible for creating a single, trusted result from the work of many nodes.

- **`Runtime`**: Think of it as the "Easy Mode". It is used for operations that are guaranteed to be Byzantine Fault Tolerant (BFT). You ask the network to execute something, and CRE handles the underlying complexity to ensure you get back one final, secure, and trustworthy result.

- **`NodeRuntime`**: Think of this as the "Manual Mode". It is used when a BFT guarantee cannot be provided automatically (e.g. calling a standard API). You tell each node to perform a task on its own. Each node returns its own individual answer, and you are responsible for telling the SDK how to combine them into a single, trusted result by providing an aggregation algorithm. This is always used inside a `cre.RunInNodeMode` block.

Learn more about [Consensus and Aggregation](/cre/reference/sdk/consensus).

### SDK Clients: `EVMClient` & `HTTPClient`

The primary SDK clients you use inside a callback to interact with capabilities. For example, you use an EVM client to read from a smart contract and an HTTP client to make offchain API requests.

**Language-specific implementations:**

- **Go SDK**: `evm.Client` and `http.Client`
- **TypeScript SDK**: `EVMClient` and `HTTPClient` classes

### `Bindings` (Go SDK only)

A Go package generated from a smart contract's ABI using the `cre generate-bindings` CLI command. Bindings create a type-safe Go interface for a specific smart contract, abstracting away the low-level complexity of ABI encoding and decoding.

Using generated bindings is the recommended best practice for Go workflows, as they provide helper methods for:

- Reading from `view`/`pure` functions.
- Encoding data structures for onchain writes.
- Creating triggers for and decoding event logs.

This makes your workflow code cleaner, safer, and easier to maintain. Learn more in the [Generating Contract Bindings](/cre/guides/workflow/using-evm-client/generating-bindings) guide.

**Note for TypeScript**: The TypeScript SDK uses [Viem](https://viem.sh/) for type-safe contract interactions with manual ABI definitions instead of generated bindings.

### Async Patterns

Asynchronous operations in the SDK (like contract reads or HTTP requests) return a placeholder for a future result:

- **Go SDK**: Operations return a `Promise`, and you must call `.Await()` to pause execution and wait for the result.
- **TypeScript SDK**: Operations return an object with a `.result()` method that you call to wait for the result.

### `Secrets`

Securely managed credentials (e.g., API keys) made available to your workflow at runtime. Secrets can be fetched within a callback using the runtime's secret retrieval method:

- **Go SDK**: `runtime.GetSecret()`
- **TypeScript SDK**: `runtime.getSecret()`

## Underlying architectural concepts

### Capability

A conceptual, decentralized "microservice" that is backed by its own DON. Capabilities are the fundamental building blocks of the CRE platform (e.g., HTTP Fetch, EVM Read). You do not interact with them directly; instead, you use the SDK's developer-facing clients (like `evm.Client`) to invoke them.

### Consensus

The mechanism by which a DON comes to a single, reliable, and tamper-proof result, even if individual nodes observe slightly different data. Consensus is what makes the outputs of capabilities secure and trustworthy.

## Where to go next?

- **[Getting Started](/cre/getting-started/overview)**: Start building your first workflow.
- **[About CRE](/cre)**: Learn more about the vision and high-level architecture of CRE.

---

# Service Quotas
Source: https://docs.chain.link/cre/service-quotas
Last Updated: 2025-12-18

This page documents the service quotas for Chainlink Runtime Environment (CRE) workflows.

<Aside type="note" title="Subject to change">
  The quotas documented on this page are subject to change. Check back regularly for updates.
</Aside>

## Per-owner quotas

These quotas apply to each workflow owner (user account) within an organization.

| Quota Key                                                                                                                                              | Description                                                 | Value |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------- | ----- |
| <a href="#perowner-workflowexecutionconcurrencylimit" id="perowner-workflowexecutionconcurrencylimit">`PerOwner.WorkflowExecutionConcurrencyLimit`</a> | Maximum number of workflows that can execute simultaneously | 5     |
| <a href="#perowner-vaultsecretslimit" id="perowner-vaultsecretslimit">`PerOwner.VaultSecretsLimit`</a>                                                 | Maximum number of secrets that can be stored per owner      | 100   |

<Aside type="note" title="Quota enforcement">
  When workflow executions exceed the quotas defined above, they are queued and automatically retried for up to 10
  minutes. If an execution cannot be completed within this 10-minute window, it will be dropped and will not run.
</Aside>

## Per-workflow quotas

These quotas apply to each individual workflow.

### Workflow deployment quotas

| Quota Key                                                                                                                                           | Description                                | Value  |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ | ------ |
| <a href="#perworkflow-wasmbinarysizelimit" id="perworkflow-wasmbinarysizelimit">`PerWorkflow.WASMBinarySizeLimit`</a>                               | Maximum size of the compiled WASM binary   | 100 MB |
| <a href="#perworkflow-wasmcompressedbinarysizelimit" id="perworkflow-wasmcompressedbinarysizelimit">`PerWorkflow.WASMCompressedBinarySizeLimit`</a> | Maximum size of the compressed WASM binary | 20 MB  |
| <a href="#perworkflow-wasmconfigsizelimit" id="perworkflow-wasmconfigsizelimit">`PerWorkflow.WASMConfigSizeLimit`</a>                               | Maximum size of the workflow configuration | 1 MB   |

### Trigger quotas

| Quota Key                                                                                                                            | Description                                                            | Value |
| ------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------- | ----- |
| <a href="#perworkflow-triggersubscriptionlimit" id="perworkflow-triggersubscriptionlimit">`PerWorkflow.TriggerSubscriptionLimit`</a> | Maximum number of triggers that can be registered to a single workflow | 10    |

### Execution quotas

| Quota Key                                                                                                                               | Description                                                     | Value     |
| --------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------- | --------- |
| <a href="#perworkflow-executionconcurrencylimit" id="perworkflow-executionconcurrencylimit">`PerWorkflow.ExecutionConcurrencyLimit`</a> | Maximum number of concurrent executions for a specific workflow | 5         |
| <a href="#perworkflow-executiontimeout" id="perworkflow-executiontimeout">`PerWorkflow.ExecutionTimeout`</a>                            | Maximum total execution time for a single workflow run          | 5 minutes |
| <a href="#perworkflow-wasmmemorylimit" id="perworkflow-wasmmemorylimit">`PerWorkflow.WASMMemoryLimit`</a>                               | Maximum memory allocated to a workflow                          | 100 MB    |
| <a href="#perworkflow-executionresponselimit" id="perworkflow-executionresponselimit">`PerWorkflow.ExecutionResponseLimit`</a>          | Maximum size of the data a workflow can return                  | 100 KB    |

### General capability quotas

| Quota Key                                                                                                                                  | Description                                                                                            | Value     |
| ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ | --------- |
| <a href="#perworkflow-capabilityconcurrencylimit" id="perworkflow-capabilityconcurrencylimit">`PerWorkflow.CapabilityConcurrencyLimit`</a> | Maximum concurrent capability calls (HTTP, EVM read/write, secrets) that can execute within a workflow | 3         |
| <a href="#perworkflow-capabilitycalltimeout" id="perworkflow-capabilitycalltimeout">`PerWorkflow.CapabilityCallTimeout`</a>                | Maximum time a single capability call can take to complete                                             | 3 minutes |

### Secrets quotas

| Quota Key                                                                                                                         | Description                                                                                                                      | Value |
| --------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- | ----- |
| <a href="#perworkflow-wasmsecreetssizelimit" id="perworkflow-wasmsecreetssizelimit">`PerWorkflow.WASMSecretsSizeLimit`</a>        | Maximum total size of secrets accessible to a workflow                                                                           | 1 MB  |
| <a href="#perworkflow-secretsconcurrencylimit" id="perworkflow-secretsconcurrencylimit">`PerWorkflow.SecretsConcurrencyLimit`</a> | Maximum number of secrets that can be fetched concurrently. [Learn how to fetch multiple secrets](/cre/guides/workflow/secrets). | 5     |

### Consensus quotas

| Quota Key                                                                                                                                              | Description                                                      | Value |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------- | ----- |
| <a href="#perworkflow-consensus-observationsizelimit" id="perworkflow-consensus-observationsizelimit">`PerWorkflow.Consensus.ObservationSizeLimit`</a> | Maximum size of data that can be passed to consensus aggregation | 25 KB |
| <a href="#perworkflow-consensus-calllimit" id="perworkflow-consensus-calllimit">`PerWorkflow.Consensus.CallLimit`</a>                                  | Maximum number of consensus calls per workflow execution         | 2,000 |

### Logging quotas

| Quota Key                                                                                           | Description                                         | Value |
| --------------------------------------------------------------------------------------------------- | --------------------------------------------------- | ----- |
| <a href="#perworkflow-loglinelimit" id="perworkflow-loglinelimit">`PerWorkflow.LogLineLimit`</a>    | Maximum size of a single log line                   | 1 KB  |
| <a href="#perworkflow-logeventlimit" id="perworkflow-logeventlimit">`PerWorkflow.LogEventLimit`</a> | Maximum number of log events per workflow execution | 1,000 |

## Trigger-specific quotas

### Cron trigger

| Quota Key                                                                                                                                                             | Description                                 | Value      |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- | ---------- |
| <a href="#perworkflow-crontrigger-fastestscheduleinterval" id="perworkflow-crontrigger-fastestscheduleinterval">`PerWorkflow.CRONTrigger.FastestScheduleInterval`</a> | Minimum interval between cron trigger fires | 30 seconds |

<Aside type="note" title="Minimum cron schedule">
  While the rate quota allows firing once per 30 seconds, you should consider the [Concurrent Workflow
  Instances](#execution-quotas) quota when setting your cron schedule to avoid overlapping executions.
</Aside>

### HTTP trigger

| Quota Key                                                                                                                   | Description                                    | Value                                  |
| --------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------- | -------------------------------------- |
| <a href="#perworkflow-httptrigger-ratelimit" id="perworkflow-httptrigger-ratelimit">`PerWorkflow.HTTPTrigger.RateLimit`</a> | Maximum rate at which an HTTP trigger can fire | Rate: 1 per 30 seconds <br /> Burst: 3 |

### EVM log trigger

| Quota Key                                                                                                                                                             | Description                                                                                                                                                                                                                  | Value                                   |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| <a href="#perworkflow-logtrigger-eventratelimit" id="perworkflow-logtrigger-eventratelimit">`PerWorkflow.LogTrigger.EventRateLimit`</a>                               | Maximum rate at which log events can be processed                                                                                                                                                                            | Rate: 10 per 6 seconds <br /> Burst: 10 |
| <a href="#perworkflow-logtrigger-filteraddresslimit" id="perworkflow-logtrigger-filteraddresslimit">`PerWorkflow.LogTrigger.FilterAddressLimit`</a>                   | Maximum number of contract addresses that can be monitored                                                                                                                                                                   | 5                                       |
| <a href="#perworkflow-logtrigger-filtertopicsperslotlimit" id="perworkflow-logtrigger-filtertopicsperslotlimit">`PerWorkflow.LogTrigger.FilterTopicsPerSlotLimit`</a> | Maximum number of topic values that can be specified within a single topic position (Topics[0], Topics[1], Topics[2], or Topics[3]). [Learn about topic filtering](/cre/guides/workflow/using-triggers/evm-log-trigger). | 10                                      |
| <a href="#perworkflow-logtrigger-eventsizelimit" id="perworkflow-logtrigger-eventsizelimit">`PerWorkflow.LogTrigger.EventSizeLimit`</a>                               | Maximum size of a single log event                                                                                                                                                                                           | 5 KB                                    |

## Capability-specific quotas

### EVM write capability

| Quota Key                                                                                                                                                          | Description                                               | Value     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------- | --------- |
| <a href="#perworkflow-chainwrite-targetslimit" id="perworkflow-chainwrite-targetslimit">`PerWorkflow.ChainWrite.TargetsLimit`</a>                                  | Maximum number of destination chains for write operations | 10        |
| <a href="#perworkflow-chainwrite-reportsizelimit" id="perworkflow-chainwrite-reportsizelimit">`PerWorkflow.ChainWrite.ReportSizeLimit`</a>                         | Maximum size of a report payload                          | 5 KB      |
| <a href="#perworkflow-chainwrite-evm-transactiongaslimit" id="perworkflow-chainwrite-evm-transactiongaslimit">`PerWorkflow.ChainWrite.EVM.TransactionGasLimit`</a> | Gas quota per EVM transaction                             | 5,000,000 |

### EVM read capability

| Quota Key                                                                                                                                        | Description                                                      | Value |
| ------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------- | ----- |
| <a href="#perworkflow-chainread-calllimit" id="perworkflow-chainread-calllimit">`PerWorkflow.ChainRead.CallLimit`</a>                            | Maximum number of EVM read calls per workflow execution          | 10    |
| <a href="#perworkflow-chainread-logqueryblocklimit" id="perworkflow-chainread-logqueryblocklimit">`PerWorkflow.ChainRead.LogQueryBlockLimit`</a> | Maximum number of blocks that can be queried for historical logs | 100   |
| <a href="#perworkflow-chainread-payloadsizelimit" id="perworkflow-chainread-payloadsizelimit">`PerWorkflow.ChainRead.PayloadSizeLimit`</a>       | Maximum size of an EVM read request payload                      | 5 KB  |

### HTTP capability

| Quota Key                                                                                                                                        | Description                                            | Value      |
| ------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------ | ---------- |
| <a href="#perworkflow-httpaction-calllimit" id="perworkflow-httpaction-calllimit">`PerWorkflow.HTTPAction.CallLimit`</a>                         | Maximum number of HTTP requests per workflow execution | 5          |
| <a href="#perworkflow-httpaction-responsesizelimit" id="perworkflow-httpaction-responsesizelimit">`PerWorkflow.HTTPAction.ResponseSizeLimit`</a> | Maximum size of an HTTP response                       | 100 KB     |
| <a href="#perworkflow-httpaction-connectiontimeout" id="perworkflow-httpaction-connectiontimeout">`PerWorkflow.HTTPAction.ConnectionTimeout`</a> | Maximum time to establish an HTTP connection           | 10 seconds |
| <a href="#perworkflow-httpaction-requestsizelimit" id="perworkflow-httpaction-requestsizelimit">`PerWorkflow.HTTPAction.RequestSizeLimit`</a>    | Maximum size of an HTTP request payload                | 10 KB      |
| <a href="#perworkflow-httpaction-cacheagelimit" id="perworkflow-httpaction-cacheagelimit">`PerWorkflow.HTTPAction.CacheAgeLimit`</a>             | Maximum time HTTP responses can be cached              | 10 minutes |

## Quota increases

[Contact us](/cre/support-feedback) to discuss quota increases.

---

# Support & Feedback
Source: https://docs.chain.link/cre/support-feedback
Last Updated: 2025-11-04

Need help with CRE? Have feedback, want to report a bug, or request a feature? You can submit a support request directly through the CRE UI.

<Aside type="note" title="CRE Account Required">
  You must have a CRE account to submit support requests. If you don't have an account yet, see [Creating Your
  Account](/cre/account/creating-account).
</Aside>

## How to submit a support request

1. Go to <a href="https://cre.chain.link" target="_blank" rel="noopener noreferrer">cre.chain.link</a>
2. Log in to your account (if you're not already logged in)
3. In the left sidebar, click **Help**
4. The support form will open as a slide-out panel
5. Select your request type from the dropdown:
   - **Support Request** - Need help with an issue
   - **Bug Report** - Found a problem
   - **Feature Request** - Suggest an improvement
   - **General Feedback** - Share your thoughts
   - **Other** - Anything else
6. Describe your issue or feedback
7. Click **Request** to submit

## What to include in your request

To help us assist you faster, please include:

**For bug reports:**

- Steps to reproduce the issue
- Expected behavior vs. actual behavior
- CRE CLI version (run `cre version` to check)
- Error messages or logs (if applicable)
- Operating system

**For support requests:**

- What you're trying to accomplish
- What's not working or unclear
- Any error messages you're seeing
- Relevant code snippets or configuration files

**For feature requests:**

- Your use case
- Why this feature would help you

---

# Release Notes
Source: https://docs.chain.link/cre/release-notes
Last Updated: 2025-11-20

This page provides detailed release notes for CRE. It includes information on new features, significant changes, and known limitations.

## CLI v1.0.2 - November 20, 2025

**<a href="https://github.com/smartcontractkit/cre-cli/releases/tag/v1.0.2" target="_blank">CRE CLI version 1.0.2</a> is now available.** This release includes various improvements based on user feedback.

### How to update

- **Automatic update**: When you run any CRE command, the CLI will automatically detect if a newer version is available and prompt you to update. Simply run `cre update` to install the latest version.
- **Fresh installation**: If you're installing the CLI for the first time, follow the [CLI Installation guide](/cre/getting-started/cli-installation).

## Release - November 4, 2025

**Chainlink Runtime Environment (CRE) is now live.** CRE is the all-in-one orchestration layer unlocking institutional-grade smart contracts—data-connected, compliance-ready, privacy-preserving, and interoperable across blockchains and existing systems.

### Available now

- **Build and simulate workflows**: Create an account at [cre.chain.link](https://cre.chain.link), install the CRE CLI, and start building workflows using the Go or TypeScript SDKs. Simulate your workflows locally to test and debug before deployment—simulation makes real calls to live APIs and public EVM blockchains.

### Early Access

- **Deploy and run workflows**: Deploy your workflows to a Chainlink DON with full lifecycle management (deploy, activate, pause, update, delete). Monitor and debug with detailed logs, events, and performance metrics in the CRE UI. To request Early Access, <a href="https://cre.chain.link/request-access" target="_blank">share details about your project</a>.

---

# Getting Started: Overview
Source: https://docs.chain.link/cre/getting-started/overview
Last Updated: 2025-11-04

<Aside type="note" title="First time here?">
  Before you begin, we strongly recommend reading the [**About CRE**](/cre/) page to understand the fundamental concepts
  of the platform, such as the trigger-and-callback model.
</Aside>

This multi-part tutorial guides you through building a complete workflow from a blank slate.

### What you'll build

You will build a simple but powerful **"Onchain Calculator"** workflow. By the end of this tutorial, your workflow will:

1. Run on a schedule using the **Cron Trigger**.
2. Fetch a random number from a public API using the **HTTP Capability**.
3. Read a value from a smart contract using the **EVM Read Capability**.
4. Combine the two values and write the final result back to the blockchain using the **EVM Write Capability**.

This tutorial is designed to teach you the core features of the CRE SDK in a logical progression. You can use any of the supported languages, Go or TypeScript using the language selector. By the end, you'll have a solid understanding of the end-to-end development process for building and simulating workflows that interact with both offchain and onchain data sources.


<Aside type="caution" title="Educational Example Disclaimer">
  This page includes an educational example to use a Chainlink system, product, or service and is provided to
  demonstrate how to interact with Chainlink's systems, products, and services to integrate them into your own. This
  template is provided "AS IS" and "AS AVAILABLE" without warranties of any kind, it has not been audited, and it may be
  missing key checks or error handling to make the usage of the system, product or service more clear. Do not use the
  code in this example in a production environment without completing your own audits and application of best practices.
  Neither Chainlink Labs, the Chainlink Foundation, nor Chainlink node operators are responsible for unintended outputs
  that are generated due to errors in code.
</Aside>

### Where to go next?

- **[Installing the CLI](/cre/getting-started/cli-installation)**: Download and install the `cre` command-line tool.

#### Tutorial structure

- **[Part 1: Project Setup & Simulation](/cre/getting-started/part-1-project-setup)**: Initialize a new, blank CRE project and run your first "Hello World!" simulation.

- **[Part 2: Fetching Offchain Data](/cre/getting-started/part-2-fetching-data)**: Modify your workflow to fetch data from an external API using the `http.Client`.

- **[Part 3: Reading an Onchain Value](/cre/getting-started/part-3-reading-onchain-value)**: Generate contract bindings and use the `evm.Client` to read a value from the blockchain.

- **[Part 4: Writing Onchain](/cre/getting-started/part-4-writing-onchain)**: Complete the calculator by writing your computed result back to a smart contract on Sepolia.

- **[Conclusion](/cre/getting-started/conclusion)**: Review what you've learned and find resources to continue your journey.

---

# Installing the CRE CLI
Source: https://docs.chain.link/cre/getting-started/cli-installation
Last Updated: 2025-11-04

These guides explain how to install the Chainlink Developer Platform CLI (also referred to as the CRE CLI).

---

# Installing the CRE CLI on macOS and Linux
Source: https://docs.chain.link/cre/getting-started/cli-installation/macos-linux
Last Updated: 2025-11-20

This page explains how to install the CRE CLI on macOS or Linux. The recommended version at the time of writing is **v1.0.2**.

## Installation

Choose your installation method:

- **[Automatic installation](#automatic-installation)** - Quick setup with a single command
- **[Manual installation](#manual-installation)** - Download and install the binary yourself

### Automatic installation

The easiest way to install the CRE CLI is using the installation script:

```bash
curl -sSL https://cre.chain.link/install.sh | bash
```

This script will:

- Detect your operating system and architecture automatically
- Download the correct binary for your system
- Verify the binary's integrity
- Install it to `$HOME/.cre`
- Make the binary executable

After the script completes, verify the installation:

```bash
cre version
```

**Expected output:** `cre version v1.0.2`

<Aside type="note" title="macOS Gatekeeper">
  If you see warnings about "unrecognized developer/source" on macOS, run:{" "}
</Aside>

### Manual installation

If you prefer to install manually or the automatic installation doesn't work for your environment, follow these steps:

The CRE CLI is publicly available on GitHub. Visit the releases page and download the appropriate binary archive for your operating system and architecture.

<Aside title="Which file should I download?">
  The file you need depends on your operating system and CPU architecture:

  - On **macOS** (darwin), run <CopyText text="uname -m" code />:
    - `arm64` (Apple Silicon) → Download `cre_darwin_arm64.zip`
    - `x86_64` (Intel) → Download `cre_darwin_amd64.zip`
  - On **Linux**, run <CopyText text="uname -m" code />:
    - `aarch64` (ARM) → Download `cre_linux_arm64.tar.gz`
    - `x86_64` (AMD/Intel) → Download `cre_linux_amd64.tar.gz`
</Aside>

After downloading the correct file from the releases page, move on to the next step to verify its integrity.

#### 1. Verify file integrity

Before installing, verify the file integrity using a checksum to ensure the binary hasn't been tampered with:

**Check the SHA-256 checksum**

Run the following command in the directory where you downloaded the archive (replace the filename with your specific binary):

```bash
shasum -a 256 cre_darwin_arm64.zip
```

**Verify against official checksums**

Compare the output with the official checksum below:

| File                     | SHA-256 Checksum                                                 |
| ------------------------ | ---------------------------------------------------------------- |
| `cre_darwin_amd64.zip`   | 482e53d3a5f8471034d30c935196d2dca2ab09a5fe1ab2083ad336172565291b |
| `cre_darwin_arm64.zip`   | 92b0409801dd4e44f90a85331615f3e4b8cf1fe9f90a8eab3ad8bb3458b4b6cf |
| `cre_linux_amd64.tar.gz` | d3a8b9b999b4b8bf73b2235d27386acf4efbc8f05d86d9e6066abda23ee9ce9b |
| `cre_linux_arm64.tar.gz` | f52d618727ccc8fb6ab4b1f9418b09424c8505428131a49c01bb33ca2bc86fe3 |

If the checksum doesn't match, do not proceed with installation. Contact your Chainlink point of contact for assistance.

#### 2. Extract and install

1. **Navigate** to the directory where you downloaded the archive.

2. **Extract the archive**

   For `.tar.gz` files:

   ```bash
   tar -xzf cre_linux_arm64.tar.gz
   ```

   For `.zip` files:

   ```bash
   unzip cre_darwin_arm64.zip
   ```

3. **Rename the extracted binary to `cre`**

   ```bash
   mv cre_v1.0.2_darwin_arm64 cre
   ```

4. **Make it executable**:
   ```bash
   chmod +x cre
   ```
   **Note (macOS Gatekeeper)**: If you see warnings about "unrecognized developer/source", remove extended attributes:
   ```bash
   xattr -c cre
   ```

#### 3. Add the CLI to your PATH

Now that you have the `cre` binary, you need to make it accessible from anywhere on your system. This means you can run `cre` commands from any directory, not just where the binary is located.

**Recommended approach: Move to a standard location**

The easiest and most reliable method is to move the `cre` binary to a directory that's already in your system's PATH. For example:

```bash
sudo mv cre /usr/local/bin/
```

This command moves the `cre` binary to `/usr/local/bin/`, which is typically included in your PATH by default.

**Alternative: Add current directory to PATH**

If you prefer to keep the binary in its current location, you can add that directory to your PATH:

1. **Find your current directory:**

   ```bash
   pwd
   ```

   Note the full path (e.g., `/Users/yourname/Downloads/cre`)

2. **Add to your shell profile** (choose based on your shell):

   For **zsh** (default on newer macOS):

   ```bash
   echo 'export PATH="/Users/yourname/Downloads/cre:$PATH"' >> ~/.zshrc
   source ~/.zshrc
   ```

   For **bash**:

   ```bash
   echo 'export PATH="/Users/yourname/Downloads/cre:$PATH"' >> ~/.bash_profile
   source ~/.bash_profile
   ```

   Replace `/Users/yourname/Downloads/cre` with your actual directory path from step 1.

3. **For temporary access** (this session only):
   ```bash
   export PATH="$(pwd):$PATH"
   ```

#### 4. Verify the installation

**Test that `cre` is accessible:**

Open a **new terminal window** and run:

```bash
cre version
```

**Expected output:**

You should see version information: `cre version v1.0.2`.

**If it doesn't work:**

- Make sure you opened a **new terminal window** after making PATH changes
- Check the binary location: `which cre` should return the path to your installation
- Check that the binary has execute permissions: `ls -la $(which cre)`
- Verify your PATH includes the correct directory: `echo $PATH`

#### 5. Confirm your PATH (troubleshooting)

If you're having issues, check what directories are in your PATH:

```bash
echo "$PATH" | tr ':' '\n'
```

You should see either:

- `/usr/local/bin` (if you moved the binary there)
- Your custom directory (if you added it to PATH)

## Next steps

Now that you have the `cre` CLI installed, you'll need to create a CRE account and authenticate before you can use it.

- **[Creating Your Account](/cre/account/creating-account)**: Create your CRE account and set up two-factor authentication
- **[Logging in with the CLI](/cre/account/cli-login)**: Authenticate the CLI with your account

Once you're authenticated, you're ready to build your first workflow:

- **[Getting Started — Part 1: Project Setup & Simulation](/cre/getting-started/part-1-project-setup)**: Initialize a new, blank CRE project and run your first "Hello World!" simulation.

---

# Installing the CRE CLI on Windows
Source: https://docs.chain.link/cre/getting-started/cli-installation/windows
Last Updated: 2025-11-20

This page explains how to install the Chainlink Developer Platform CLI (also referred to as the CRE CLI) on Windows. The recommended version at the time of writing is **v1.0.2**.

## Installation

Choose your installation method:

- **[Automatic installation](#automatic-installation)** - Quick setup with a PowerShell script
- **[Manual installation](#manual-installation)** - Download and install the binary yourself

### Automatic installation

The easiest way to install the CRE CLI is using the installation script. Open **PowerShell** and run:

```powershell
irm https://cre.chain.link/install.ps1 | iex
```

This script will:

- Download the correct binary for Windows
- Verify the binary's integrity
- Install it to `$env:LOCALAPPDATA\Programs\cre`
- Make the binary executable

After the script completes, **open a new PowerShell window** and verify the installation:

```powershell
cre version
```

**Expected output:** `cre version v1.0.2`

### Manual installation

If you prefer to install manually or the automatic installation doesn't work for your environment, follow these steps:

The CRE CLI is publicly available on GitHub. Click the button below to access the releases page and download `cre_windows_amd64.zip`.

After downloading the file from the releases page, move on to the next step to verify its integrity.

#### 1. Verify file integrity

Before installing, verify the file integrity using a checksum to ensure the binary hasn't been tampered with.

**Check the SHA-256 checksum**

Open a PowerShell terminal and run the following command in the directory where you downloaded the archive:

```powershell
Get-FileHash cre_windows_amd64.zip -Algorithm SHA256
```

**Verify against the official checksum**

Compare the `Hash` value in the output with the official checksum below:

| File                    | SHA-256 Checksum                                                 |
| ----------------------- | ---------------------------------------------------------------- |
| `cre_windows_amd64.zip` | 60fe65b74619c4164c0a9d6442611bf8537a04a6daf1ed3ecefc608cbbffdb01 |

If the checksum doesn't match, do not proceed with installation. Contact your Chainlink point of contact for assistance.

#### 2. Extract and install

1. Navigate to the directory where you downloaded the archive.
2. Right-click the `.zip` file and select **Extract All...**.
3. Choose a permanent location for the extracted folder (e.g., `C:\Program Files\cre-cli`).
4. Inside the extracted folder, rename the file `cre_v1.0.2_windows_amd64.exe` to `cre.exe`.

#### 3. Add the CLI to your PATH

To run `cre` commands from any directory, you need to add the folder where you saved `cre.exe` to your system's PATH environment variable.

1. Open the **Start Menu** and search for "environment variables".
2. Select **Edit the system environment variables**.
3. In the System Properties window, click the **Environment Variables...** button.
4. In the **System variables** section, find and select the `Path` variable, then click **Edit...**.
5. Click **New** and add the full path to the folder where you saved `cre.exe` (e.g., `C:\Program Files\cre-cli`).
6. Click **OK** on all windows to save your changes.

<Aside type="note" title="Note">
  You must open a **new** terminal window for the PATH changes to take effect.
</Aside>

#### 4. Verify the installation

Open a new **PowerShell** or **Command Prompt** window and run:

```bash
cre version
```

You should see version information: `cre version v1.0.2`.

## Next steps

Now that you have the `cre` CLI installed, you'll need to create a CRE account and authenticate before you can use it.

- **[Creating Your Account](/cre/account/creating-account)**: Create your CRE account and set up two-factor authentication
- **[Logging in with the CLI](/cre/account/cli-login)**: Authenticate the CLI with your account

Once you're authenticated, you're ready to build your first workflow:

- **[Getting Started — Part 1: Project Setup & Simulation](/cre/getting-started/part-1-project-setup)**: Initialize a new, blank CRE project and run your first "Hello World!" simulation.

---

# Conclusion & Next Steps
Source: https://docs.chain.link/cre/getting-started/conclusion
Last Updated: 2025-11-04

You've built a complete, end-to-end CRE workflow from scratch.

You started with an empty project and progressively built a workflow that:

- Fetches data from an offchain API with consensus
- Reads values from a smart contract
- Performs calculations combining onchain and offchain data
- Writes results back to the blockchain

**This is no small achievement.** You've mastered the core pattern that powers most CRE workflows: the trigger-and-callback model with capabilities for HTTP, EVM, and consensus.

## What's next?

Now that you have a working workflow, here's your natural progression from simulation to production and beyond.

### 1. See a complete example

Ready to see all these concepts in a more complex, real-world scenario?

- **[Run the Custom Data Feed Demo](/cre/templates/running-demo-workflow)** - Explore an advanced template that combines multiple capabilities

**Why this matters:** Templates show production-ready patterns.

### 2. Deploy your Calculator workflow to Production

You've simulated your workflow locally. **The logical next step is to deploy it to the CRE production environment** so it runs across a Decentralized Oracle Network (DON).


<Aside type="note" title="Deployment access required">
  - Deploying workflows requires Early Access approval. If you don't have deployment access yet, <a href="https://cre.chain.link/request-access" target="_blank" rel="noopener noreferrer">request it here</a>.
  - **While you wait:** Continue building and simulating workflows using [`cre workflow simulate`](/cre/guides/operations/simulating-workflows).
</Aside>

**Follow this deployment sequence:**

1. **[Link a Wallet Key](/cre/organization/linking-keys)** - Connect your wallet address to your organization (required before deployment)
2. **[Deploy Your Workflow](/cre/guides/operations/deploying-workflows)** - Push your calculator workflow live
3. **[Monitor Your Workflow](/cre/guides/operations/monitoring-workflows)** - Watch it execute in production and debug any issues

**Why this matters:** Deploying moves your workflow from local simulation to production execution across a DON.

### 3. Explore different triggers

You used a **Cron trigger** (time-based). **Most production workflows react to real-world events.**

**Try these next:**

- **[HTTP Trigger](/cre/guides/workflow/using-triggers/http-trigger/overview)** - Let external systems trigger your workflow via API calls
- **[EVM Log Trigger](/cre/guides/workflow/using-triggers/evm-log-trigger)** - React to onchain events (e.g., token transfers, contract events)

**Why this matters:** Event-driven workflows are more powerful than scheduled ones. They respond instantly to real-world changes.

### 4. Add secrets

Your calculator used a public API. **Real workflows often need API keys and other sensitive data.**

**Learn how to secure your secrets:**

- **[Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation)** - Store secrets in your local environment for development
- **[Using Secrets with Deployed Workflows](/cre/guides/workflow/secrets/using-secrets-deployed)** - Store secrets in the Vault DON for production
- **[Managing Secrets with 1Password](/cre/guides/workflow/secrets/managing-secrets-1password)** - Best practice: inject secrets at runtime

**Why this matters:** Hardcoded credentials are a security risk. CRE's secrets management lets you safely use authenticated APIs and private keys.

### 5. Build your own consumer contract

You used a **pre-deployed consumer contract**. **For production workflows, you'll create custom contracts tailored to your use case.**

**Learn the secure pattern:**

- **[Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts)** - Create contracts that safely receive CRE data

**Why this matters:** Consumer contracts enforce business logic and validation onchain, enabling trustless and verifiable execution.

## Reference: Deepen Your Understanding

Want to dive deeper into specific concepts from the Getting Started guide? Use this section as a quick reference.

**Workflow Structure & Triggers**

- **[Core SDK Reference](/cre/reference/sdk/core/)** - Fundamental building blocks (`InitWorkflow`, `Handler`, `Runtime`)
- **[Triggers Overview](/cre/guides/workflow/using-triggers/overview)** - Compare all available event sources

**HTTP & Offchain Data**

- **[API Interactions Guide](/cre/guides/workflow/using-http-client/)** - Complete patterns for HTTP requests
- **[Consensus & Aggregation](/cre/reference/sdk/consensus)** - All aggregation methods (median, mode, custom)
- **[Consensus Computing Concept](/cre/concepts/consensus-computing)** - How CRE's consensus-based execution works

**EVM & Onchain Interactions**

- **[EVM Client Overview](/cre/guides/workflow/using-evm-client/overview)** - Introduction to smart contract interactions
- **[Onchain Read Guide](/cre/guides/workflow/using-evm-client/onchain-read)** - Reading from a smart contract
- **[Onchain Write Guide](/cre/guides/workflow/using-evm-client/onchain-write/overview)** - Complete write patterns and report generation

**Configuration & Secrets**

- **[Project Configuration](/cre/reference/project-configuration/)** - Complete guide to `project.yaml`, `workflow.yaml`, and targets
- **[Secrets Guide](/cre/guides/workflow/secrets)** - All secrets management patterns

**All Capabilities**

- **[Capabilities Overview](/cre/capabilities/)** - See the full list of CRE capabilities and how they work together

---

# Using Triggers
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/overview
Last Updated: 2025-11-04

Triggers are a special type of capability that start your workflows. They are event-driven services that watch for a specific condition to be met. When the condition occurs, the trigger fires and instructs CRE to run the callback function you have registered for that event.

A single workflow can contain multiple handlers, allowing you to react to different events with specific logic.

This section provides detailed guides for each available trigger type:

- **[Cron Trigger](/cre/guides/workflow/using-triggers/cron-trigger)**: Run workflows on a time-based schedule.
- **[HTTP Trigger](/cre/guides/workflow/using-triggers/http-trigger/overview)**: Start workflows in response to an HTTP request from an external system.
- **[EVM Log Trigger](/cre/guides/workflow/using-triggers/evm-log-trigger)**: Initiate workflows in response to a specific event being emitted by a smart contract.

---

# Testing HTTP Triggers in Simulation
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/http-trigger/testing-in-simulation
Last Updated: 2025-11-10

During development, you can test your HTTP trigger workflows locally using the `cre workflow simulate` command. The simulator allows you to provide test payloads without setting up authorization keys or JWT authentication.

This guide focuses specifically on HTTP trigger simulation with detailed examples and scenarios. For a general overview of workflow simulation covering all trigger types, see [Simulating Workflows](/cre/guides/operations/simulating-workflows).

## Prerequisites

Before running a simulation:

- **CRE CLI installed**: You need the CRE CLI to run simulations. See [CLI Installation](/cre/getting-started/cli-installation) if you haven't installed it yet.
- **CRE account & authentication**: You must have a CRE account and be logged in with the CLI. See [Create your account](/cre/account/creating-account) and [Log in with the CLI](/cre/account/cli-login) for instructions.
- **HTTP trigger configured**: Your workflow must have an HTTP trigger handler registered. See [Configuration & Handler](/cre/guides/workflow/using-triggers/http-trigger/configuration) for setup instructions.

<Aside type="note" title="No authorization required for simulation">
  During simulation, you can use an empty authorization configuration (e.g., `&http.Config{}` in Go or `trigger({})` in
  TypeScript). This simplifies local testing—just remember to add `authorizedKeys` before deploying.
</Aside>

## Basic simulation

To simulate a workflow with an HTTP trigger, run the `simulate` command from your project root:

```bash
cre workflow simulate <workflow-folder> --target <target-name>
```

**Example:**

```bash
cre workflow simulate my-http-workflow --target staging-settings
```

The simulator will detect your HTTP trigger and prompt you to select it from available triggers.

## Providing input data

You have three ways to provide JSON input to your HTTP trigger during simulation:

### 1. Interactive mode (default)

When you run the simulation without input flags, the CLI prompts you to enter JSON data:

```bash
$ cre workflow simulate my-http-workflow --target staging-settings

# Select the HTTP trigger when prompted
? Select a trigger to simulate: HTTP Trigger

# Enter your JSON input:
? Enter JSON input for the HTTP trigger:
{"userId": "123", "action": "purchase", "amount": 50}
```

The simulator converts your JSON into a payload and passes it to your callback function.

### 2. Inline JSON string

For non-interactive execution (useful for CI/CD or scripting), pass JSON directly using the `--http-payload` flag along with `--non-interactive` and `--trigger-index`:

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload '{"userId":"123","action":"purchase","amount":50}' --target staging-settings
```

<Aside type="note" title="Non-interactive requirements">
  The `--http-payload` flag requires `--non-interactive` mode. You must also specify `--trigger-index` (0-based index of
  your HTTP trigger). If your HTTP trigger is the only trigger or the first one defined, use `--trigger-index 0`.
</Aside>

<Aside type="note" title="Escaping quotes">
  When passing JSON inline, use single quotes around the entire JSON string and double quotes for JSON keys and string
  values. On Windows, you may need to escape double quotes differently: `"{\"userId\":\"123\"}"`
</Aside>

### 3. Input from file

For complex payloads or reusable test data, store your JSON in a file and reference it in non-interactive mode.

**Option 1: File in workflow folder**

Create the payload file in your workflow directory:

**`my-http-workflow/test-payload.json`:**

```json
{
  "userId": "123",
  "action": "purchase",
  "amount": 50,
  "metadata": {
    "timestamp": "2025-11-10T10:00:00Z",
    "source": "mobile-app"
  }
}
```

**Simulate:**

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload test-payload.json --target staging-settings
```

**Option 2: File at project root**

Create the payload file at your project root. Simulate (using `../` to reference the parent directory):

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload ../test-payload.json --target staging-settings
```

<Aside type="note" title="File paths are relative to the workflow folder">
  The CLI resolves file paths relative to the **workflow folder** you're simulating. Use just the filename for files in
  the workflow folder, or `../filename.json` for files at the project root.
</Aside>

## Example workflow simulation

Let's simulate a complete workflow that processes HTTP requests.

**Setup your config file:**

For this example, create a `config.staging.json` file with:

```json
{
  "minimumAmount": 10
}
```

This configuration sets the minimum purchase amount to 10, which we'll test with different scenarios.

**Workflow code:**

```ts
import { cre, type Runtime, type HTTPPayload, Runner, decodeJson } from "@chainlink/cre-sdk"

type Config = {
  minimumAmount: number
}

type RequestData = {
  userId: string
  action: string
  amount: number
}

const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): string => {
  const requestData = decodeJson(payload.input) as RequestData

  // Validate required fields
  if (!requestData.userId || !requestData.action || requestData.amount === undefined) {
    runtime.log("Missing required fields")
    return "Error: Missing required fields (userId, action, amount)"
  }

  runtime.log(`Processing ${requestData.action} for user ${requestData.userId}`)

  if (requestData.amount < runtime.config.minimumAmount) {
    runtime.log(`Amount ${requestData.amount} below minimum ${runtime.config.minimumAmount}`)
    return "Amount too low"
  }

  runtime.log(`Processing amount: ${requestData.amount}`)
  return `Successfully processed ${requestData.action}`
}

const initWorkflow = (config: Config) => {
  const http = new cre.capabilities.HTTPCapability()

  return [
    cre.handler(http.trigger({}), onHttpTrigger), // Empty config OK for simulation
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

**Run the simulation:**

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload '{"userId":"user_123","action":"purchase","amount":100}' --target staging-settings
```

**Expected output:**

```
Workflow compiled
2025-11-10T11:28:25Z [SIMULATION] Simulator Initialized

2025-11-10T11:28:25Z [SIMULATION] Running trigger trigger=http-trigger@1.0.0-alpha
2025-11-10T11:28:25Z [USER LOG] Processing purchase for user user_123
2025-11-10T11:28:25Z [USER LOG] Processing amount: 100

Workflow Simulation Result:
 "Successfully processed purchase"

2025-11-10T11:28:25Z [SIMULATION] Execution finished signal received
2025-11-10T11:28:25Z [SIMULATION] Skipping WorkflowEngineV2
```

## Testing different scenarios

Use simulation to test various input scenarios:

### Valid request

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload '{"userId":"123","action":"purchase","amount":100}' --target staging-settings
```

### Invalid input (below minimum)

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload '{"userId":"123","action":"purchase","amount":5}' --target staging-settings
```

**Expected output:**

```
Workflow compiled
2025-11-10T11:34:38Z [SIMULATION] Simulator Initialized

2025-11-10T11:34:38Z [SIMULATION] Running trigger trigger=http-trigger@1.0.0-alpha
2025-11-10T11:34:38Z [USER LOG] Processing purchase for user 123
2025-11-10T11:34:38Z [USER LOG] Amount 5 below minimum 10

Workflow Simulation Result:
 "Amount too low"

2025-11-10T11:34:38Z [SIMULATION] Execution finished signal received
2025-11-10T11:34:38Z [SIMULATION] Skipping WorkflowEngineV2
```

### Missing fields

```bash
cre workflow simulate my-http-workflow --non-interactive --trigger-index 0 --http-payload '{"userId":"123","action":"purchase"}' --target staging-settings
```

**Expected output:**

```
Workflow compiled
2025-11-10T11:37:57Z [SIMULATION] Simulator Initialized

2025-11-10T11:37:57Z [SIMULATION] Running trigger trigger=http-trigger@1.0.0-alpha
2025-11-10T11:37:57Z [USER LOG] Missing required fields

Workflow Simulation Result:
 "Error: Missing required fields (userId, action, amount)"

2025-11-10T11:37:57Z [SIMULATION] Execution finished signal received
2025-11-10T11:37:57Z [SIMULATION] Skipping WorkflowEngineV2
```

This helps you verify error handling and edge cases before deployment.

## Simulation vs production behavior

| Aspect                     | Simulation                          | Production                              |
| -------------------------- | ----------------------------------- | --------------------------------------- |
| **Authorization**          | Not required (empty config allowed) | Required (`authorizedKeys` must be set) |
| **Signature verification** | Skipped                             | Strictly enforced                       |
| **Execution**              | Immediate, synchronous              | Asynchronous via DON                    |
| **Logs**                   | Printed to terminal                 | Available in CRE UI                     |

<Aside type="caution" title="Add authorization before deploying">
  Simulation allows empty authorization configs for convenience, but **deployed workflows require `authorizedKeys`**.
  The CLI will reject deployments with HTTP triggers that lack authorization configuration.
</Aside>

## Next steps

Once you've tested your workflow locally:

1. **Add authorization**: Configure `authorizedKeys` in your HTTP trigger for production
2. **Deploy your workflow**: Use [`cre workflow deploy`](/cre/guides/operations/deploying-workflows) to register it
3. **Trigger it in production**: Follow the [Triggering Deployed Workflows](/cre/guides/workflow/using-triggers/http-trigger/triggering-deployed-workflows) guide

---

# Triggering Deployed Workflows
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/http-trigger/triggering-deployed-workflows
Last Updated: 2025-11-10

Once you've [deployed a workflow](/cre/guides/operations/deploying-workflows) with an HTTP trigger, you can execute it by sending authenticated HTTP requests to the CRE gateway. This guide explains how to trigger deployed workflows in production.

## What you'll learn

This guide covers the complete technical specification for triggering deployed workflows:

- **Request format** - The JSON-RPC structure for workflow execution requests
- **JWT authentication** - How to generate cryptographically signed tokens
- **Signature process** - The ECDSA signing steps for Ethereum-compatible authentication
- **Reference implementations** - Code examples in Go and TypeScript

<Aside type="note" title="Just testing?">
  If you're testing deployed workflows during development, the [Local Testing
  Tool](/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool) is much simpler—it handles all JWT
  generation automatically. This guide is for production implementations where you need full control over the HTTP
  requests.
</Aside>

## Prerequisites

- **Deployed workflow**: Your workflow must be deployed with an HTTP trigger. See [Deploying Workflows](/cre/guides/operations/deploying-workflows).
- **Workflow ID**: Available from deployment output or the [CRE UI](/cre/guides/operations/monitoring-workflows).
- **Private key**: The private key corresponding to one of the `authorizedKeys` configured in your HTTP trigger.

<Aside type="note" title="For easier testing">
  If you're testing your deployed workflow during development, consider using the [Local Testing
  Tool](/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool) instead. It handles JWT generation
  automatically and triggers the workflow for you.
</Aside>

## Finding your workflow ID

Your workflow ID is a 64-character hexadecimal string (without `0x` prefix) that uniquely identifies your deployed workflow.

### From deployment output

When you deploy a workflow, the CLI displays the workflow ID:

```bash
$ cre workflow deploy my-workflow --target production-settings

...
Details:
   Workflow ID:    a1b2c3d4e5f67890a1b2c3d4e5f67890a1b2c3d4e5f67890a1b2c3d4e5f67890
...
```

### From the CRE UI

1. Log in to <a href="https://cre.chain.link/workflows" target="_blank" rel="noopener noreferrer">cre.chain.link/workflows</a>
2. Click on your workflow name
3. In the **Overview** section, find the **Workflow ID** field
4. Click the copy button to copy it to your clipboard

## Request format

All workflow executions use JSON-RPC 2.0 format:

```http
POST https://01.gateway.zone-a.cre.chain.link
Content-Type: application/json
Authorization: Bearer <JWT_TOKEN>

{
  "id": "unique-request-id",
  "jsonrpc": "2.0",
  "method": "workflows.execute",
  "params": {
    "input": {
      "key1": "value1",
      "key2": "value2"
    },
    "workflow": {
      "workflowID": "your-64-character-workflow-id"
    }
  }
}
```

### Request components

| Field                        | Description                                                        |
| ---------------------------- | ------------------------------------------------------------------ |
| `jsonrpc`                    | Always `"2.0"` (JSON-RPC version)                                  |
| `id`                         | Unique identifier for this request (any string, used for tracking) |
| `method`                     | Always `"workflows.execute"`                                       |
| `params.input`               | Your custom JSON payload (passed to your workflow callback)        |
| `params.workflow.workflowID` | Your 64-character workflow ID (no `0x` prefix)                     |

<Aside type="tip" title="Request ID for tracking">
  The `id` field helps you correlate requests with responses. Use a UUID or timestamp-based identifier for easier
  debugging and request tracking.
</Aside>

## JWT authentication

The `Authorization` header must contain a Bearer JWT (JSON Web Token) that proves the request was signed by an authorized key. The JWT has three parts: `header.payload.signature`.

### JWT structure

The JWT is a base64url-encoded string consisting of three parts separated by dots:

```
<base64url(header)>.<base64url(payload)>.<base64url(signature)>
```

### 1. Header

The JWT header specifies the signing algorithm:

```json
{
  "alg": "ETH",
  "typ": "JWT"
}
```

Base64url-encode this JSON to create the header part.

### 2. Payload

The JWT payload contains request metadata and a digest of the request body:

```json
{
  "digest": "0x<sha256_hash_of_request_body>",
  "iss": "0xYourEVMAddress",
  "iat": 1762807282,
  "exp": 1762807582,
  "jti": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Payload fields:**

| Field    | Description                                                                  |
| -------- | ---------------------------------------------------------------------------- |
| `digest` | SHA256 hash of the JSON-RPC request body (with `0x` prefix)                  |
| `iss`    | Issuer - your EVM address (the public key corresponding to your private key) |
| `iat`    | Issued at time (Unix timestamp in seconds)                                   |
| `exp`    | Expiration time (Unix timestamp, **max 5 minutes** after `iat`)              |
| `jti`    | JWT ID (UUID v4 for replay protection)                                       |

#### Computing the digest

The `digest` is a SHA256 hash of your JSON-RPC request body **serialized as UTF-8 encoded JSON in ascending lexicographic order** (sorted by key names):

**Original request:**

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "method": "workflows.execute",
  "params": {
    "input": { "key1": "value1", "key2": "value2" },
    "workflow": { "workflowID": "a1b2c3..." }
  }
}
```

**Keys must be sorted alphabetically at every level:**

```json
{
  "id": "req-123",
  "jsonrpc": "2.0",
  "method": "workflows.execute",
  "params": { "input": { "key1": "value1", "key2": "value2" }, "workflow": { "workflowID": "a1b2c3..." } }
}
```

Then compute: `digest = "0x" + sha256(sorted_json_string)`

<Aside type="caution" title="Key ordering is critical">
  The digest **must** be computed from a JSON string with keys sorted in ascending lexicographic order at all nesting
  levels. Incorrect ordering will cause signature verification to fail.
</Aside>

### 3. Signature

The signature is an ECDSA signature of the message `<base64url(header)>.<base64url(payload)>` using your private key.

**Signing process:**

1. Concatenate the encoded header and payload: `message = base64url(header) + "." + base64url(payload)`
2. Sign the message using ECDSA with your private key:
   - Prepend the Ethereum signed message prefix: `"\x19Ethereum Signed Message:\n" + len(message) + message`
   - Hash the prefixed message with Keccak256
   - Sign the hash using your private key
3. Extract the signature components: `r` (32 bytes), `s` (32 bytes), `v` (1 byte, recovery ID)
4. Concatenate: `signature_bytes = r || s || v`
5. Base64url-encode the signature bytes

## Reference implementations

Manual JWT generation requires careful cryptographic operations. Use these reference implementations as guidance:

### For Go

Production-grade utilities:

- **JWT creation**: <a href="https://github.com/smartcontractkit/chainlink/blob/develop/core/utils/jwt.go" target="_blank" rel="noopener noreferrer">jwt.go</a> - See the `CreateRequestJWT` method for complete JWT generation
- **ECDSA signatures**: <a href="https://github.com/smartcontractkit/chainlink/blob/develop/core/utils/eth_signatures.go" target="_blank" rel="noopener noreferrer">eth_signatures.go</a> - See the `GenerateEthSignature` method for Ethereum-compatible signing

### For TypeScript/JavaScript

The CRE SDK includes a reference implementation using `viem`:

- **Complete implementation**: <a href="https://github.com/smartcontractkit/cre-sdk-typescript/tree/main/packages/cre-http-trigger/src" target="_blank" rel="noopener noreferrer">cre-http-trigger source code</a>
- Key files:
  - `create-jwt.ts` - JWT header, payload, and signing logic
  - `utils.ts` - SHA256 hashing and base64url encoding helpers

### For testing

If you're testing deployed workflows during development, use the [Local Testing Tool](/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool) which runs a local proxy server that handles the entire JWT generation and request flow automatically.

## Example request (conceptual)

Here's what a complete curl request looks like:

```bash
curl -X POST https://01.gateway.zone-a.cre.chain.link \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer eyJhbGciOiJFVEgiLCJ0eXAiOiJKV1QifQ.eyJkaWdlc3QiOiIweDRhMWYyYjNjNGQ1ZTZmN2E4YjljMGQxZTJmM2E0YjVjNmQ3ZThmOWEwYjFjMmQzZTRmNWE2YjdjOGQ5ZTBmMWEiLCJpc3MiOiIweGIwOEUwMDRiZDJiNWFGZjFGNUY5NTBkMTQxZjQ0OUIxYzA1ODAwZWIiLCJpYXQiOjE3MzM4MzIwMDAsImV4cCI6MTczMzgzMjMwMCwianRpIjoiNTUwZTg0MDAtZTI5Yi00MWQ0LWE3MTYtNDQ2NjU1NDQwMDAwIn0.r7s8v9recoveryId..." \
  -d '{
    "jsonrpc": "2.0",
    "id": "req-123",
    "method": "workflows.execute",
    "params": {
      "input": {
        "userId": "user_123",
        "action": "purchase",
        "amount": 100
      },
      "workflow": {
        "workflowID": "a1b2c3d4e5f67890a1b2c3d4e5f67890a1b2c3d4e5f67890a1b2c3d4e5f67890"
      }
    }
  }'
```

## Response format

### Success response

When your request is successfully accepted, the gateway returns a JSON-RPC response:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "method": "workflows.execute",
  "result": {
    "workflow_id": "<your-workflow-id>",
    "workflow_execution_id": "<your-workflow-execution-id>",
    "status": "ACCEPTED"
  }
}
```

**Response fields:**

| Field                          | Description                                                                                                 |
| ------------------------------ | ----------------------------------------------------------------------------------------------------------- |
| `jsonrpc`                      | JSON-RPC version (always `"2.0"`)                                                                           |
| `id`                           | The request ID you provided in the request                                                                  |
| `method`                       | The method called (always `"workflows.execute"`)                                                            |
| `result.workflow_id`           | Your workflow ID (with `0x` prefix)                                                                         |
| `result.workflow_execution_id` | Unique execution ID for this workflow run (use this to track execution in the CRE UI)                       |
| `result.status`                | Execution status (typically `"ACCEPTED"` when the workflow trigger is successfully accepted by the gateway) |

<Aside type="tip" title="Track your execution">
  Copy the `workflow_execution_id` to track this specific execution in the [CRE UI](https://cre.chain.link/workflows).
  You can view logs, events, and execution details using this ID.
</Aside>

### Error response

If the request fails (e.g., invalid JWT, unauthorized key, workflow not found), the gateway returns an error response. Example:

```json
{
  "jsonrpc": "2.0",
  "id": "req-123",
  "method": "",
  "error": {
    "code": -32600,
    "message": "Auth failure: signer '0x...' is not authorized for workflow '0x...'. Ensure that the signer is registered in the workflow definition"
  }
}
```

<Aside type="note" title="HTTP status codes">
  Error responses typically return HTTP status `400 Bad Request`. The JSON-RPC error object provides detailed
  information about what went wrong.
</Aside>

## Verifying execution

After triggering your workflow, verify execution in the CRE UI:

1. Go to <a href="https://cre.chain.link/workflows" target="_blank" rel="noopener noreferrer">cre.chain.link/workflows</a>
2. Click on your workflow
3. Check the **Execution** tab for recent runs
4. Click on an **Execution ID** to view detailed logs and events

See [Monitoring & Debugging Workflows](/cre/guides/operations/monitoring-workflows) for complete monitoring guidance.

## Security considerations

### Private key protection

- **Never commit private keys** to version control
- **Use environment variables** or secret management tools (e.g., AWS Secrets Manager, HashiCorp Vault)
- **Rotate keys periodically** and update your workflow's `authorizedKeys` if compromised

### Request expiration

- JWT tokens expire after the `exp` timestamp (max 5 minutes after `iat`)
- This prevents replay attacks with captured requests
- Generate new JWTs for each request

### Replay protection

- The `jti` (JWT ID) field provides replay protection
- Use a unique UUID for every request
- The gateway may reject duplicate `jti` values within the expiration window

## Next steps

### For easier testing

Manual JWT generation is complex and error-prone. For development and testing:

- **[Local Testing Tool](/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool)** - Automatically generates JWTs and sends requests

### Additional resources

- **[HTTP Trigger SDK Reference](/cre/reference/sdk/triggers/http-trigger)** - Complete API documentation
- **[Monitoring Workflows](/cre/guides/operations/monitoring-workflows)** - Track execution history and debug issues

---

# Testing with Local JWT Server
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool
Last Updated: 2025-11-04

The <a href="https://github.com/smartcontractkit/cre-sdk-typescript/tree/main/packages/cre-http-trigger" target="_blank" rel="noopener noreferrer">`cre-http-trigger` TypeScript package</a> simplifies testing deployed workflows with HTTP triggers by handling JWT generation, signing, and request formatting automatically. It provides a local HTTP server that acts as a proxy between your test requests and the CRE gateway.

Manually creating JWT tokens for HTTP trigger requests involves complex steps: computing SHA256 digests with sorted JSON keys, generating ECDSA signatures, and encoding everything correctly (see [Triggering Deployed Workflows](/cre/guides/workflow/using-triggers/http-trigger/triggering-deployed-workflows)).

The `cre-http-trigger` tool eliminates this complexity by:

1. **Managing JWT generation** - Automatically creates properly signed JWT tokens
2. **Handling request formatting** - Constructs valid JSON-RPC requests
3. **Providing a simple API** - Send test requests with plain JSON payloads
4. **Enabling local testing** - Run a local server that forwards requests to the gateway

<Aside type="note" title="For testing only">
  This tool is designed for **development and testing** of deployed workflows. For production integrations, implement
  JWT generation directly in your backend service for better security and control.
</Aside>


<Aside type="caution" title="Educational Example Disclaimer">
  This page includes an educational example to use a Chainlink system, product, or service and is provided to
  demonstrate how to interact with Chainlink's systems, products, and services to integrate them into your own. This
  template is provided "AS IS" and "AS AVAILABLE" without warranties of any kind, it has not been audited, and it may be
  missing key checks or error handling to make the usage of the system, product or service more clear. Do not use the
  code in this example in a production environment without completing your own audits and application of best practices.
  Neither Chainlink Labs, the Chainlink Foundation, nor Chainlink node operators are responsible for unintended outputs
  that are generated due to errors in code.
</Aside>

## Prerequisites

- **Bun runtime**: The tool requires <a href="https://bun.sh" target="_blank" rel="noopener noreferrer">Bun</a> version 1.2.21 or higher
- **Deployed workflow**: Your workflow must be deployed with an HTTP trigger
- **Workflow ID**: Available from deployment output or the CRE UI
- **Private key**: The private key corresponding to one of the `authorizedKeys` in your HTTP trigger configuration

## Installation

Clone the CRE SDK TypeScript repository and navigate to the HTTP trigger package:

```bash
git clone https://github.com/smartcontractkit/cre-sdk-typescript.git
cd cre-sdk-typescript/packages/cre-http-trigger
```

## Setup

### 1. Install dependencies

```bash
bun install
```

### 2. Configure environment variables

Create a `.env` file in the package root:

```bash
PRIVATE_KEY=0xYourPrivateKeyHere
GATEWAY_URL=https://01.gateway.zone-a.cre.chain.link
```

Replace `0xYourPrivateKeyHere` with your EVM private key.

<Aside type="caution" title="Keep your .env file secure">
  Never commit `.env` files to version control. Add `.env` to your `.gitignore` file. The package repository already
  includes `.env` in `.gitignore` by default.
</Aside>

### 3. Start the server

Run the local proxy server:

```bash
bun start
```

The server starts on **port 2000** by default. You should see:

```
🚀 HTTP Trigger Server running at http://localhost:2000
```

## Usage

With the server running, you can trigger workflows by sending HTTP POST requests to `http://localhost:2000/trigger`.

### Basic request

Send a POST request with your workflow ID as a query parameter and input data in the body:

```bash
curl -X POST "http://localhost:2000/trigger?workflowID=<your-workflow-id>" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "userId": "user_123",
      "action": "purchase",
      "amount": 100
    }
  }'
```

### Request format

**Query parameter:**

| Field        | Type   | Description                                    |
| ------------ | ------ | ---------------------------------------------- |
| `workflowID` | string | Your 64-character workflow ID (no `0x` prefix) |

**Request body:**

| Field   | Type   | Description                                   |
| ------- | ------ | --------------------------------------------- |
| `input` | object | JSON payload passed to your workflow callback |

### Example

```bash
curl -X POST "http://localhost:2000/trigger?workflowID=<your-workflow-id>" \
  -H "Content-Type: application/json" \
  -d '{
    "input": {
      "userId": "user_456",
      "action": "purchase",
      "amount": 250
    }
  }'
```

**Server logs:**

When the server processes your request, you'll see logs in the terminal where the server is running:

```
🚀 Triggering workflow...

   Workflow: {
  workflowID: "<your-workflow-id>",
}

   Input: {
  "userId": "user_456",
  "action": "purchase",
  "amount": 250
}

   Signed by: <your-public-address>

   Response: Response (297 bytes) {
  ok: true,
  url: "https://01.gateway.zone-a.cre.chain.link/",
  status: 200,
  statusText: "OK",
  ...
}
```

**Client response:**

Your curl command receives a JSON response:

```json
{
  "success": true,
  "response": {
    "jsonrpc": "2.0",
    "id": "<your-request-id>",
    "method": "workflows.execute",
    "result": {
      "workflow_id": "<your-workflow-id>",
      "workflow_execution_id": "<your-workflow-execution-id>",
      "status": "ACCEPTED"
    }
  }
}
```

**Response fields:**

| Field                          | Description                                                                           |
| ------------------------------ | ------------------------------------------------------------------------------------- |
| `success`                      | Always `true` for successful proxy requests                                           |
| `response.jsonrpc`             | JSON-RPC version (always `"2.0"`)                                                     |
| `response.id`                  | Unique request ID generated by the tool                                               |
| `response.method`              | Always `"workflows.execute"`                                                          |
| `result.workflow_id`           | Your workflow ID (with `0x` prefix)                                                   |
| `result.workflow_execution_id` | Unique execution ID for this workflow run (use this to track execution in the CRE UI) |
| `result.status`                | Execution status (typically `"ACCEPTED"` for valid requests)                          |

<Aside type="tip" title="Track your execution">
  Copy the `workflow_execution_id` to track this specific execution in the [CRE UI](https://cre.chain.link/workflows).
  You can view logs, events, and execution details using this ID.
</Aside>

### Health check endpoint

Verify the server is running:

```bash
curl http://localhost:2000/health
```

**Response:**

```
OK
```

## How it works

The tool performs the following steps automatically:

1. **Receives your request** at `/trigger` with `workflowID` as a query parameter and `input` in the request body
2. **Loads configuration** from environment variables (`PRIVATE_KEY`, `GATEWAY_URL`)
3. **Constructs JSON-RPC payload**:
   ```json
   {
     "jsonrpc": "2.0",
     "id": "generated-uuid",
     "method": "workflows.execute",
     "params": {
       "input": { ... },
       "workflow": { "workflowID": "..." }
     }
   }
   ```
4. **Computes digest** of the request body (SHA256 with sorted keys)
5. **Creates JWT payload** with `digest`, `iss`, `iat`, `exp`, `jti`
6. **Signs JWT** using your private key with ECDSA
7. **Sends authenticated request** to the CRE gateway with the JWT in the `Authorization` header
8. **Returns the response** to your test client

This eliminates the need to manually handle cryptographic operations and request formatting.

## Code structure

If you're curious about the implementation or need to customize the tool:

| File                      | Purpose                                                                |
| ------------------------- | ---------------------------------------------------------------------- |
| `src/index.ts`            | HTTP server setup (Bun server with `/health` and `/trigger` endpoints) |
| `src/trigger-workflow.ts` | Main orchestration: calls config, JWT generation, and sends request    |
| `src/create-jwt.ts`       | JWT creation: header, payload, signing with ECDSA                      |
| `src/get-config.ts`       | Loads `PRIVATE_KEY` and `GATEWAY_URL` from environment                 |
| `src/schemas.ts`          | Zod schemas for input validation                                       |
| `src/utils.ts`            | Helper functions (SHA256 hashing, base64url encoding)                  |

All code is available in the <a href="https://github.com/smartcontractkit/cre-sdk-typescript/tree/main/packages/cre-http-trigger" target="_blank" rel="noopener noreferrer">cre-http-trigger package</a>.

## Monitoring execution

After triggering workflows with the tool, verify execution in the CRE UI:

1. Go to <a href="https://cre.chain.link/workflows" target="_blank" rel="noopener noreferrer">cre.chain.link/workflows</a>
2. Click on your workflow
3. Navigate to the **Execution** tab
4. Find recent executions and click **Execution ID** to view Events and Logs

See [Monitoring & Debugging Workflows](/cre/guides/operations/monitoring-workflows) for complete guidance.

## Troubleshooting

### "Invalid private key" error

**Cause:** The `PRIVATE_KEY` environment variable is missing or malformed.

**Solution:** Ensure your `.env` file contains a valid EVM private key (with or without `0x` prefix).

### "Unauthorized" error

**Cause:** The private key doesn't correspond to any of the `authorizedKeys` in your workflow's HTTP trigger configuration.

**Solution:** Verify that the public address derived from your private key matches an entry in your workflow's `authorizedKeys`.

### "Workflow not found" error

**Cause:** Incorrect `workflowID` or the workflow isn't deployed.

**Solution:** Double-check the workflow ID from the CRE UI or deployment output.

### Server won't start

**Cause:** Port 2000 is already in use.

**Solution:** Stop any other services using port 2000, or modify `src/index.ts` to use a different port.

## Production considerations

This tool is **not intended for production use**. For production integrations:

1. **Implement JWT generation in your backend** - Use your backend service's language and crypto libraries
2. **Secure private key storage** - Use a secrets manager
3. **Add retry logic** - Handle transient network errors and gateway timeouts
4. **Monitor and log** - Track requests, responses, and execution outcomes
5. **Rate limiting** - Implement appropriate request throttling

## Next steps

- **[Monitoring Workflows](/cre/guides/operations/monitoring-workflows)** - View execution logs and debug issues

---

# Building Consumer Contracts
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts
Last Updated: 2025-11-20

When your workflow [writes data to the blockchain](/cre/guides/workflow/using-evm-client/onchain-write), it doesn't call your contract directly. Instead, it submits a signed report to a Chainlink `KeystoneForwarder` contract, which then calls your contract.

This guide explains how to build a consumer contract that can securely receive and process data from a CRE workflow.

**In this guide:**

1. [Core Concepts: The Onchain Data Flow](#1-core-concepts-the-onchain-data-flow)
2. [The IReceiver Standard](#2-the-ireceiver-standard)
3. [Using ReceiverTemplate](#3-using-receivertemplate)
4. [Working with Simulation](#4-working-with-simulation)
5. [Advanced Usage](#5-advanced-usage-optional)
6. [Complete Examples](#6-complete-examples)
7. [Security Considerations](#7-security-considerations)

## 1. Core Concepts: The Onchain Data Flow

1. **Workflow Execution**: Your workflow [produces a final, signed report](/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain).
2. **EVM Write**: The EVM capability sends this report to the Chainlink-managed `KeystoneForwarder` contract.
3. **Forwarder Validation**: The `KeystoneForwarder` validates the report's signatures.
4. **Callback to Your Contract**: If the report is valid, the forwarder calls a designated function (`onReport`) on your consumer contract to deliver the data.

## 2. The `IReceiver` Standard

To be a valid target for the `KeystoneForwarder`, your consumer contract must satisfy two main requirements:

### 2.1 Implement the `IReceiver` Interface

The `KeystoneForwarder` needs a standardized function to call. This is defined by the `IReceiver` interface, which mandates an `onReport` function.

```sol
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

import {IERC165} from "./IERC165.sol";

/// @title IReceiver - receives keystone reports
/// @notice Implementations must support the IReceiver interface through ERC165.
interface IReceiver is IERC165 {
  /// @notice Handles incoming keystone reports.
  /// @dev If this function call reverts, it can be retried with a higher gas
  /// limit. The receiver is responsible for discarding stale reports.
  /// @param metadata Report's metadata.
  /// @param report Workflow report.
  function onReport(
    bytes calldata metadata,
    bytes calldata report
  ) external;
}
```

- `metadata`: Contains information about the workflow (ID, name, owner). This is encoded by the Forwarder using `abi.encodePacked` with the following structure: `bytes32 workflowId`, `bytes10 workflowName`, `address workflowOwner`.
- `report`: The raw, ABI-encoded data payload from your workflow.

### 2.2 Support ERC165 Interface Detection

[ERC165](https://eips.ethereum.org/EIPS/eip-165) is a standard that allows contracts to publish the interfaces they support. The `KeystoneForwarder` uses this to check if your contract supports the `IReceiver` interface before sending a report.

Link to the `IERC165` interface: [IERC165.sol](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/introspection/IERC165.sol)

## 3. Using `ReceiverTemplate`

### 3.1 Overview

While you can implement these standards manually, we provide an abstract contract, `ReceiverTemplate.sol`, that does the heavy lifting for you. Inheriting from it is the recommended best practice.

**Key features:**

- **Secure by Default**: Requires forwarder address at deployment, ensuring your contract is protected from the start
- **Layered Security**: Add optional workflow ID validation, workflow owner verification, or any combination for defense-in-depth
- **Flexible Configuration**: All permission settings can be updated via setter functions after deployment
- **Simplified Logic**: You only need to implement `_processReport(bytes calldata report)` with your business logic
- **Built-in Access Control**: Includes OpenZeppelin's `Ownable` for secure permission management
- **ERC165 Support**: Includes the necessary `supportsInterface` function
- **Metadata Access**: Helper function to decode workflow ID, name, and owner for custom validation logic

### 3.2 Contract Source Code

```sol
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.0;

import {IERC165} from "./IERC165.sol";
import {IReceiver} from "./IReceiver.sol";
import {Ownable} from "@openzeppelin/contracts/access/Ownable.sol";

/// @title ReceiverTemplate - Abstract receiver with optional permission controls
/// @notice Provides flexible, updatable security checks for receiving workflow reports
/// @dev The forwarder address is required at construction time for security.
///      Additional permission fields can be configured using setter functions.
abstract contract ReceiverTemplate is IReceiver, Ownable {
  // Required permission field at deployment, configurable after
  address private s_forwarderAddress; // If set, only this address can call onReport

  // Optional permission fields (all default to zero = disabled)
  address private s_expectedAuthor; // If set, only reports from this workflow owner are accepted
  bytes10 private s_expectedWorkflowName; // Only validated when s_expectedAuthor is also set
  bytes32 private s_expectedWorkflowId; // If set, only reports from this specific workflow ID are accepted

  // Hex character lookup table for bytes-to-hex conversion
  bytes private constant HEX_CHARS = "0123456789abcdef";

  // Custom errors
  error InvalidForwarderAddress();
  error InvalidSender(address sender, address expected);
  error InvalidAuthor(address received, address expected);
  error InvalidWorkflowName(bytes10 received, bytes10 expected);
  error InvalidWorkflowId(bytes32 received, bytes32 expected);
  error WorkflowNameRequiresAuthorValidation();

  // Events
  event ForwarderAddressUpdated(address indexed previousForwarder, address indexed newForwarder);
  event ExpectedAuthorUpdated(address indexed previousAuthor, address indexed newAuthor);
  event ExpectedWorkflowNameUpdated(bytes10 indexed previousName, bytes10 indexed newName);
  event ExpectedWorkflowIdUpdated(bytes32 indexed previousId, bytes32 indexed newId);
  event SecurityWarning(string message);

  /// @notice Constructor sets msg.sender as the owner and configures the forwarder address
  /// @param _forwarderAddress The address of the Chainlink Forwarder contract (cannot be address(0))
  /// @dev The forwarder address is required for security - it ensures only verified reports are processed
  constructor(
    address _forwarderAddress
  ) Ownable(msg.sender) {
    if (_forwarderAddress == address(0)) {
      revert InvalidForwarderAddress();
    }
    s_forwarderAddress = _forwarderAddress;
    emit ForwarderAddressUpdated(address(0), _forwarderAddress);
  }

  /// @notice Returns the configured forwarder address
  /// @return The forwarder address (address(0) if disabled)
  function getForwarderAddress() external view returns (address) {
    return s_forwarderAddress;
  }

  /// @notice Returns the expected workflow author address
  /// @return The expected author address (address(0) if not set)
  function getExpectedAuthor() external view returns (address) {
    return s_expectedAuthor;
  }

  /// @notice Returns the expected workflow name
  /// @return The expected workflow name (bytes10(0) if not set)
  function getExpectedWorkflowName() external view returns (bytes10) {
    return s_expectedWorkflowName;
  }

  /// @notice Returns the expected workflow ID
  /// @return The expected workflow ID (bytes32(0) if not set)
  function getExpectedWorkflowId() external view returns (bytes32) {
    return s_expectedWorkflowId;
  }

  /// @inheritdoc IReceiver
  /// @dev Performs optional validation checks based on which permission fields are set
  function onReport(
    bytes calldata metadata,
    bytes calldata report
  ) external override {
    // Security Check 1: Verify caller is the trusted Chainlink Forwarder (if configured)
    if (s_forwarderAddress != address(0) && msg.sender != s_forwarderAddress) {
      revert InvalidSender(msg.sender, s_forwarderAddress);
    }

    // Security Checks 2-4: Verify workflow identity - ID, owner, and/or name (if any are configured)
    if (s_expectedWorkflowId != bytes32(0) || s_expectedAuthor != address(0) || s_expectedWorkflowName != bytes10(0)) {
      (bytes32 workflowId, bytes10 workflowName, address workflowOwner) = _decodeMetadata(metadata);

      if (s_expectedWorkflowId != bytes32(0) && workflowId != s_expectedWorkflowId) {
        revert InvalidWorkflowId(workflowId, s_expectedWorkflowId);
      }
      if (s_expectedAuthor != address(0) && workflowOwner != s_expectedAuthor) {
        revert InvalidAuthor(workflowOwner, s_expectedAuthor);
      }

      // ================================================================
      // WORKFLOW NAME VALIDATION - REQUIRES AUTHOR VALIDATION
      // ================================================================
      // Do not rely on workflow name validation alone. Workflow names are unique
      // per owner, but not across owners.
      // Furthermore, workflow names use 40-bit truncation (bytes10), making collisions possible.
      // Therefore, workflow name validation REQUIRES author (workflow owner) validation.
      // The code enforces this dependency at runtime.
      // ================================================================
      if (s_expectedWorkflowName != bytes10(0)) {
        // Author must be configured if workflow name is used
        if (s_expectedAuthor == address(0)) {
          revert WorkflowNameRequiresAuthorValidation();
        }
        // Validate workflow name matches (author already validated above)
        if (workflowName != s_expectedWorkflowName) {
          revert InvalidWorkflowName(workflowName, s_expectedWorkflowName);
        }
      }
    }

    _processReport(report);
  }

  /// @notice Updates the forwarder address that is allowed to call onReport
  /// @param _forwarder The new forwarder address
  /// @dev WARNING: Setting to address(0) disables forwarder validation.
  ///      This makes your contract INSECURE - anyone can call onReport() with arbitrary data.
  ///      Only use address(0) if you fully understand the security implications.
  function setForwarderAddress(
    address _forwarder
  ) external onlyOwner {
    address previousForwarder = s_forwarderAddress;

    // Emit warning if disabling forwarder check
    if (_forwarder == address(0)) {
      emit SecurityWarning("Forwarder address set to zero - contract is now INSECURE");
    }

    s_forwarderAddress = _forwarder;
    emit ForwarderAddressUpdated(previousForwarder, _forwarder);
  }

  /// @notice Updates the expected workflow owner address
  /// @param _author The new expected author address (use address(0) to disable this check)
  function setExpectedAuthor(
    address _author
  ) external onlyOwner {
    address previousAuthor = s_expectedAuthor;
    s_expectedAuthor = _author;
    emit ExpectedAuthorUpdated(previousAuthor, _author);
  }

  /// @notice Updates the expected workflow name from a plaintext string
  /// @param _name The workflow name as a string (use empty string "" to disable this check)
  /// @dev IMPORTANT: Workflow name validation REQUIRES author validation to be enabled.
  ///      The workflow name uses only 40-bit truncation, making collision attacks feasible
  ///      when used alone. However, since workflow names are unique per owner, validating
  ///      both the name AND the author address provides adequate security.
  ///      You must call setExpectedAuthor() before or after calling this function.
  ///      The name is hashed using SHA256 and truncated to bytes10.
  function setExpectedWorkflowName(
    string calldata _name
  ) external onlyOwner {
    bytes10 previousName = s_expectedWorkflowName;

    if (bytes(_name).length == 0) {
      s_expectedWorkflowName = bytes10(0);
      emit ExpectedWorkflowNameUpdated(previousName, bytes10(0));
      return;
    }

    // Convert workflow name to bytes10:
    // SHA256 hash → hex encode → take first 10 chars → hex encode those chars
    bytes32 hash = sha256(bytes(_name));
    bytes memory hexString = _bytesToHexString(abi.encodePacked(hash));
    bytes memory first10 = new bytes(10);
    for (uint256 i = 0; i < 10; i++) {
      first10[i] = hexString[i];
    }
    s_expectedWorkflowName = bytes10(first10);
    emit ExpectedWorkflowNameUpdated(previousName, s_expectedWorkflowName);
  }

  /// @notice Updates the expected workflow ID
  /// @param _id The new expected workflow ID (use bytes32(0) to disable this check)
  function setExpectedWorkflowId(
    bytes32 _id
  ) external onlyOwner {
    bytes32 previousId = s_expectedWorkflowId;
    s_expectedWorkflowId = _id;
    emit ExpectedWorkflowIdUpdated(previousId, _id);
  }

  /// @notice Helper function to convert bytes to hex string
  /// @param data The bytes to convert
  /// @return The hex string representation
  function _bytesToHexString(
    bytes memory data
  ) private pure returns (bytes memory) {
    bytes memory hexString = new bytes(data.length * 2);

    for (uint256 i = 0; i < data.length; i++) {
      hexString[i * 2] = HEX_CHARS[uint8(data[i] >> 4)];
      hexString[i * 2 + 1] = HEX_CHARS[uint8(data[i] & 0x0f)];
    }

    return hexString;
  }

  /// @notice Extracts all metadata fields from the onReport metadata parameter
  /// @param metadata The metadata bytes encoded using abi.encodePacked(workflowId, workflowName, workflowOwner)
  /// @return workflowId The unique identifier of the workflow (bytes32)
  /// @return workflowName The name of the workflow (bytes10)
  /// @return workflowOwner The owner address of the workflow
  function _decodeMetadata(
    bytes memory metadata
  ) internal pure returns (bytes32 workflowId, bytes10 workflowName, address workflowOwner) {
    // Metadata structure (encoded using abi.encodePacked by the Forwarder):
    // - First 32 bytes: length of the byte array (standard for dynamic bytes)
    // - Offset 32, size 32: workflow_id (bytes32)
    // - Offset 64, size 10: workflow_name (bytes10)
    // - Offset 74, size 20: workflow_owner (address)
    assembly {
      workflowId := mload(add(metadata, 32))
      workflowName := mload(add(metadata, 64))
      workflowOwner := shr(mul(12, 8), mload(add(metadata, 74)))
    }
    return (workflowId, workflowName, workflowOwner);
  }

  /// @notice Abstract function to process the report data
  /// @param report The report calldata containing your workflow's encoded data
  /// @dev Implement this function with your contract's business logic
  function _processReport(
    bytes calldata report
  ) internal virtual;

  /// @inheritdoc IERC165
  function supportsInterface(
    bytes4 interfaceId
  ) public pure virtual override returns (bool) {
    return interfaceId == type(IReceiver).interfaceId || interfaceId == type(IERC165).interfaceId;
  }
}
```

### 3.3 Quick Start

The simplest way to use `ReceiverTemplate` is to inherit from it and implement the `_processReport` function:

```sol
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.26;
import {ReceiverTemplate} from "./ReceiverTemplate.sol";

contract MyConsumer is ReceiverTemplate {
  uint256 public s_storedValue;
  event ValueUpdated(uint256 newValue);

  // Constructor requires forwarder address
  constructor(
    address _forwarderAddress
  ) ReceiverTemplate(_forwarderAddress) {}

  // Implement your business logic here
  function _processReport(
    bytes calldata report
  ) internal override {
    uint256 newValue = abi.decode(report, (uint256));
    s_storedValue = newValue;
    emit ValueUpdated(newValue);
  }
}
```

### 3.4 Configuring Permissions

The forwarder address is configured at deployment via the constructor and provides your first line of defense. After deploying your contract, the owner can configure additional security checks or update the forwarder address if needed.


<Aside type="caution" title="For simulation">
  When using `cre workflow simulate`, **do not configure metadata-based validation checks** (`setExpectedWorkflowId`, `setExpectedAuthor`, `setExpectedWorkflowName`). The simulation uses a `MockForwarder` that doesn't provide this metadata. See [Working with Simulation](#4-working-with-simulation) for details.
</Aside>


<Aside type="tip" title="Finding forwarder addresses">
  For a complete list of `KeystoneForwarder` and `MockForwarder` contract addresses on all supported networks, see [Supported Networks](/cre/guides/workflow/using-evm-client/supported-networks).
</Aside>

**Configuration examples:**

```solidity
// Example: Update forwarder address (e.g., when moving from simulation to production)
myConsumer.setForwarderAddress(0xF8344CFd5c43616a4366C34E3EEE75af79a74482); // Ethereum Sepolia KeystoneForwarder

// Example: Add workflow ID check for additional security
myConsumer.setExpectedWorkflowId(0x1234...); // Your specific workflow ID

// Example: Add workflow owner check
myConsumer.setExpectedAuthor(0xYourAddress...);

// Example: Add workflow name check (requires author validation to be set)
myConsumer.setExpectedWorkflowName("my_workflow");

// Example: Disable a check later
myConsumer.setExpectedWorkflowName(""); // Empty string disables the check
```


<Aside type="tip" title="Recommended production setup">
  The forwarder address is required at deployment and provides basic security. For production contracts, we strongly recommend adding additional validation:

  - Use `setExpectedWorkflowId()` if only one workflow writes to your contract (highest security)
  - Use `setExpectedAuthor()` if multiple workflows from the same owner write to your contract
</Aside>

**What the template handles for you:**

- Validates the caller address against the configured forwarder (required at deployment)
- Validates the workflow ID (if `expectedWorkflowId` is configured)
- Validates the workflow owner (if `expectedAuthor` is configured)
- Validates the workflow name (if both `expectedWorkflowName` AND `expectedAuthor` are configured)
- Implements ERC165 interface detection
- Provides access control via OpenZeppelin's `Ownable`
- Calls your `_processReport` function with validated data

**What you implement:**

- Pass the forwarder address to the constructor during deployment
- Your business logic in `_processReport`
- (Optional) Configure additional permissions after deployment using setter functions

#### How workflow names are encoded

The `workflowName` field in the metadata uses the **`bytes10`** type rather than plaintext strings. When you call `setExpectedWorkflowName("my_workflow")`, the `ReceiverTemplate` automatically encodes it using the same algorithm as the CRE engine:

1. Compute SHA256 hash of the workflow name
2. Convert hash to hex string (64 characters)
3. Take the first 10 hex characters (e.g., `"b76f3ae1de"`)
4. Hex-encode those 10 ASCII characters to get `bytes10` (20 hex characters / 10 bytes)

**Example:** `"my_workflow"` → SHA256 → `"b76f3ae1de..."` → hex-encode → `0x62373666336165316465`

This encoding ensures consistent, fixed-size representation regardless of the original workflow name length.


<Aside type="caution" title="Workflow name validation requires author validation">
  Workflow name validation is **only performed when author validation is also configured**. The code enforces this at runtime: if you set `expectedWorkflowName`, you must also set `expectedAuthor`, otherwise the validation will revert with `WorkflowNameRequiresAuthorValidation()`. This prevents the 40-bit collision attack by ensuring workflow names are validated in combination with the owner address. See [Security Considerations](#7-security-considerations) for details.
</Aside>

**Usage:**

```solidity
// Set the expected author first (required)
myConsumer.setExpectedAuthor(0xYourAddress...);

// Then set the expected workflow name (only works with author validation)
myConsumer.setExpectedWorkflowName("my_workflow");

// To disable the workflow name check
myConsumer.setExpectedWorkflowName(""); // Empty string clears the stored value
```

## 4. Working with Simulation

When you run `cre workflow simulate`, your workflow interacts with a **`MockKeystoneForwarder`** contract that does not provide workflow metadata (`workflow_name`, `workflow_owner`).

<Aside type="caution" title="Temporary limitation">
  This is a **temporary limitation** until the `MockKeystoneForwarder` is updated to provide full metadata.
</Aside>

### Deploying for Simulation

When deploying your consumer contract for simulation, pass the **Mock Forwarder address** to the constructor:

```solidity
// Deploy with MockForwarder address for Ethereum Sepolia simulation
address mockForwarder = 0x15fC6ae953E024d975e77382eEeC56A9101f9F88; // Ethereum Sepolia MockForwarder
MyConsumer myConsumer = new MyConsumer(mockForwarder);
```

Find Mock Forwarder addresses for all networks in the [Supported Networks](/cre/guides/workflow/using-evm-client/supported-networks) page.


<Aside type="caution" title="Important: Different addresses for simulation vs production">
  The `MockKeystoneForwarder` address used during simulation is **different** from the `KeystoneForwarder` address used by deployed workflows. After testing with simulation, deploy a new instance with the production `KeystoneForwarder` address, or update the forwarder address using `setForwarderAddress()`. See [Supported Networks](/cre/guides/workflow/using-evm-client/supported-networks) for forwarder addresses.
</Aside>

### Metadata-based validation

**Do not configure these validation checks** during simulation - they require metadata that `MockKeystoneForwarder` doesn't provide:

- `setExpectedWorkflowId()`
- `setExpectedAuthor()`
- `setExpectedWorkflowName()`

Setting any of these will cause your simulation to fail.

### Transitioning to Production

Once you're ready to deploy your workflow to production:

**Option 1: Deploy a new contract instance**

```solidity
// Deploy with production KeystoneForwarder address
address keystoneForwarder = 0xF8344CFd5c43616a4366C34E3EEE75af79a74482; // Ethereum Sepolia
MyConsumer myConsumer = new MyConsumer(keystoneForwarder);

// Configure additional security checks
myConsumer.setExpectedWorkflowId(0xYourWorkflowId);
```

**Option 2: Update existing contract's forwarder**

```solidity
// Update forwarder to production KeystoneForwarder
myConsumer.setForwarderAddress(0xF8344CFd5c43616a4366C34E3EEE75af79a74482); // Ethereum Sepolia

// Add metadata-based validation
myConsumer.setExpectedWorkflowId(0xYourWorkflowId);
```

See [Configuring Permissions](#34-configuring-permissions) for complete details.

## 5. Advanced Usage (Optional)

### 5.1 Custom Validation Logic

You can override `onReport` to add your own validation logic before or after the standard checks:

```solidity
import { ReceiverTemplate } from "./ReceiverTemplate.sol";

contract AdvancedConsumer is ReceiverTemplate {
  uint256 private s_minReportInterval = 1 hours;
  uint256 private s_lastReportTime;

  error ReportTooFrequent(uint256 timeSinceLastReport, uint256 minInterval);

  event MinReportIntervalUpdated(uint256 previousInterval, uint256 newInterval);

  constructor(address _forwarderAddress) ReceiverTemplate(_forwarderAddress) {}

  // Add custom validation before parent's checks
  function onReport(bytes calldata metadata, bytes calldata report) external override {
    // Custom check: Rate limiting
    if (block.timestamp < s_lastReportTime + s_minReportInterval) {
      revert ReportTooFrequent(block.timestamp - s_lastReportTime, s_minReportInterval);
    }

    // Call parent implementation for standard permission checks
    super.onReport(metadata, report);

    s_lastReportTime = block.timestamp;
  }

  function _processReport(bytes calldata report) internal override {
    // Your business logic here
    uint256 value = abi.decode(report, (uint256));
    // ... store or process the value ...
  }

  /// @notice Returns the minimum interval between reports
  /// @return The minimum interval in seconds
  function getMinReportInterval() external view returns (uint256) {
    return s_minReportInterval;
  }

  /// @notice Returns the timestamp of the last report
  /// @return The last report timestamp
  function getLastReportTime() external view returns (uint256) {
    return s_lastReportTime;
  }

  /// @notice Updates the minimum interval between reports
  /// @param _interval The new minimum interval in seconds
  function setMinReportInterval(uint256 _interval) external onlyOwner {
    uint256 previousInterval = s_minReportInterval;
    s_minReportInterval = _interval;
    emit MinReportIntervalUpdated(previousInterval, _interval);
  }
}
```

### 5.2 Using Metadata Fields in Your Logic

The `_decodeMetadata` helper function is available for use in your `_processReport` implementation. This allows you to access workflow metadata for custom business logic:

```solidity
contract MetadataAwareConsumer is ReceiverTemplate {
  mapping(bytes32 => uint256) public s_reportCountByWorkflow;

  constructor(address _forwarderAddress) ReceiverTemplate(_forwarderAddress) {}

  function _processReport(bytes calldata report) internal override {
    // Access the metadata to get workflow ID
    bytes calldata metadata = msg.data[4:]; // Skip function selector
    (bytes32 workflowId, , ) = _decodeMetadata(metadata);

    // Use workflow ID in your business logic
    s_reportCountByWorkflow[workflowId]++;

    // Process the report data
    uint256 value = abi.decode(report, (uint256));
    // ... your logic here ...
  }
}
```


<Aside type="note" title="Advanced access control">
  For production systems requiring even more sophisticated access control (such as role-based permissions or two-step ownership transfer), consider extending the template to use OpenZeppelin's `AccessControl` instead of `Ownable`, or implementing a custom ownership transfer pattern.
</Aside>

## 6. Complete Examples

### Example 1: Simple Consumer Contract

This example inherits from `ReceiverTemplate` to store a temperature value.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.26;
import { ReceiverTemplate } from "./ReceiverTemplate.sol";

contract TemperatureConsumer is ReceiverTemplate {
  int256 public s_currentTemperature;
  event TemperatureUpdated(int256 newTemperature);

  // Constructor requires forwarder address
  constructor(address _forwarderAddress) ReceiverTemplate(_forwarderAddress) {}

  function _processReport(bytes calldata report) internal override {
    int256 newTemperature = abi.decode(report, (int256));
    s_currentTemperature = newTemperature;
    emit TemperatureUpdated(newTemperature);
  }
}
```

**Deployment:**

```solidity
// For simulation: Use MockForwarder address
address mockForwarder = 0x15fC6ae953E024d975e77382eEeC56A9101f9F88; // e.g. Ethereum Sepolia
TemperatureConsumer temperatureConsumer = new TemperatureConsumer(mockForwarder);

// For production: Use KeystoneForwarder address
address keystoneForwarder = 0xF8344CFd5c43616a4366C34E3EEE75af79a74482; // e.g. Ethereum Sepolia
TemperatureConsumer temperatureConsumer = new TemperatureConsumer(keystoneForwarder);
```

**Adding additional security after deployment:**

```solidity
// Add workflow ID check for highest security
temperatureConsumer.setExpectedWorkflowId(0xYourWorkflowId...);
```

### Example 2: The Proxy Pattern

For more complex scenarios, it's best to separate your Chainlink-aware code from your core business logic. The **Proxy Pattern** is a robust architecture that uses two contracts to achieve this:

- **A Logic Contract**: Holds the state and the core functions of your application. It knows nothing about the Forwarder contract or the `onReport` function.
- **A Proxy Contract**: Acts as the secure entry point. It inherits from `ReceiverTemplate` and forwards validated reports to the Logic Contract.

This separation makes your business logic more modular and reusable.

#### The Logic Contract (`ReserveManager.sol`)

This contract, our "vault", holds the state and the `updateReserves` function. For security, it only accepts calls from its trusted Proxy. It also includes an owner-only function to update the proxy address, making the system upgradeable without requiring a migration.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

import { Ownable } from "@openzeppelin/contracts/access/Ownable.sol";

contract ReserveManager is Ownable {
  struct UpdateReserves {
    uint256 ethPrice;
    uint256 btcPrice;
  }

  address private s_proxyAddress;
  uint256 private s_lastEthPrice;
  uint256 private s_lastBtcPrice;
  uint256 private s_lastUpdateTime;

  event ReservesUpdated(uint256 ethPrice, uint256 btcPrice, uint256 updateTime);
  event ProxyAddressUpdated(address indexed previousProxy, address indexed newProxy);

  modifier onlyProxy() {
    require(msg.sender == s_proxyAddress, "Caller is not the authorized proxy");
    _;
  }

  constructor() Ownable(msg.sender) {}

  /// @notice Returns the proxy address
  /// @return The authorized proxy address
  function getProxyAddress() external view returns (address) {
    return s_proxyAddress;
  }

  /// @notice Returns the last ETH price
  /// @return The last recorded ETH price
  function getLastEthPrice() external view returns (uint256) {
    return s_lastEthPrice;
  }

  /// @notice Returns the last BTC price
  /// @return The last recorded BTC price
  function getLastBtcPrice() external view returns (uint256) {
    return s_lastBtcPrice;
  }

  /// @notice Returns the last update timestamp
  /// @return The timestamp of the last update
  function getLastUpdateTime() external view returns (uint256) {
    return s_lastUpdateTime;
  }

  /// @notice Updates the authorized proxy address
  /// @param _proxyAddress The new proxy address
  function setProxyAddress(address _proxyAddress) external onlyOwner {
    address previousProxy = s_proxyAddress;
    s_proxyAddress = _proxyAddress;
    emit ProxyAddressUpdated(previousProxy, _proxyAddress);
  }

  /// @notice Updates the reserve prices
  /// @param data The new reserve data containing ETH and BTC prices
  function updateReserves(UpdateReserves memory data) external onlyProxy {
    s_lastEthPrice = data.ethPrice;
    s_lastBtcPrice = data.btcPrice;
    s_lastUpdateTime = block.timestamp;
    emit ReservesUpdated(data.ethPrice, data.btcPrice, block.timestamp);
  }
}
```

#### The Proxy Contract (`UpdateReservesProxy.sol`)

This contract, our "bouncer", is the only contract that interacts with the Chainlink platform. It inherits `ReceiverTemplate` to validate incoming reports and then calls the `ReserveManager`.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

import { ReserveManager } from "./ReserveManager.sol";
import { ReceiverTemplate } from "./ReceiverTemplate.sol";

contract UpdateReservesProxy is ReceiverTemplate {
  ReserveManager private s_reserveManager;

  constructor(address _forwarderAddress, address reserveManagerAddress) ReceiverTemplate(_forwarderAddress) {
    s_reserveManager = ReserveManager(reserveManagerAddress);
  }

  /// @notice Returns the reserve manager contract address
  /// @return The ReserveManager contract instance
  function getReserveManager() external view returns (ReserveManager) {
    return s_reserveManager;
  }

  /// @inheritdoc ReceiverTemplate
  function _processReport(bytes calldata report) internal override {
    ReserveManager.UpdateReserves memory updateReservesData = abi.decode(report, (ReserveManager.UpdateReserves));
    s_reserveManager.updateReserves(updateReservesData);
  }
}
```

**Configuring permissions after deployment:**

```solidity
// Additional validation can be added after deployment
updateReservesProxy.setExpectedWorkflowId(0xYourWorkflowId...);
```


<Aside type="note" title="KeystoneForwarder address shown">
  The examples above use the Ethereum Sepolia forwarder address. For other networks, see [Supported Networks](/cre/guides/workflow/using-evm-client/supported-networks).
</Aside>

#### How it Works

The deployment and configuration process involves these steps:

1. **Deploy the Logic Contract**: Deploy `ReserveManager.sol`. The wallet that deploys this contract becomes its `owner`.
2. **Deploy the Proxy Contract**: Deploy `UpdateReservesProxy.sol`, passing the forwarder address and the address of the deployed `ReserveManager` contract to its constructor.
3. **Link the Contracts**: The `owner` of the `ReserveManager` contract must call its `setProxyAddress` function, passing in the address of the `UpdateReservesProxy` contract. This authorizes the proxy to call the logic contract.
4. **Configure Permissions** (Recommended): The `owner` of the proxy should call setter functions to enable security checks:
   ```solidity
   updateReservesProxy.setForwarderAddress(0xF8344CFd5c43616a4366C34E3EEE75af79a74482);
   updateReservesProxy.setExpectedWorkflowId(0xYourWorkflowId...);
   ```
5. **Configure Workflow**: In your workflow's `config.json`, use the address of the **Proxy Contract** as the receiver address.
6. **Execution Flow**: When your workflow runs:
   - The Chainlink Forwarder calls `onReport` on your **Proxy**
   - The Proxy validates the report (forwarder address is verified automatically; additional checks like workflow ID can be added)
   - The Proxy's `_processReport` function calls the `updateReserves` function on your **Logic Contract**
   - Because the caller is the trusted proxy, the `onlyProxy` check passes, and your state is securely updated
7. **(Optional) Upgrade**: If you later need to deploy a new proxy, the owner can:
   - Deploy the new proxy contract with the appropriate forwarder address
   - Call `setProxyAddress` on the `ReserveManager` to point it to the new proxy's address
   - Update the workflow configuration to use the new proxy address

#### End-to-End Sequence

## 7. Security Considerations

### Forwarder address

**The forwarder address is the foundation of your contract's security.** The `KeystoneForwarder` contract performs cryptographic verification of DON signatures before calling your consumer. By requiring the forwarder address in the constructor, `ReceiverTemplate` ensures your contract is secure from deployment.


<Aside type="caution" title="Never set forwarder to address(0) in production">
  While the `setForwarderAddress()` function allows updating to `address(0)`, this disables the critical security check and allows **anyone** to call your `onReport()` function with arbitrary data. The function emits a `SecurityWarning` event if you attempt this. Only use `address(0)` for testing if you fully understand the implications.
</Aside>

### Replay protection

The `KeystoneForwarder` contract includes built-in replay protection that prevents successful reports from being executed multiple times. By requiring the forwarder address at construction time, `ReceiverTemplate` ensures your consumer benefits from this protection automatically.


<Aside type="note" title="Failed reports can be retried">
  If a report fails (reverts), the forwarder's replay protection allows it to be retried. This is safe because reverts undo all state changes, ensuring no duplicate effects occur in your contract.
</Aside>

### Additional validation layers

The forwarder address provides baseline security, but you can add additional validation for defense-in-depth:

- **`expectedWorkflowId`**: Ensures only one specific workflow can update your contract. Use this when a single workflow writes to your consumer (highest security for single-workflow scenarios).
- **`expectedAuthor`**: Restricts to workflows owned by a specific address. Use this when multiple workflows from the same owner should access your contract.
- **`expectedWorkflowName`**: Can be used in combination with `expectedAuthor` for additional validation. Requires author validation to be configured. See [Workflow name validation](#workflow-name-validation) below.

### Workflow name validation


<Aside type="caution" title="Workflow name validation requires author validation">
  The `expectedWorkflowName` check in `ReceiverTemplate.onReport()` **requires author validation** to be configured:

  - **Collision Risk**: Workflow names use only 40-bit truncation (bytes10), making collision attacks computationally feasible when used alone
  - **Unique per owner**: Workflow names are unique per owner but not across different owners
  - **Runtime enforcement**: The code enforces that if `expectedWorkflowName` is set, `expectedAuthor` must also be set, otherwise it reverts with `WorkflowNameRequiresAuthorValidation()`

  By combining workflow name (40-bit) with author validation (160-bit address), the contract achieves adequate collision resistance. You can safely use workflow name validation as long as author validation is also enabled.
</Aside>

### Best practices

1. **Always deploy with a valid forwarder address** - The constructor requires this for security. Use `MockForwarder` for simulation, `KeystoneForwarder` for production. Forwarder addresses are available in the [Supported Networks](/cre/guides/workflow/using-evm-client/supported-networks) page.
2. **Add additional validation for production**:
   - **Single workflow**: Use `setExpectedWorkflowId()` to restrict to one specific workflow (highest security)
   - **Multiple workflows from same owner**: Use `setExpectedAuthor()` to restrict to workflows you own
   - **Multiple workflows from different owners**: Implement custom validation logic in your `onReport()` override
3. **Keep your owner key secure** - The owner can update all permission settings
4. **Test permission configurations** - Verify your security settings work as expected before production deployment
5. **Workflow name validation** - Can be used with `setExpectedWorkflowName()` but requires `setExpectedAuthor()` to also be configured for security

---

# Writing Data Onchain
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain
Last Updated: 2025-11-04

This guide shows you how to write data from your CRE workflow to a smart contract on the blockchain using the TypeScript SDK. You'll learn the complete two-step process with examples for both single values and structs.

**What you'll learn:**

- How to ABI-encode data using viem
- How to generate signed reports with `runtime.report()`
- How to submit reports with `evmClient.writeReport()`
- How to handle single values, structs, and complex types

## Prerequisites

Before you begin, ensure you have:

1. **A consumer contract** deployed that implements the `IReceiver` interface
   - See [Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts) if you need to create one
2. **The contract's address** where you want to send data
3. **Basic familiarity** with the [Getting Started tutorial](/cre/getting-started/part-1-project-setup)

<Aside type="note" title="Follow along with Part 4">
  This guide provides detailed explanations for the concepts covered in [Part 4: Writing
  Onchain](/cre/getting-started/part-4-writing-onchain-ts) of the Getting Started tutorial. If you prefer a hands-on
  tutorial, start there!
</Aside>

## Understanding what happens behind the scenes

Before we dive into the code, here's what happens when you call `evmClient.writeReport()`:

1. **Your workflow** generates a signed report containing your ABI-encoded data (via `runtime.report()`)
2. **The EVM Write capability** submits this report to a Chainlink-managed `KeystoneForwarder` contract
3. **The forwarder** validates the report's cryptographic signatures to ensure it came from a trusted DON
4. **The forwarder** calls your consumer contract's `onReport(bytes metadata, bytes report)` function to deliver the data

This is why your consumer contract must implement the `IReceiver` interface—it's not receiving data directly from your workflow, but from the Chainlink Forwarder as an intermediary that provides security and verification.

<Aside type="note" title="Want more details?">
  For a deeper explanation of the secure write flow and why CRE uses this architecture, see the [Onchain Write
  Overview](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts).
</Aside>

## The write pattern

Writing data onchain with the TypeScript SDK follows this pattern:

1. **ABI-encode your data** using viem's `encodeAbiParameters()`
2. **Generate a signed report** using `runtime.report()`
3. **Submit the report** using `evmClient.writeReport()`
4. **Check the transaction status** and handle the result

Let's see how this works for different types of data.

## Writing a single value

This example shows how to write a single `uint256` value to your consumer contract.

### Step 1: Set up your imports

```typescript
import { cre, getNetwork, hexToBase64, bytesToHex, TxStatus, type Runtime } from "@chainlink/cre-sdk"
import { encodeAbiParameters, parseAbiParameters } from "viem"
```

### Step 2: ABI-encode your value

Use viem's `encodeAbiParameters()` to encode a single value:

```typescript
// For a single uint256
const reportData = encodeAbiParameters(parseAbiParameters("uint256"), [12345n])

// For a single address
const reportData = encodeAbiParameters(parseAbiParameters("address"), ["0x1234567890123456789012345678901234567890"])

// For a single bool
const reportData = encodeAbiParameters(parseAbiParameters("bool"), [true])
```

<Aside type="note" title="BigInt literals">
  Use the `n` suffix for integer literals to create `bigint` values in TypeScript. This is required for **all** Solidity
  integer types (`uint8`, `uint256`, `int8`, `int256`, etc.) when using viem: `12345n`, `1000000000000000000n`, etc.
</Aside>

### Step 3: Generate the signed report

Convert the encoded data to base64 and generate a report:

```typescript
const reportResponse = runtime
  .report({
    encodedPayload: hexToBase64(reportData),
    encoderName: "evm",
    signingAlgo: "ecdsa",
    hashingAlgo: "keccak256",
  })
  .result()
```

**Report parameters:**

- `encodedPayload`: Your ABI-encoded data converted to base64
- `encoderName`: Always `"evm"` for EVM chains
- `signingAlgo`: Always `"ecdsa"` for EVM chains
- `hashingAlgo`: Always `"keccak256"` for EVM chains

### Step 4: Submit to the blockchain

```typescript
const writeResult = evmClient
  .writeReport(runtime, {
    receiver: config.consumerAddress,
    report: reportResponse,
    gasConfig: {
      gasLimit: config.gasLimit,
    },
  })
  .result()
```

**WriteReport parameters:**

- `receiver`: The address of your consumer contract (must implement `IReceiver`)
- `report`: The signed report from `runtime.report()`
- `gasConfig.gasLimit`: Gas limit for the transaction (as a string, e.g., `"500000"`)

### Step 5: Check the transaction status

```typescript
if (writeResult.txStatus === TxStatus.SUCCESS) {
  const txHash = bytesToHex(writeResult.txHash || new Uint8Array(32))
  runtime.log(`Transaction successful: ${txHash}`)
  return txHash
}

throw new Error(`Transaction failed with status: ${writeResult.txStatus}`)
```

## Writing a struct

This example shows how to write multiple values as a struct to your consumer contract.

### Your consumer contract

Let's say your consumer contract expects data in this format:

```solidity
struct CalculatorResult {
  uint256 offchainValue;
  int256 onchainValue;
  uint256 finalResult;
}
```

### Step 1: ABI-encode the struct

Use viem to encode all fields as a tuple:

```typescript
const reportData = encodeAbiParameters(
  parseAbiParameters("uint256 offchainValue, int256 onchainValue, uint256 finalResult"),
  [100n, 50n, 150n]
)
```

<Aside type="note" title="Struct encoding">
  In viem, structs are encoded as tuples. List all fields with their types and names, then provide the values in the
  same order. The field names help with readability but don't affect encoding.
</Aside>

### Step 2: Generate and submit

The rest of the process is identical to writing a single value:

```typescript
// Generate signed report
const reportResponse = runtime
  .report({
    encodedPayload: hexToBase64(reportData),
    encoderName: "evm",
    signingAlgo: "ecdsa",
    hashingAlgo: "keccak256",
  })
  .result()

// Submit to blockchain
const writeResult = evmClient
  .writeReport(runtime, {
    receiver: config.consumerAddress,
    report: reportResponse,
    gasConfig: {
      gasLimit: config.gasLimit,
    },
  })
  .result()

// Check status
if (writeResult.txStatus === TxStatus.SUCCESS) {
  runtime.log(`Successfully wrote struct to contract`)
}
```

## Organizing ABIs for reusable data structures

For workflows that interact with consumer contracts multiple times or use complex data structures, organizing your ABI definitions in dedicated files improves code maintainability and type safety.

### Why organize ABIs?

- **Reusability**: Define data structures once, use them across multiple workflows
- **Type safety**: TypeScript can infer types from your ABI definitions
- **Maintainability**: Update contract interfaces in one place
- **Consistency**: Match the pattern used for [reading from contracts](/cre/guides/workflow/using-evm-client/onchain-read-ts)

### File structure

Create a `contracts/abi/` directory in your project root to store ABI definitions:

```
my-cre-project/
├── contracts/
│   └── abi/
│       ├── ConsumerContract.ts    # Consumer contract data structures
│       └── index.ts                # Export all ABIs
├── my-workflow/
│   └── main.ts
└── project.yaml
```

### Creating an ABI file

Let's say your consumer contract expects a `CalculatorResult` struct. Create `contracts/abi/ConsumerContract.ts`:

```typescript
import { parseAbiParameters } from "viem"

// Define the ABI parameters for your struct
export const CalculatorResultParams = parseAbiParameters(
  "uint256 offchainValue, int256 onchainValue, uint256 finalResult"
)

// Define the TypeScript type for type safety
export type CalculatorResult = {
  offchainValue: bigint
  onchainValue: bigint
  finalResult: bigint
}
```

### Creating an index file

For cleaner imports, create `contracts/abi/index.ts`:

```typescript
export { CalculatorResultParams, type CalculatorResult } from "./ConsumerContract"
```

### Using the organized ABI

Now you can import and use these definitions in your workflow:

```typescript
import { cre, getNetwork, hexToBase64, bytesToHex, TxStatus, type Runtime } from "@chainlink/cre-sdk"
import { encodeAbiParameters } from "viem"
import { CalculatorResultParams, type CalculatorResult } from "../contracts/abi"

const writeDataOnchain = (runtime: Runtime<Config>): string => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: runtime.config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // Create type-safe data object
  const data: CalculatorResult = {
    offchainValue: 100n,
    onchainValue: 50n,
    finalResult: 150n,
  }

  // Encode using imported ABI parameters
  const reportData = encodeAbiParameters(CalculatorResultParams, [
    data.offchainValue,
    data.onchainValue,
    data.finalResult,
  ])

  // Generate and submit report (same as before)
  const reportResponse = runtime
    .report({
      encodedPayload: hexToBase64(reportData),
      encoderName: "evm",
      signingAlgo: "ecdsa",
      hashingAlgo: "keccak256",
    })
    .result()

  const writeResult = evmClient
    .writeReport(runtime, {
      receiver: runtime.config.consumerAddress,
      report: reportResponse,
      gasConfig: { gasLimit: runtime.config.gasLimit },
    })
    .result()

  if (writeResult.txStatus === TxStatus.SUCCESS) {
    const txHash = bytesToHex(writeResult.txHash || new Uint8Array(32))
    return txHash
  }

  throw new Error(`Transaction failed`)
}
```

### When to use this pattern

Use organized ABI files when:

- You have **multiple workflows** writing to the same consumer contract
- Your data structures are **complex** (nested structs, arrays, multiple parameters)
- You want **type checking** when constructing data objects
- Your project has **multiple consumer contracts** with different interfaces

For simple, one-off workflows with single values, inline `parseAbiParameters()` is sufficient.

## Complete code example

Here's a full workflow that writes a struct to a consumer contract:

### Configuration (`config.json`)

```json
{
  "schedule": "0 */5 * * * *",
  "chainSelectorName": "ethereum-testnet-sepolia",
  "consumerAddress": "0xYourConsumerContractAddress",
  "gasLimit": "500000"
}
```

### Workflow code (`main.ts`)

```typescript
import { cre, getNetwork, hexToBase64, bytesToHex, TxStatus, type Runtime, Runner } from "@chainlink/cre-sdk"
import { encodeAbiParameters, parseAbiParameters } from "viem"
import { z } from "zod"

// Config schema
const configSchema = z.object({
  schedule: z.string(),
  chainSelectorName: z.string(),
  consumerAddress: z.string(),
  gasLimit: z.string(),
})

type Config = z.infer<typeof configSchema>

const writeDataOnchain = (runtime: Runtime<Config>): string => {
  // Get network info
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: runtime.config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${runtime.config.chainSelectorName}`)
  }

  // Create EVM client
  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // 1. Encode your data (struct with 3 fields)
  const reportData = encodeAbiParameters(
    parseAbiParameters("uint256 offchainValue, int256 onchainValue, uint256 finalResult"),
    [100n, 50n, 150n]
  )

  runtime.log(`Encoded data for consumer contract`)

  // 2. Generate signed report
  const reportResponse = runtime
    .report({
      encodedPayload: hexToBase64(reportData),
      encoderName: "evm",
      signingAlgo: "ecdsa",
      hashingAlgo: "keccak256",
    })
    .result()

  runtime.log(`Generated signed report`)

  // 3. Submit to blockchain
  const writeResult = evmClient
    .writeReport(runtime, {
      receiver: runtime.config.consumerAddress,
      report: reportResponse,
      gasConfig: {
        gasLimit: runtime.config.gasLimit,
      },
    })
    .result()

  // 4. Check status and return
  if (writeResult.txStatus === TxStatus.SUCCESS) {
    const txHash = bytesToHex(writeResult.txHash || new Uint8Array(32))
    runtime.log(`Transaction successful: ${txHash}`)
    return txHash
  }

  throw new Error(`Transaction failed with status: ${writeResult.txStatus}`)
}

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      writeDataOnchain
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Working with complex types

### Arrays

```typescript
// Array of uint256
const reportData = encodeAbiParameters(parseAbiParameters("uint256[]"), [[100n, 200n, 300n]])

// Array of addresses
const reportData = encodeAbiParameters(parseAbiParameters("address[]"), [["0xAddress1", "0xAddress2", "0xAddress3"]])
```

### Nested structs

```typescript
// Struct with nested struct: ReserveData { uint256 total, Asset { address token, uint256 balance } }
const reportData = encodeAbiParameters(parseAbiParameters("uint256 total, (address token, uint256 balance) asset"), [
  1000n,
  ["0xTokenAddress", 500n],
])
```

### Multiple parameters with mixed types

```typescript
// address recipient, uint256 amount, bool isActive
const reportData = encodeAbiParameters(parseAbiParameters("address recipient, uint256 amount, bool isActive"), [
  "0xRecipientAddress",
  42000n,
  true,
])
```

## Type conversions

### JavaScript/TypeScript to Solidity

| Solidity Type            | TypeScript Type            | Example                                |
| ------------------------ | -------------------------- | -------------------------------------- |
| `uint256`, `uint8`, etc. | `bigint`                   | `12345n`                               |
| `int256`, `int8`, etc.   | `bigint`                   | `-12345n`                              |
| `address`                | `string` (hex)             | `"0x1234..."`                          |
| `bool`                   | `boolean`                  | `true`                                 |
| `bytes`, `bytes32`       | `Uint8Array` or hex string | `new Uint8Array(...)` or `"0xabcd..."` |
| `string`                 | `string`                   | `"Hello"`                              |
| Arrays                   | `Array`                    | `[100n, 200n]`                         |
| Struct                   | Tuple                      | `[100n, "0x...", true]`                |

### Helper functions

The SDK provides utilities for data conversion:

```typescript
import { hexToBase64, bytesToHex } from "@chainlink/cre-sdk"

// Convert hex string to base64 (for report generation)
const base64 = hexToBase64(hexString)

// Convert Uint8Array to hex string (for logging, display)
const hex = bytesToHex(uint8Array)
```

## Handling errors

Always check the transaction status and handle potential failures:

```typescript
const writeResult = evmClient
  .writeReport(runtime, {
    receiver: config.consumerAddress,
    report: reportResponse,
    gasConfig: {
      gasLimit: config.gasLimit,
    },
  })
  .result()

// Check for success
if (writeResult.txStatus === TxStatus.SUCCESS) {
  runtime.log(`Success! TxHash: ${bytesToHex(writeResult.txHash || new Uint8Array(32))}`)
} else if (writeResult.txStatus === TxStatus.REVERTED) {
  runtime.log(`Transaction reverted: ${writeResult.errorMessage || "Unknown error"}`)
  throw new Error(`Write failed: ${writeResult.errorMessage}`)
} else if (writeResult.txStatus === TxStatus.FATAL) {
  runtime.log(`Fatal error: ${writeResult.errorMessage || "Unknown error"}`)
  throw new Error(`Fatal write error: ${writeResult.errorMessage}`)
}
```

<Aside type="caution" title="Gas limit configuration">
  Make sure your `gasLimit` is sufficient for your transaction. If it's too low, the transaction will run out of gas and
  revert.
</Aside>

## Next steps

- **[Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts)** - Learn how to create contracts that receive workflow data
- **[EVM Client Reference](/cre/reference/sdk/evm-client-ts)** - Complete API documentation for `EVMClient`
- **[Part 4: Writing Onchain](/cre/getting-started/part-4-writing-onchain-ts)** - Hands-on tutorial

---

# API Interactions
Source: https://docs.chain.link/cre/guides/workflow/using-http-client
Last Updated: 2025-11-04

The CRE SDK provides an HTTP client that allows your workflows to interact with external APIs. Use it to fetch offchain data, send results to other systems, or trigger external events.

These guides will walk you through the common use cases for the HTTP client.

## Guides

- **[Making GET Requests](/cre/guides/workflow/using-http-client/get-request)**: Learn how to fetch data from a public API using a `GET` request.
- **[Making POST Requests](/cre/guides/workflow/using-http-client/post-request)**: Learn how to send data to an external endpoint using a `POST` request.
- **[Submitting Reports via HTTP](/cre/guides/workflow/using-http-client/submitting-reports-http)**: Learn how to submit cryptographically signed reports to an external HTTP endpoint.

---

# Managing Secrets
Source: https://docs.chain.link/cre/guides/workflow/secrets
Last Updated: 2025-11-04

Secrets are sensitive values like API keys, private keys, database URLs, and authentication tokens that your workflow needs to access at runtime. CRE provides different approaches for managing secrets depending on whether you're developing locally or running workflows in production.

This guide helps you choose the right approach for your use case.

## Which guide do I need?

Your workflow environment determines how you manage secrets:

### 1. Local development and simulation

**When to use:** You're testing and debugging workflows on your local machine using `cre workflow simulate`.

**How it works:**

- Secrets declared in `secrets.yaml`
- Values provided via `.env` file or environment variables
- Secrets injected locally by the CLI
- **No Vault DON required**

**→ Follow this guide:** [Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation)

### 2. Deployed workflows

**When to use:** Your workflow is deployed to the Workflow DON.

**How it works:**

- Secrets stored in the **Vault DON** (decentralized secret storage)
- Managed via `cre secrets` CLI commands (`create`, `update`, `delete`, `list`)
- Your workflow retrieves secrets from the Vault at runtime
- **Vault DON required**

**→ Follow this guide:** [Using Secrets with Deployed Workflows](/cre/guides/workflow/secrets/using-secrets-deployed)

### 3. Secure secret management (Best practice)

**When to use:** Any environment where you want to avoid storing secrets in plaintext `.env` files.

**How it works:**

- Use **1Password CLI** to store and inject secrets
- Secrets never stored in plaintext on your filesystem
- Works for both simulation and production

**→ Follow this guide:** [Managing Secrets with 1Password CLI](/cre/guides/workflow/secrets/managing-secrets-1password)

## Quick comparison

| Aspect             | Local Simulation                     | Deployed Workflows                 |
| ------------------ | ------------------------------------ | ---------------------------------- |
| **Environment**    | Your local machine                   | Workflow DON                       |
| **Secret storage** | `.env` file or environment variables | Vault DON                          |
| **CLI commands**   | None (automatic via simulation)      | `cre secrets create/update/delete` |
| **Workflow code**  | `runtime.GetSecret()`                | `runtime.GetSecret()` (same API)   |
| **Authentication** | Not required                         | `cre login` required               |
| **Use case**       | Development and testing              | Deployed workflows                 |

## How secrets work in your workflow

Regardless of where secrets are stored (locally or in the Vault), your workflow code uses the same API to access them:

The CRE runtime automatically handles retrieving the secret from the appropriate source based on your environment.

## Getting started

1. **For local development:** Start with [Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation) to learn the basics
2. **For deployed workflows:** Once your workflow is ready to deploy, follow [Using Secrets with Deployed Workflows](/cre/guides/workflow/secrets/using-secrets-deployed)
3. **For enhanced security:** Implement [1Password CLI integration](/cre/guides/workflow/secrets/managing-secrets-1password) to eliminate plaintext secrets

## Reference

For detailed CLI command documentation, see:

- [Secrets Management CLI Reference](/cre/reference/cli/secrets) — Complete documentation for `cre secrets` commands

---

# Using Secrets with Deployed Workflows
Source: https://docs.chain.link/cre/guides/workflow/secrets/using-secrets-deployed
Last Updated: 2025-11-04

When your workflow is deployed, it cannot access your local `.env` file or environment variables. Instead, secrets must be stored in the **Vault DON**—a decentralized, secure secret storage system that your deployed workflows can access at runtime.

This guide explains how to manage secrets for deployed workflows using the `cre secrets` CLI commands.


<Aside type="note" title="For local simulation">
  If you're developing and testing locally with `cre workflow simulate`, you don't need the Vault DON. See [Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation) instead.
</Aside>

## Prerequisites

Before managing secrets for deployed workflows, ensure you have:

1. **CRE CLI installed**: See the [Installation Guide](/cre/getting-started/cli-installation/macos-linux)
2. **Authentication**: You must be logged in with `cre login`
3. **Owner address configured**: Your `workflow-owner-address` must be set in your project configuration

## How secrets work with deployed workflows

The workflow is similar to local development, but with a critical difference in where secrets are stored:

1. **Declare**: Define secret identifiers in a YAML file
2. **Store**: Push secrets to the Vault DON using `cre secrets create`
3. **Use**: Your deployed workflow accesses secrets from the Vault using `runtime.GetSecret()`

**Key difference from simulation:**

- **Local simulation**: Secrets read from your environment variables or `.env` file on your machine
- **Deployed workflows**: Secrets retrieved from Vault DON by the workflow

## Step-by-step guide

### Step 1: Create a secrets YAML file

Create a YAML file at the root of your project that declares the secrets you want to store.

**Example `production-secrets.yaml`:**

```yaml
secretsNames:
  API_KEY:
    - API_KEY_VALUE

  DATABASE_URL:
    - DATABASE_URL_VALUE
```

**Structure:**

- `secretsNames` — Top-level key containing all secrets
- Each secret has:
  - **Key** (e.g., `API_KEY`) — The identifier your workflow code will use
  - **Value** — An array containing the environment variable name that holds the actual value


<Aside type="caution" title="Don't commit secrets files">
  Add your production secrets YAML files to `.gitignore`. Never commit them to version control.
</Aside>

### Step 2: Provide secret values as environment variables

Set the actual secret values as environment variables. These can be provided in two ways:

**Option A: Export in your shell**

```bash
export API_KEY_VALUE="your-actual-api-key"
export DATABASE_URL_VALUE="postgresql://user:pass@host:5432/db"
```

**Option B: Use a `.env` file**

Create a `.env` file (or add to your existing one):

```bash
# .env
API_KEY_VALUE=your-actual-api-key
DATABASE_URL_VALUE=postgresql://user:pass@host:5432/db
```

The `cre` CLI will automatically load variables from `.env` when you run the commands.


<Aside type="tip" title="Use 1Password CLI">
  For enhanced security, use [1Password CLI](/cre/guides/workflow/secrets/managing-secrets-1password) to inject secrets without storing them in plaintext files.
</Aside>

### Step 3: Upload secrets to the Vault DON

Use the `cre secrets create` command to upload your secrets to the Vault:

```bash
cre secrets create production-secrets.yaml --target production-settings
```

**What happens:**

1. The CLI reads your YAML file and environment variables
2. It registers the request onchain (for authorization)
3. It submits the secrets to the Vault DON
4. The secrets are stored securely and associated with your owner address

**Example output:**

```bash
{"level":"info","owner":"<your-owner-address>","digest":"041eb7a8...","time":"2025-10-22T00:14:56+02:00","message":"IsRequestAllowlisted query succeeded"}
{"level":"info","digest":"041eb7a8...","deadline":"2025-10-23T22:14:56Z","time":"2025-10-22T00:14:59+02:00","message":"AllowlistRequest submitted"}
Digest allowlisted; proceeding to gateway POST
Secret created: secret_id=API_KEY, owner=<your-owner-address>, namespace=main
Secret created: secret_id=DATABASE_URL, owner=<your-owner-address>, namespace=main
```


<Aside type="note" title="Default timeout">
  Secrets operations have a 48-hour timeout by default. You can customize this with the `--timeout` flag (e.g., `--timeout 1h` or `--timeout 7d`).
</Aside>

### Step 4: Use secrets in your workflow code

Your workflow code uses the same API to access secrets, whether running in local simulation or deployed to a workflow DON. The CRE runtime automatically retrieves secrets from the appropriate source.


<Aside type="note" title="Namespace parameter">
  The `namespace` parameter is **optional** and only relevant for deployed workflows using the Vault DON. If omitted, it defaults to `"main"`. For local simulation, the namespace parameter is ignored—secrets are read from your `.env` file regardless.
</Aside>

**Important:**

- The secret identifier (`"API_KEY"`) must match what you declared in your YAML file
- Secrets are fetched at runtime from the Vault DON
- The namespace parameter is optional—defaults to `"main"` if omitted
- The same code works for both simulation (reads from `.env`) and production (reads from Vault)

### Step 5: Verify secrets are stored

You can list all secrets stored in the Vault for your owner address:

```bash
cre secrets list --target production-settings
```

**Example output:**

```
{"level":"info","owner":"<your-owner-address>","digest":"225d8b6f...","time":"2025-10-22T19:10:12-05:00","message":"IsRequestAllowlisted query succeeded"}
{"level":"info","digest":"225d8b6f...","deadline":"2025-10-25T00:10:12Z","time":"2025-10-22T19:10:16-05:00","message":"AllowlistRequest submitted"}

Digest allowlisted; proceeding to gateway POST: owner=<your-owner-address>, requestID=f9148fcb-3e4e-45bf-bbde-2124ddd577e4, digest=0x225d8b6f...
Secret identifier: secret_id=API_KEY, owner=<your-owner-address>, namespace=main
Secret identifier: secret_id=DATABASE_URL, owner=<your-owner-address>, namespace=main
```


<Aside type="note" title="Security: List only shows IDs">
  The `list` command only shows secret identifiers, never the actual secret values. Secret values are only accessible to your deployed workflows at runtime.
</Aside>

## Managing secrets lifecycle

### Updating secrets

To update existing secrets, use the `cre secrets update` command:

```bash
# Update your environment variable with the new value
export API_KEY_VALUE="new-api-key-value"

# Update the secret in the Vault
cre secrets update production-secrets.yaml --target production-settings
```

**Example output:**

```
{"level":"info","owner":"<your-owner-address>","digest":"10854ac2...","time":"2025-10-22T19:12:32-05:00","message":"IsRequestAllowlisted query succeeded"}
{"level":"info","digest":"10854ac2...","deadline":"2025-10-25T00:12:32Z","time":"2025-10-22T19:12:40-05:00","message":"AllowlistRequest submitted"}

Digest allowlisted; proceeding to gateway POST: owner=<your-owner-address>, requestID=7433514f-4008-46dd-822a-633732b64ec9, digest=0x10854ac2...
Secret updated: secret_id=API_KEY, owner=<your-owner-address>, namespace=main
Secret updated: secret_id=DATABASE_URL, owner=<your-owner-address>, namespace=main
```


<Aside type="note" title="Update vs. Create">
  - `create` adds new secrets
  - `update` modifies existing secrets
  - You cannot update a secret that doesn't exist—use `create` first
</Aside>

### Deleting secrets

To remove secrets from the Vault:

**Step 1: Create a deletion YAML file** (`secrets-to-delete.yaml`):

```yaml
secretsNames:
  - API_KEY
  - DATABASE_URL
```


<Aside type="note" title="Delete uses a simpler YAML format">
  Unlike `create` and `update` which map secret IDs to environment variables, the `delete` command uses a simple list of secret IDs under `secretsNames`.
</Aside>

**Step 2: Run the delete command:**

```bash
cre secrets delete secrets-to-delete.yaml --target production-settings
```


<Aside type="caution" title="Permanent deletion">
  Deleting secrets is permanent and cannot be undone. Double-check your secret identifiers before running this command.
</Aside>

## About namespaces

When you look at CLI outputs, you'll notice secrets are organized by **namespaces**. A namespace is simply a way to group related secrets together.


<Aside type="note" title="Default namespace">
  In the current CLI version, all secrets operations (`create`, `update`, `delete`) automatically use the default `"main"` namespace. This is why you see `namespace=main` in the output. Custom namespace support for these commands may be added in future CLI versions.
</Aside>

## Using with multi-sig wallets

All `cre secrets` commands support the `--unsigned` flag for multi-sig wallet operations. This generates raw transaction data instead of sending transactions directly.

For complete multi-sig setup and usage, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Troubleshooting

### "Secret not found" error in deployed workflow

**Problem:** Your workflow throws a "secret not found" error when calling `runtime.GetSecret()`.

**Solution:**

1. Verify the secret exists: `cre secrets list --target production-settings`
2. Check that the secret ID in your code matches exactly
3. Recreate the secret if necessary: `cre secrets create ...`

### "Timeout expired" error

**Problem:** The CLI returns a timeout error when creating/updating secrets.

**Solution:**
The onchain authorization has expired. Re-run the command to create a new authorization.

### Different secrets for simulation vs. production

**Problem:** You want different secret values when simulating vs. running in production.

**Solution:**

- For simulation: Store values in your local `.env` file
- For production: Use `cre secrets create` with different values
- The secret IDs stay the same—only the values differ

## Learn more

- **[Secrets CLI Reference](/cre/reference/cli/secrets)** — Complete CLI command documentation
- **[Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation)** — For local development
- **[Managing Secrets with 1Password](/cre/guides/workflow/secrets/managing-secrets-1password)** — Best practice for secure secret management
- **[Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets)** — For multi-sig secret operations

---

# Managing Secrets with 1Password CLI
Source: https://docs.chain.link/cre/guides/workflow/secrets/managing-secrets-1password
Last Updated: 2025-11-04

While using a `.env` file or exporting environment variables is convenient for initial testing, the recommended best practice for managing sensitive data like private keys and API tokens is to use a dedicated secrets manager.

This guide explains how to use **1Password CLI** to securely inject secrets into your workflow's environment at runtime, ensuring your secrets are never stored in plaintext on your filesystem.


<Aside type="tip" title="Works for both simulation and production">
  This approach works for:

  - **Local simulation**: Inject secrets when running `cre workflow simulate`
  - **Deployed workflows**: Inject secrets when running `cre secrets create/update` to upload to the Vault DON
</Aside>

## Prerequisites

Before you begin, ensure you have:

1. **Installed 1Password CLI:** Follow the [1Password CLI installation guide](https://developer.1password.com/docs/cli/get-started/).
2. **Stored Your Secret in 1Password:** Save the secret you need (e.g., your `CRE_ETH_PRIVATE_KEY`) in a vault that your 1Password CLI is configured to access.

## Step 1: Get the secret reference

A secret reference is a unique URI that points to a specific field in an item in your 1Password vault.

1. Open the 1Password desktop app.
2. Find the item containing your secret.
3. Right-click on the specific field (e.g., the `private key` field).
4. Select **Copy Secret Reference**.

Your clipboard will now contain a reference, which is a safe, non-secret string that looks like this: `op://<vault-name>/<item-name>/<field-name>`

## Step 2: Use the secret reference in your `.env` file

Open your project's `.env` file and replace the plaintext secret with the secret reference you just copied.

**Before:**

```bash
# .env
CRE_ETH_PRIVATE_KEY=0x123...abc
```

**After:**

```bash
# .env
CRE_ETH_PRIVATE_KEY="op://Private/Sepolia-Dev-Key/private key"
```

## Step 3: Run commands with `op run`

The `op run` command is a utility that loads the secrets from your references into the environment and then executes your command, ensuring the secrets only exist in memory for the duration of the process.

### For local simulation

To run your workflow simulation, prefix your command with `op run --env-file ../.env --`:

```bash
op run --env-file ../.env -- cre workflow simulate my-workflow --target staging-settings
```

### For deployed workflows

To upload secrets to the Vault DON, use the same pattern:

```bash
op run --env-file .env -- cre secrets create production-secrets.yaml --target production-settings
```

**What's happening here?**

- `op run` scans the `.env` file for any `op://` references.
- It securely authenticates with 1Password to fetch the real secret values.
- It injects these values as environment variables into a new, temporary sub-shell.
- It then executes your `cre` command within that secure sub-shell.
- When the command finishes, the sub-shell is destroyed, and the secrets vanish from the environment.

By following this pattern, you can manage your secrets securely without ever exposing them in plaintext. For more advanced use cases, see the official [1Password CLI documentation](https://developer.1password.com/docs/cli/secret-references).

---

# Simulating Workflows
Source: https://docs.chain.link/cre/guides/operations/simulating-workflows
Last Updated: 2025-11-04

Workflow simulation is a local execution environment that **compiles your workflow to <a href="https://webassembly.org" target="blank" ref="noopener noreferrer">WebAssembly (WASM)</a>** and runs it on **your machine**. It allows you to test and debug your workflow logic before deploying it. The simulator makes real calls to public testnets and live HTTP endpoints, giving you high confidence that your code will work as expected when deployed.

## When to use simulation

Use workflow simulation to:

- **Test workflow logic during development**: Validate that your code behaves correctly before deploying.
- **Debug errors in a controlled environment**: Catch and fix issues locally without deploying to a live network.
- **Test different trigger types**: Manually select and test how your workflow responds to [cron](/cre/guides/workflow/using-triggers/cron-trigger), [HTTP](/cre/guides/workflow/using-triggers/http-trigger/overview), or [EVM log](/cre/guides/workflow/using-triggers/evm-log-trigger) triggers.
- **Verify onchain interactions**: Test [read](/cre/guides/workflow/using-evm-client/onchain-read) and [write](/cre/guides/workflow/using-evm-client/onchain-write/overview) operations against real testnets.

## Basic usage

The `cre workflow simulate` command compiles your workflow and executes it locally.

**Basic syntax:**

```bash
cre workflow simulate <workflow-name-or-path> [flags]
```

**Example:**

```bash
cre workflow simulate my-workflow --target staging-settings
```

### What happens during simulation

1. **Compilation**: The CLI compiles your workflow to WebAssembly (WASM).
2. **Trigger selection**: You're prompted to select which trigger to test (cron, HTTP, or EVM log).
3. **Execution**: The workflow runs locally, making real calls to configured RPCs and HTTP endpoints.
4. **Output**: The simulator displays logs from your workflow and the final execution result.

### Prerequisites

Before running a simulation:

- **CRE account & authentication**: You must have a CRE account and be logged in with the CLI. See [Create your account](/cre/account/creating-account) and [Log in with the CLI](/cre/account/cli-login) for instructions.
- **CRE CLI installed**: You must have the CRE CLI installed on your machine. See [CLI Installation](/cre/getting-started/cli-installation) for instructions.
- **Project configuration**: You must run the command from your project root directory.
- **Valid `workflow.yaml`**: Your workflow directory must contain a `workflow.yaml` file with correct paths to your workflow code, config, and secrets (optional).
- **RPC URLs configured**: If your workflow interacts with blockchains, configure RPC endpoints in your `project.yaml` for the target you're using. Without this, the simulator cannot register the EVM capability and your workflow will fail. See [Project Configuration](/cre/reference/project-configuration) for setup instructions.
- **Private key**: Set `CRE_ETH_PRIVATE_KEY` in your `.env` file if your workflow performs onchain writes.

<Aside type="tip" title="New to CRE?">
  If these prerequisites seem overwhelming, we strongly recommend starting with the [Getting Started
  guide](/cre/getting-started/overview). It walks you through setting up your first project, configuring your
  environment, and running your first simulation step by step.
</Aside>

## Interactive vs non-interactive modes

### Interactive mode (default)

In interactive mode, the simulator prompts you to select a trigger and provide necessary inputs.

**Example:**

```bash
cre workflow simulate my-workflow --target staging-settings
```

**What you'll see:**

```
Workflow compiled

🚀 Workflow simulation ready. Please select a trigger:
1. cron-trigger@1.0.0 Trigger
2. http-trigger@1.0.0-alpha Trigger
3. evm:ChainSelector:16015286601757825753@1.0.0 LogTrigger

Enter your choice (1-3):
```

Select a trigger by entering its number, and follow any additional prompts for trigger-specific inputs.

### Non-interactive mode

Non-interactive mode allows you to run simulations without prompts, making it ideal for CI/CD pipelines or automated testing.

**Requirements:**

- Use the `--non-interactive` flag
- Specify `--trigger-index` (0-based index of the trigger to run)
- Provide trigger-specific flags as needed (see [Trigger-specific configuration](#trigger-specific-configuration))

**Example:**

```bash
cre workflow simulate my-workflow --non-interactive --trigger-index 0 --target staging-settings
```

## The `--broadcast` flag

By default, the simulator performs a **dry run** for onchain write operations. It prepares the transaction but does not broadcast it to the blockchain.

To actually broadcast transactions during simulation, use the `--broadcast` flag:

```bash
cre workflow simulate my-workflow --broadcast --target staging-settings
```


<Aside type="caution" title="Broadcast requires a funded wallet">
  When using `--broadcast`, your wallet (specified by `CRE_ETH_PRIVATE_KEY` in `.env`) must be funded with the native
  token of the chain you're writing to (e.g., Sepolia ETH for Ethereum Sepolia testnet). Visit
  <a href="https://faucets.chain.link" target="blank">faucets.chain.link</a> to get your testnet tokens.
</Aside>

**Use case:** Use `--broadcast` when you want to test the complete end-to-end flow, including actual onchain state changes, on a testnet.

## Trigger-specific configuration

Different trigger types require different inputs for simulation.

### Cron trigger

[Cron triggers](/cre/guides/workflow/using-triggers/cron-trigger) do not require additional configuration. When selected, they execute immediately.

<Aside type="note" title="Single trigger workflows">
  If your workflow has only one trigger, the simulator will automatically run it without prompting you to select.
</Aside>

**Interactive example:**

```bash
cre workflow simulate my-workflow --target staging-settings
```

Select the cron trigger when prompted (if multiple triggers are defined)

**Non-interactive example:**

```bash
# Assuming the cron trigger is the first trigger defined in your workflow (index 0)
cre workflow simulate my-workflow --non-interactive --trigger-index 0 --target staging-settings
```

<Aside type="tip" title="Finding your trigger index">
  The `--trigger-index` is 0-based and corresponds to the order in which triggers are defined in your workflow code. The
  first trigger is index `0`, the second is `1`, and so on. If you're unsure which index to use, run the simulator in
  interactive mode first to see the list of triggers.
</Aside>

### HTTP trigger

[HTTP triggers](/cre/guides/workflow/using-triggers/http-trigger/configuration) require a JSON payload.

**Interactive mode:**

When you select an HTTP trigger, the simulator prompts you to provide JSON input. You can:

- Enter the JSON directly
- Provide a file path (e.g., `./payload.json`)

**Non-interactive mode:**

Use the `--http-payload` flag with:

- A JSON string: `--http-payload '{"key":"value"}'`
- A file path: `--http-payload @./payload.json` (with or without `@` prefix)

**Example:**

```bash
cre workflow simulate my-workflow --non-interactive --trigger-index 1 --http-payload @./http_trigger_payload.json --target staging-settings
```

<Aside type="tip" title="Detailed HTTP trigger testing guide">
  For a complete guide with workflow examples, test scenarios, and expected output, see [Testing HTTP Triggers in
  Simulation](/cre/guides/workflow/using-triggers/http-trigger/testing-in-simulation).
</Aside>

### EVM log trigger

[EVM log triggers](/cre/guides/workflow/using-triggers/evm-log-trigger) require a transaction hash and event index to fetch a specific log event from the blockchain.

**Interactive mode:**

When you select an EVM log trigger, the simulator prompts you for:

1. **Transaction hash** (e.g., `0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e`)
2. **Event index** (0-based index of the log in the transaction, e.g., `0`)

The simulator fetches the log from the configured RPC and passes it to your workflow.

**Non-interactive mode:**

Use the `--evm-tx-hash` and `--evm-event-index` flags:

```bash
cre workflow simulate my-workflow \
  --non-interactive \
  --trigger-index 2 \
  --evm-tx-hash 0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e \
  --evm-event-index 0 \
  --target staging-settings
```

## Additional flags

### `--engine-logs` (`-g`)

Enables detailed engine logging for debugging purposes. This shows internal logs from the workflow execution engine.

```bash
cre workflow simulate my-workflow --engine-logs --target staging-settings
```

### `--target` (`-T`)

Specifies which target environment to use from your configuration files. This determines which RPC URLs, settings, and secrets are loaded.

```bash
cre workflow simulate my-workflow --target staging-settings
```

### `--verbose` (`-v`)

Enables debug-level logging for the CLI itself (not the workflow). Useful for troubleshooting CLI issues.

```bash
cre workflow simulate my-workflow --verbose --target staging-settings
```

## Understanding the output

When you run a simulation, you'll see the following output:

### 1. Compilation confirmation

```
Workflow compiled
```

This indicates your workflow was successfully compiled to WASM.

### 2. Trigger selection menu (interactive mode only)

If your workflow has multiple triggers, you'll see a menu:

```
🚀 Workflow simulation ready. Please select a trigger:
1. cron-trigger@1.0.0 Trigger
2. http-trigger@1.0.0-alpha Trigger
3. evm:ChainSelector:16015286601757825753@1.0.0 LogTrigger

Enter your choice (1-3):
```

If your workflow has only one trigger, it will run automatically without this prompt.

### 3. User logs

Logs from your workflow code (e.g., `logger.Info()` calls) appear with timestamps:

```
2025-10-24T19:07:27Z [USER LOG] Running CronTrigger
2025-10-24T19:07:27Z [USER LOG] fetching por url https://api.example.com
2025-10-24T19:07:27Z [USER LOG] ReserveInfo { "totalReserve": 494515082.75 }
```

### 4. Final execution result

The simulator displays the value returned by your workflow:

```
Workflow Simulation Result:
 {
  "result": 47
}
```

### 5. Transaction details (if your workflow writes onchain)

If your workflow performs onchain writes, the simulator will show transaction information:

**Without `--broadcast` (dry run):**

The transaction is prepared but not sent. You'll see a zero address (`0x0000...`) as the transaction hash:

```
2025-10-24T23:01:50Z [USER LOG] Write report transaction succeeded: 0x0000000000000000000000000000000000000000000000000000000000000000
```

**With `--broadcast`:**

The transaction is actually sent to the blockchain. You'll see a real transaction hash:

```
2025-10-24T17:55:48Z [USER LOG] Write report transaction succeeded: 0x1013abc0b6f345fad15b19a56cabbbaab2a2aa94f81eb3a709058adf18a4f23f
```

## Limitations

While simulation provides high confidence in your workflow's behavior, it has some limitations:

- **Single-node execution**: Simulation runs on a single node (your local machine) rather than across a DON. There is no actual consensus or quorum, it is simulated.
- **Manual trigger execution**: Time-based triggers (cron) execute immediately when selected, not on a schedule. You must manually initiate each simulation run.
- **Simplified environment**: The simulation environment mimics production but is not identical. Some edge cases or network conditions may only appear in a deployed environment.

Despite these limitations, simulation is an essential tool for catching bugs, validating logic, and testing integrations before deploying to production.

## Next steps

- **Deploy your workflow**: Once you're confident your workflow works correctly, see [Deploying Workflows](/cre/guides/operations/deploying-workflows).
- **CLI reference**: For a complete list of flags and options, see the [CLI Workflow Commands reference](/cre/reference/cli/workflow/).

---

# Deploying Workflows
Source: https://docs.chain.link/cre/guides/operations/deploying-workflows
Last Updated: 2025-11-04

<Aside type="note" title="Early Access required">
  Workflow deployment is currently in Early Access. To <a href="https://cre.chain.link/request-access" target="_blank" rel="noopener noreferrer">request Early Access</a>, please share details about your project and use case—this helps us provide better support as you build with CRE.

  **While you wait:** Continue building and simulating workflows using [`cre workflow simulate`](/cre/guides/operations/simulating-workflows).
</Aside>

When you deploy a workflow, you take your locally tested code and register it with the onchain Workflow Registry contract. This makes your workflow "live" so it can activate and respond to triggers across a [Decentralized Oracle Network (DON)](/cre/key-terms#decentralized-oracle-network-don).

## Prerequisites

Before you can deploy a workflow, you must have:

- **Early Access approval**: Workflow deployment is currently in Early Access. <a href="https://cre.chain.link/request-access" target="_blank" rel="noopener noreferrer">Request access here</a> if you haven't already.
- **[Logged in](/cre/reference/cli/authentication#cre-login)**: Authenticated with the platform by running `cre login`. To check if you are logged in, run `cre whoami`.
- **[Linked your key](/cre/reference/cli/account#cre-account-link-key)**: Linked your EOA or multi-sig wallet to your account by running `cre account link-key`.
- **A funded wallet**: The account you are deploying from must be funded with ETH on Ethereum Mainnet to pay the gas fees for the onchain registration transaction to the Workflow Registry contract.

## The deployment process

The `cre workflow deploy` command handles the entire end-to-end process for you:

1. **Compiles** your workflow to a WASM binary.
2. **Uploads** the compiled binary and any associated configuration files (like your config file or `secrets.yaml`) to the CRE Storage Service.
3. **Registers** the workflow onchain by submitting a transaction to the Workflow Registry contract on **Ethereum Mainnet**. Your wallet must have ETH for gas fees, and your `project.yaml` must include an RPC configuration for `ethereum-mainnet` (e.g., `https://ethereum-rpc.publicnode.com`). This transaction contains the metadata for your workflow, including its name, owner, and the URL of its artifacts in the storage service.


<Aside type="caution" title="Early Access: Disclaimer">
  Chainlink Runtime Environment (CRE) deployment is in the "Early Access" stage of development, which means that CRE currently has functionality which is under development and may be changed in later versions. By using CRE, you expressly acknowledge and agree to accept the Chainlink <a href="https://chain.link/terms" target="_blank" rel="noopener noreferrer">Terms of Service</a>, which provides important information and disclosures.
</Aside>

### Step 1: Ensure your configuration is correct

Before deploying, ensure your `workflow.yaml` file is correctly configured. The `workflow-name` is required under the `user-workflow` section for your target environment.

If you are deploying from a multi-sig wallet, specify your multi-sig address in the `workflow-owner-address` field. If you are deploying from a standard EOA, you can leave this field unchanged—the owner will be automatically derived from the `CRE_ETH_PRIVATE_KEY` in your `.env` file.

For more details on configuration, see the [Project Configuration](/cre/reference/project-configuration) reference.

### Step 2: Run the deploy command

**From your project root directory**, run the `deploy` command with the path to your workflow folder.

```bash
cre workflow deploy <workflow-folder-path> [flags]
```

Example command to target the `production-settings` environment:

```bash
cre workflow deploy my-workflow --target production-settings
```

**Available flags:**

| Flag             | Description                                                                             |
| ---------------- | --------------------------------------------------------------------------------------- |
| `--target`       | Sets the target environment from your configuration files (e.g., `production-settings`) |
| `--output`       | The output file for the compiled WASM binary (default: `"./binary.wasm.br.b64"`)        |
| `--unsigned`     | Return the raw transaction instead of broadcasting it to the network                    |
| `--yes`          | Skip confirmation prompts and proceed with the operation                                |
| `--project-root` | Path to the project root directory                                                      |
| `--env`          | Path to your `.env` file (default: `".env"`)                                            |
| `--verbose`      | Enable verbose logging to print `DEBUG` level logs                                      |

### Step 3: Monitor the output

The CLI will provide detailed logs of the deployment process, including the compilation, upload to the CRE Storage Service, and the final onchain transaction.

```bash
> cre workflow deploy my-workflow --target production-settings

Deploying Workflow :     my-workflow
Target :                 production-settings
Owner Address :          <your-owner-address>

Compiling workflow...
Workflow compiled successfully

Verifying ownership...
Workflow owner link status: owner=<your-owner-address>, linked=true
Key ownership verified

Uploading files...
✔ Loaded binary from: ./binary.wasm.br.b64
✔ Uploaded binary to: https://storage.cre.example.com/artifacts/<workflow-id>/binary.wasm
✔ Loaded config from: ./config.json
✔ Uploaded config to: https://storage.cre.example.com/artifacts/<workflow-id>/config

Preparing deployment transaction...
Preparing transaction for workflowID: <your-workflow-id>
Transaction details:
  Chain Name:   ethereum-mainnet
  To:           0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5
  Function:     UpsertWorkflow
  Inputs:
    [0]:        my-workflow
    [1]:        my-workflow
    [2]:        <your-workflow-id>
    [3]:        0
    [4]:        zone-a
    [5]:        https://storage.cre.example.com/artifacts/<workflow-id>/binary.wasm
    [6]:        https://storage.cre.example.com/artifacts/<workflow-id>/config
    [7]:        0x
    [8]:        false
  Data:         b377bfc50000000000000000000000000000000000...
Estimated Cost:
  Gas Price:      0.00100001 gwei
  Total Cost:     0.00000079 ETH
? Do you want to execute this transaction?:
  ▸ Yes
    No

Transaction confirmed
View on explorer: https://etherscan.io/tx/0x58599f6...d916b

[OK] Workflow deployed successfully

Details:
   Contract address:    0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5
   Transaction hash:    0x58599f6...d916b
   Workflow Name:       my-workflow
   Workflow ID:         <your-workflow-id>
   Binary URL:          https://storage.cre.example.com/artifacts/<workflow-id>/binary.wasm
   Config URL:          https://storage.cre.example.com/artifacts/<workflow-id>/config
```

## Verifying your deployment

After a successful deployment, you can verify that your workflow was registered correctly in two ways:

1. **CRE UI**: View your deployed workflow in the [CRE platform](https://cre.chain.link). Navigate to the **Workflows** section to see your workflow's status, ID, and execution history.

2. **Block Explorer**: Check the Workflow Registry contract on a block explorer. The CLI output will provide the transaction hash for the registration. The `WorkflowRegistry` contract is deployed on **Ethereum Mainnet** at the address <a href="https://etherscan.io/address/0x4ac54353fa4fa961afcc5ec4b118596d3305e7e5#code" target="_blank" rel="noopener noreferrer">`0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5`</a>.

## Using multi-sig wallets

The `deploy` command supports multi-sig wallets through the `--unsigned` flag. When using this flag, the CLI generates raw transaction data that you can submit through your multi-sig wallet interface instead of sending the transaction directly.

For complete setup instructions, configuration requirements, and step-by-step guidance, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Next steps

- [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows): Learn how to control workflow execution
- [Monitoring Workflows](/cre/guides/operations/monitoring-workflows): Track your workflow's execution and performance
- [Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows): Deploy new versions of your workflow

---

# Activating & Pausing Workflows
Source: https://docs.chain.link/cre/guides/operations/activating-pausing-workflows
Last Updated: 2025-11-04

After deploying a workflow, you can control its operational state using the `cre workflow activate` and `cre workflow pause` commands. These commands modify the workflow's status in the Workflow Registry contract, determining whether it can respond to triggers.

**Workflow states:**

- **Active** — The workflow can respond to its configured triggers and execute
- **Paused** — The workflow cannot respond to triggers and will not execute

## Prerequisites

Before activating or pausing workflows, ensure you have:

- **A [deployed workflow](/cre/guides/operations/deploying-workflows)**: You must have a workflow that has been successfully deployed to the Workflow Registry.
- **Workflow ownership**: You must be the owner of the workflow (the account that originally deployed it). Only the workflow owner can activate or pause it.
- **Local workflow folder**: You must run these commands from your project directory. The CLI reads the workflow name and configuration from your `workflow.yaml` file to identify which workflow to activate or pause.
- **[Logged in](/cre/reference/cli/authentication#cre-login)**: Authenticated with the platform by running `cre login`. To check your authentication status, run `cre whoami`.
- **A funded wallet**: The account you are using must be funded with ETH on Ethereum Mainnet to pay the gas fees for the onchain transaction to the Workflow Registry contract.

## Activating a workflow

The `cre workflow activate` command changes a paused workflow's status to active, allowing its triggers to fire and the workflow to execute.

### When to activate

You typically use `activate` after pausing a workflow to resume execution after maintenance or debugging.

### Usage

Run the command from your project root:

```bash
cre workflow activate my-workflow --target production-settings
```

The CLI identifies which workflow to activate based on:

- `workflow-name` from your `workflow.yaml` file
- `workflow-owner-address` (either from `workflow.yaml` or derived from your private key in `.env`)

### What happens during activation

1. The CLI fetches the workflow matching your workflow name and owner address
2. It validates that the workflow is currently paused
3. If valid, it sends an onchain transaction to change the status to active

### Example output

```bash
> cre workflow activate my-workflow --target production-settings

Activating Workflow :    my-workflow
Target :                 production-settings
Owner Address :          <your-owner-address>

Activating workflow: Name=my-workflow, Owner=<your-owner-address>, WorkflowID=<your-workflow-id>
Transaction details:
  Chain Name:   ethereum-testnet-sepolia
  To:           0xF3f93fc4dc177748E7557568b5354cB009e3818a
  Function:     ActivateWorkflow
  Inputs:
    [0]:        <your-workflow-id>
    [1]:        zone-a
  Data:         530979d6000000000000000000000000...
Estimated Cost:
  Gas Price:      0.00100000 gwei
  Total Cost:     0.00000038 ETH
? Do you want to execute this transaction?:
  ▸ Yes
    No

Transaction confirmed: 0xd5b94bd...87498b
View on explorer: https://sepolia.etherscan.io/tx/0xd5b94bd...87498b

[OK] Workflow activated successfully
   Contract address:    0xF3f93fc4dc177748E7557568b5354cB009e3818a
   Transaction hash:    0xd5b94bd...87498b
   Workflow Name:       my-workflow
   Workflow ID:         <your-workflow-id>
```

## Pausing a workflow

The `cre workflow pause` command changes an active workflow's status to paused, preventing its triggers from firing and stopping execution.

### When to pause

Pause workflows when you need to:

- **Perform maintenance**: Temporarily stop execution while updating dependencies or configuration
- **Debug issues**: Halt execution to investigate errors or unexpected behavior
- **Temporarily halt operations**: Stop workflow execution without permanently deleting it

### Usage

Run the command from your project root:

```bash
cre workflow pause my-workflow --target production-settings
```

### What happens during pausing

1. The CLI fetches the workflow matching your workflow name and owner address
2. It validates that the workflow is currently active
3. If valid, it sends an onchain transaction to change the status to paused

### Example output

```bash
> cre workflow pause my-workflow --target production-settings

Pausing Workflow :       my-workflow
Target :                 production-settings
Owner Address :          <your-owner-address>

Fetching workflows to pause... Name=my-workflow, Owner=<your-owner-address>
Processing batch pause... count=1
Transaction details:
  Chain Name:   ethereum-testnet-sepolia
  To:           0xF3f93fc4dc177748E7557568b5354cB009e3818a
  Function:     BatchPauseWorkflows
  Inputs:
    [0]:        [<your-workflow-id>]
  Data:         d8b80738000000000000000000000000...
Estimated Cost:
  Gas Price:      0.00100000 gwei
  Total Cost:     0.00000021 ETH
? Do you want to execute this transaction?:
  ▸ Yes
    No

Transaction confirmed
View on explorer: https://sepolia.etherscan.io/tx/0x2e09a66...db66e
[OK] Workflows paused successfully

Details:
   Contract address:    0xF3f93fc4dc177748E7557568b5354cB009e3818a
   Transaction hash:    0x2e09a66...db66e
   Workflow Name:       my-workflow
   Workflow ID:         <your-workflow-id>
```


<Aside type="note" title="Missed triggers are not queued">
  Triggers that would have fired while a workflow was paused are **not queued or retroactively executed**. When you activate the workflow again, it will only respond to new triggers that occur after activation.
</Aside>

## Using multi-sig wallets

Both `activate` and `pause` commands support multi-sig wallets through the `--unsigned` flag. When using this flag, the CLI generates raw transaction data that you can submit through your multi-sig wallet interface instead of sending the transaction directly.

For complete setup instructions, configuration requirements, and step-by-step guidance, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Learn more

- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Learn how to deploy workflows to the registry
- [Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows) — Update your workflow code and configuration
- [Deleting Workflows](/cre/guides/operations/deleting-workflows) — Permanently remove workflows from the registry
- [CLI Reference: Workflow Commands](/cre/reference/cli/workflow) — Complete command reference with all flags and options

---

# Updating Deployed Workflows
Source: https://docs.chain.link/cre/guides/operations/updating-deployed-workflows
Last Updated: 2025-11-04

When you update a deployed workflow, you redeploy it with the same workflow name. The new deployment replaces the previous version in the Workflow Registry contract. Currently, CRE does not maintain version history—each deployment overwrites the previous one.

## Prerequisites

Before updating a deployed workflow, ensure you have:

- **A [deployed workflow](/cre/guides/operations/deploying-workflows)**: The workflow must already exist in the Workflow Registry.
- **Workflow ownership**: You must be the owner of the workflow (the account that originally deployed it). Only the workflow owner can update it.
- **Local workflow folder**: You must run this command from your project directory. The CLI reads the workflow name and configuration from your `workflow.yaml` file to identify which workflow to update.
- **[Logged in](/cre/reference/cli/authentication#cre-login)**: Authenticated with the platform by running `cre login`. To check if you are logged in, run `cre whoami`.
- **A funded wallet**: The account must be funded with ETH on Ethereum Mainnet to pay the gas fees for the onchain transaction to the Workflow Registry contract.

## Updating a workflow

To update a workflow, simply redeploy it using the same workflow name:

```bash
cre workflow deploy my-workflow --target production-settings
```

### What happens during an update

1. **Compilation**: Your updated workflow code is compiled to WASM
2. **Upload**: The new binary and configuration files are uploaded to the CRE Storage Service
3. **Registration**: A new registration transaction is sent to the Workflow Registry contract
4. **Replacement**: The previous version is replaced with the new deployment


<Aside type="caution" title="No version history">
  CRE does not currently maintain workflow version history. When you redeploy a workflow with the same name, it completely replaces the previous deployment. You cannot roll back to a previous version through the CLI.
</Aside>

## Best practices for updates

1. **Test locally first**: Always test your changes using `cre workflow simulate` before deploying to production
2. **Pause before updating** (optional): If you want to ensure no triggers fire during the update, pause the workflow first using `cre workflow pause`
3. **Monitor after deployment**: Check that the updated workflow executes correctly after deployment
4. **Keep track of changes**: Maintain your own version control (e.g., Git tags) to track workflow versions

## Using multi-sig wallets

The `deploy` command supports multi-sig wallets through the `--unsigned` flag. When using this flag, the CLI generates raw transaction data that you can submit through your multi-sig wallet interface instead of sending the transaction directly.

For complete setup instructions, configuration requirements, and step-by-step guidance, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Learn more

- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Learn about the initial deployment process
- [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows) — Control workflow execution state
- [Deleting Workflows](/cre/guides/operations/deleting-workflows) — Remove workflows from the registry

---

# Deleting Workflows
Source: https://docs.chain.link/cre/guides/operations/deleting-workflows
Last Updated: 2025-11-04

Deleting a workflow permanently removes it from the Workflow Registry contract. This action cannot be undone, and the workflow will no longer be able to respond to triggers.

## Prerequisites

Before deleting a workflow, ensure you have:

- **A [deployed workflow](/cre/guides/operations/deploying-workflows)**: The workflow must exist in the Workflow Registry.
- **Workflow ownership**: You must be the owner of the workflow (the account that originally deployed it). Only the workflow owner can delete it.
- **Local workflow folder**: You must run this command from your project directory. The CLI reads the workflow name and configuration from your `workflow.yaml` file to identify which workflow to delete.
- **[Logged in](/cre/reference/cli/authentication#cre-login)**: Authenticated with the platform by running `cre login`. To check if you are logged in, run `cre whoami`.
- **A funded wallet**: The account must be funded with ETH on Ethereum Mainnet to pay the gas fees for the onchain transaction to the Workflow Registry contract.

## Deleting a workflow

To delete a workflow, run the `cre workflow delete` command from your project root:

```bash
cre workflow delete my-workflow --target production-settings
```

The CLI identifies which workflow to delete based on:

- `workflow-name` from your `workflow.yaml` file
- `workflow-owner-address` (either from `workflow.yaml` or derived from your private key in `.env`)

### What happens during deletion

1. The CLI fetches all workflows matching your workflow name and owner address
2. It displays details about the workflow(s) to be deleted
3. It prompts you to confirm by typing the workflow name
4. Once confirmed, it sends an onchain transaction to delete the workflow from the Workflow Registry


<Aside type="caution" title="Destructive operation">
  **This action cannot be undone.** Once a workflow is deleted from the Workflow Registry, it is permanently removed and can no longer respond to triggers. The CLI requires you to type the workflow name to confirm this destructive operation.
</Aside>

### Example output

```bash
> cre workflow delete my-workflow --target production-settings

Deleting Workflow :      my-workflow
Target :                 production-settings
Owner Address :          <your-owner-address>

Found 1 workflow(s) to delete for name: my-workflow
   1. Workflow
      ID:              00f0379a2df46ad2c5af070f5871da89f589f8bff8af76ff6a44bb59bec88bf4
      Owner:           <your-owner-address>
      DON Family:      zone-a
      Tag:             my-workflow
      Binary URL:      https://storage.cre.example.com/artifacts/00f0379a.../binary.wasm
      Workflow Status: PAUSED

Are you sure you want to delete the workflow 'my-workflow'?
This action cannot be undone.

To confirm, type the workflow name: my-workflow: my-workflow
Deleting 1 workflow(s)...
Transaction details:
  Chain Name:   ethereum-testnet-sepolia
  To:           0xF3f93fc4dc177748E7557568b5354cB009e3818a
  Function:     DeleteWorkflow
  Inputs:
    [0]:        0x00f0379a2df46ad2c5af070f5871da89f589f8bff8af76ff6a44bb59bec88bf4
  Data:         695e134000f0379a2df46ad2c5af070f5871da89f589f8bff8af76ff6a44bb59bec88bf4
Estimated Cost:
  Gas Price:      0.00100001 gwei
  Total Cost:     0.00000015 ETH
? Do you want to execute this transaction?:
  ▸ Yes
    No

Transaction confirmed
View on explorer: https://sepolia.etherscan.io/tx/0xf059c32...fec7d
[OK] Deleted workflow ID: 00f0379a2df46ad2c5af070f5871da89f589f8bff8af76ff6a44bb59bec88bf4
Workflows deleted successfully.
```

### Skipping the confirmation prompt

If you want to skip the interactive confirmation prompt (e.g., in automated scripts), use the `--yes` flag:

```bash
cre workflow delete my-workflow --yes --target production-settings
```


<Aside type="caution" title="Use --yes with caution">
  The `--yes` flag bypasses the confirmation prompt and immediately deletes the workflow. Only use this flag in automated workflows or when you are absolutely certain you want to delete the workflow.
</Aside>

## Using multi-sig wallets

The `delete` command supports multi-sig wallets through the `--unsigned` flag. When using this flag, the CLI generates raw transaction data that you can submit through your multi-sig wallet interface instead of sending the transaction directly.

For complete setup instructions, configuration requirements, and step-by-step guidance, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Learn more

- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Deploy new workflows to the registry
- [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows) — Control workflow execution state
- [Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows) — Update existing workflows

---

# Using Multi-sig Wallets
Source: https://docs.chain.link/cre/guides/operations/using-multisig-wallets
Last Updated: 2025-11-04

This guide explains how to use multi-sig wallets with CRE CLI commands for deploying, activating, pausing, updating, and deleting workflows.

## How multi-sig works with CRE CLI

When managing workflows with a multi-sig wallet, the CRE CLI can generate raw transaction data that you submit through your multi-sig wallet interface. Instead of the CLI signing and sending the transaction directly, it prepares the transaction data for you to sign offline through your multi-sig wallet.

**The workflow:**

1. Run a CRE CLI command with the `--unsigned` flag
2. The CLI generates the raw transaction data
3. You submit this data to your multi-sig wallet interface
4. Signers approve the transaction
5. Once enough signatures are collected, execute the transaction onchain

## Prerequisites

Before using multi-sig wallets with CRE CLI commands, ensure you have:

### 1. Authenticated with the CLI

You must be logged in to use any CRE CLI commands. Run `cre whoami` in your terminal to verify you're logged in, or run `cre login` to authenticate.

See [Logging in with the CLI](/cre/account/cli-login) for detailed instructions.

### 2. Configure your multi-sig address

Add your multi-sig wallet address to your `project.yaml` or `workflow.yaml` under the target you're using:

```yaml
production-settings:
  user-workflow:
    workflow-owner-address: "<your-multi-sig-address>"
    workflow-name: "my-workflow"
```


<Aside type="caution" title="Configuration priority">
  If you have both `project.yaml` and `workflow.yaml`, the CLI uses the value from `workflow.yaml` for that specific workflow. However, if `project.yaml` contains a placeholder value like `"(optional) Multi-signature contract address"`, you must replace it with your actual multi-sig address in `project.yaml` as well.
</Aside>

### 3. Keep your private key in `.env`

Even when using `--unsigned`, the CLI still requires `CRE_ETH_PRIVATE_KEY` in your `.env` file:

```bash
CRE_ETH_PRIVATE_KEY=your-private-key-here
```


<Aside type="note" title="Why a private key is still needed">
  The `--unsigned` flag tells the CLI to **prepare** the transaction without **signing or sending** it. However, the CLI still needs to connect to the blockchain to read the current state (e.g., fetch workflow IDs, validate workflow status, prepare transaction data). This connection requires a private key for initialization, but the key is never used to sign the multi-sig transaction.
</Aside>

## Using the `--unsigned` flag

Add the `--unsigned` flag to any workflow management command:

- **Deploy**:

  ```bash
  cre workflow deploy my-workflow --unsigned --target production-settings
  ```

- **Activate**:

  ```bash
  cre workflow activate my-workflow --unsigned --target production-settings
  ```

- **Pause**:

  ```bash
  cre workflow pause my-workflow --unsigned --target production-settings
  ```

- **Delete**:

  ```bash
  cre workflow delete my-workflow --unsigned --target production-settings
  ```

## Example output

When you run a command with `--unsigned`, the CLI generates transaction data instead of sending the transaction:

```bash
> cre workflow activate my-workflow --unsigned --target production-settings

Activating Workflow :    my-workflow
Target :                 production-settings
Owner Address :          <your-multi-sig-address>

Activating workflow: Name=my-workflow, Owner=<your-multi-sig-address>, WorkflowID=<your-workflow-id>
--unsigned flag detected: transaction not sent on-chain.
Generating call data for offline signing and submission in your preferred tool:

MSIG workflow activation transaction prepared!
To Activate my-workflow with workflowID: <your-workflow-id>

Next steps:

   1. Submit the following transaction on the target chain:
      Chain:   ethereum-testnet-sepolia
      Contract Address: 0xF3f93fc4dc177748E7557568b5354cB009e3818a

   2. Use the following transaction data:

      530979d600f0379a2df46ad2c5af070f5871da89f589f8bff8af76ff6a44bb59bec88bf4000000000000000000000000000000000000000000000000000000000000004000000000000000000000000000000000000000000000000000000000000000067a6f6e652d610000000000000000000000000000000000000000000000000000
```

## Submitting the transaction to your multi-sig wallet

Once you have the transaction data, follow these steps:

### 1. Open your multi-sig wallet interface

Access your multi-sig wallet (e.g., Gnosis Safe) and navigate to the transaction creation page.

### 2. Create a new transaction

Enter the transaction details from the CLI output:

- **To address**: Use the contract address from the output (e.g., `0xF3f93fc4dc177748E7557568b5354cB009e3818a`)
- **Value**: `0` (no ETH is being sent)
- **Data**: Paste the full transaction data from the CLI output

### 3. Submit for signatures

Submit the transaction for approval. The transaction will require the configured number of signatures from your multi-sig signers.

### 4. Execute the transaction

Once enough signatures are collected, execute the transaction onchain. The multi-sig wallet will broadcast the signed transaction to the blockchain.

## Troubleshooting

### Error: "WorkflowOwner must be a valid Ethereum address"

This error occurs when:

- The `workflow-owner-address` is not set in your configuration
- The `workflow-owner-address` contains a placeholder value like `"(optional) Multi-signature contract address"`

**Solution:** Update your `project.yaml` or `workflow.yaml` with your actual multi-sig address:

```yaml
production-settings:
  user-workflow:
    workflow-owner-address: "0x123..." # Your actual multi-sig address
```

### Error: "failed to read keys: invalid length, need 256 bits"

This error occurs when `CRE_ETH_PRIVATE_KEY` is missing or empty in your `.env` file.

**Solution:** Ensure your `.env` file contains a valid private key:

```bash
CRE_ETH_PRIVATE_KEY=0x1234567890abcdef...
```

Remember, this key is only used for blockchain client initialization, not for signing the multi-sig transaction.

## Learn more

- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Deploy workflows to the registry
- [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows) — Control workflow execution state
- [Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows) — Update workflow code and configuration
- [Deleting Workflows](/cre/guides/operations/deleting-workflows) — Remove workflows from the registry

---

# Monitoring & Debugging Workflows
Source: https://docs.chain.link/cre/guides/operations/monitoring-workflows
Last Updated: 2025-11-04

After deploying a workflow, you can monitor its execution history, performance metrics, and logs through the CRE web interface. This guide walks you through the monitoring dashboard and debugging tools available for your deployed workflows.

## Prerequisites

- **Deployed workflow**: You must have at least one workflow deployed to your organization. See [Deploying Workflows](/cre/guides/operations/deploying-workflows) for instructions.

## Accessing the workflows dashboard

1. **Log in to the CRE UI** at <a href="https://cre.chain.link/login" target="_blank" rel="noopener noreferrer">cre.chain.link</a>

2. **Navigate to the Workflows section** by clicking **Workflows** in the left sidebar, or visit <a href="https://cre.chain.link/workflows" target="_blank" rel="noopener noreferrer">cre.chain.link/workflows</a> directly

The Workflows dashboard displays three main sections:

- **Recent executions**: Shows the most recent workflow runs across all your workflows
- **Activity chart**: Visualizes successful and unsuccessful executions over the selected time period
- **All workflows table**: Lists all deployed workflows with their status and execution history

## Understanding the workflows dashboard

### Recent executions

The top section displays the most recent workflow executions across your organization:

- **Execution ID**: A unique identifier for each workflow run (shortened for display)
- **Workflow name**: The name of the workflow that executed
- **Status**: Success or Failure indicator
- **Timestamp**: When the execution occurred

Click on any execution ID to view detailed logs and events for that specific run.

### Activity chart

The performance chart shows execution trends over the selected time period:

- **Green bars**: Successful executions
- **Red bars**: Unsuccessful executions
- **Height**: Total number of executions for that day

Use the dropdown menu to adjust the time range.

### All workflows table

The main table displays all workflows in your organization:

| Column                           | Description                                                                                         |
| -------------------------------- | --------------------------------------------------------------------------------------------------- |
| **Name**                         | The workflow name as defined in your `workflow.yaml`                                                |
| **Status**                       | Current workflow state: `Pending` (active and ready to execute)                                     |
| **Last Execution**               | Timestamp of the most recent execution, or `N/A` if not yet triggered                               |
| **Execution Results (last 24h)** | Visual bar showing successful (green) vs. failed (red) executions in the past 24 hours, with counts |

Click on any workflow row to view its detailed performance and execution history.

## Viewing workflow details

When you click on a workflow, you'll see a dedicated page with comprehensive monitoring data.

### Performance section

The Performance chart shows execution trends for this specific workflow over the selected time period. This helps you identify patterns, track reliability, and spot anomalies.

### Overview section

The Overview panel displays key workflow metadata:

| Field                    | Description                                                                                     |
| ------------------------ | ----------------------------------------------------------------------------------------------- |
| **Status**               | Current workflow state (e.g., `PENDING`, `PAUSED`)                                              |
| **Workflow ID**          | Unique identifier in the Workflow Registry (truncated, click to copy full ID)                   |
| **Workflow Owner**       | The wallet address that deployed and owns this workflow (truncated, click to copy full address) |
| **Total Runs**           | Cumulative number of executions since deployment                                                |
| **Registered On**        | Timestamp when the workflow was first deployed                                                  |
| **Total Workflow Spend** | Cumulative credits consumed by all executions                                                   |

### Execution history

The **Execution** tab shows a detailed table of all workflow runs:

| Column             | Description                                                                         |
| ------------------ | ----------------------------------------------------------------------------------- |
| **Execution ID**   | Unique identifier for this specific run (click to view details)                     |
| **Workflow ID**    | The workflow that executed (useful if viewing executions across multiple workflows) |
| **Status**         | Execution result: Success, Failure, or In Progress                                  |
| **Time Triggered** | When the workflow execution started                                                 |
| **Credits Used**   | Cost of this execution in CRE credits                                               |

Use the **ALL** dropdown filter to view all executions, only successful ones, or only failures.

<Aside type="tip" title="Filtering executions">
  To quickly identify failures, use the filter dropdown to show only failed executions. This helps you focus on
  debugging problematic runs.
</Aside>

### Deployments tab

The **Deployments** tab shows the history of workflow deployments and updates.

## Debugging individual executions

Click on any **Execution ID** to view detailed debugging information for that specific run.

### Events tab

The **Events** tab displays the events triggered by this execution in sequential order:

- **Event**: The type of event (e.g., `trigger`, `evm:ChainSelector`, `consensus`, etc.)
- **Status**: Whether the event succeeded, failed, or is in progress (`Success`, `Failure`, `In progress`)
- **Time Triggered**: When the event occurred

**What you'll see:**

- Trigger activation
- Capability execution steps (HTTP requests, EVM calls, etc.)
- Consensus operations

<Aside type="note" title="User logs are separate">
  The Events tab shows system-level events only. To see your custom log messages (e.g., `runtime.log()` in TypeScript or
  `logger.Info()` in Go), switch to the **Logs** tab.
</Aside>

### Logs tab

The **Logs** tab shows only user-emitted logs from your workflow code:

- Each log entry displays a timestamp, log level (e.g., `INFO`), and your custom message
- This is where you'll see output from `runtime.log()` (TypeScript) or `logger.Info()` (Go)
- System messages and internal events are excluded for clarity
- Logs appear in chronological order, showing the complete execution flow of your workflow

**Example log output:**

```
time=2025-10-26T13:19:57.055Z level=INFO msg="Successfully fetched offchain value" result=4
time=2025-10-26T13:19:57.055Z level=INFO msg="Successfully read onchain value" result=22
time=2025-10-26T13:19:57.055Z level=INFO msg="Final calculated result" result=26
time=2025-10-26T13:19:57.055Z level=INFO msg="Updating calculator result" consumerAddress=0x00307d6d1f88...
time=2025-10-26T13:19:57.055Z level=INFO msg="Writing report to consumer contract" offchainValue=4 onchainValue=22
time=2025-10-26T13:19:57.055Z level=INFO msg="Waiting for write report response"
```

<Aside type="tip" title="Debugging workflow logic">
  Use `runtime.log()` (TypeScript) or `logger.Info()` (Go) liberally in your workflow code during development. These
  logs appear in the Logs tab and are invaluable for debugging execution flow and inspecting variable values.
</Aside>

## Monitoring best practices

1. **Check the dashboard regularly**: Review the Activity chart to spot trends in failures or performance degradation

2. **Investigate failures immediately**: When you see red bars in the chart or failed executions, click through to view logs and identify the root cause

3. **Use descriptive log messages**: Include context in your log statements to make debugging easier:

   ```typescript
   // TypeScript
   runtime.log(`Successfully fetched offchain value: ${offchainValue}`)
   runtime.log(`Final calculated result: ${finalResult}`)
   ```

   ```go
   // Go
   logger.Info("Successfully fetched offchain value", "result", offchainValue)
   logger.Info("Final calculated result", "result", finalResult)
   ```

4. **Monitor execution frequency**: If a cron-triggered workflow shows fewer executions than expected, verify your cron schedule and workflow status

5. **Track credit usage**: Monitor the Total Workflow Spend to understand your workflow's cost over time

## Common debugging scenarios

### Workflow not executing

**Symptoms:** No recent executions in the dashboard

**Possible causes:**

- Workflow is paused (check the Status field)
- Trigger is not firing (e.g., invalid cron schedule, no matching onchain events)
- Workflow was recently deployed and hasn't been triggered yet

**Resolution:**

- Verify the workflow Status is `PENDING` (active)
- Check your trigger configuration in the code
- Use [Workflow Simulation](/cre/guides/operations/simulating-workflows) to test locally

***

### Execution failures

**Symptoms:** Red bars in Activity chart, failed executions in table

**Possible causes:**

- API request failures (HTTP errors, timeouts)
- Onchain reverts (contract calls failing)
- Invalid configuration or missing secrets
- Logic errors in workflow code

**Resolution:**

1. Click on the failed Execution ID
2. Check the **Events** tab to see which step failed
3. Review the **Logs** tab for error messages from your code
4. Fix the issue in your code and [update the workflow](/cre/guides/operations/updating-deployed-workflows)

## Related guides

- **[Deploying Workflows](/cre/guides/operations/deploying-workflows)** - Deploy your first workflow
- **[Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows)** - Control workflow execution
- **[Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows)** - Fix issues and deploy updates
- **[Simulating Workflows](/cre/guides/operations/simulating-workflows)** - Test workflows locally before deploying

---

# Account
Source: https://docs.chain.link/cre/account
Last Updated: 2025-11-04

Your CRE account is required to use the CRE CLI. You must be logged in to run any CLI commands, including simulating workflows, deploying workflows, and managing deployed workflows. This section covers everything you need to know about creating and managing your account.

## What you'll need

To use CRE, you need:

1. **A CRE account** - Created through the web interface at <a href="https://cre.chain.link" target="_blank" rel="noopener noreferrer">cre.chain.link</a> (to create a new organization) or via an invitation link from an organization Owner (to join an existing organization)
2. **Two-factor authentication** - Set up during account creation for security
3. **CLI authentication** - Connect your CLI to your account using `cre login`

## Account guides

- **[Creating Your Account](/cre/account/creating-account)** - Step-by-step guide to creating a new CRE account through the CRE UI
- **[Logging in with the CLI](/cre/account/cli-login)** - Authenticate your CLI with your CRE account to run commands
- **[Managing Authentication](/cre/account/managing-auth)** - Check your login status, handle session expiration, and log out

## Security features

Your CRE account includes several security features:

- **Two-factor authentication (2FA)** - Required during login for an additional layer of security
- **Recovery codes** - Provided during account setup to regain access if you lose your authenticator device
- **Session management** - CLI sessions automatically expire after a period of inactivity

<Aside type="tip" title="Keep your credentials secure">
  Never share your password, two-factor authentication codes, or recovery codes with anyone. Store your recovery code in
  a secure location separate from your password.
</Aside>

## Related guides

Once you have your account set up and authenticated:

- **[Understanding Organizations](/cre/organization/understanding-organizations)** - Learn about CRE organizations, roles, and permissions
- **[Linking Wallet Keys](/cre/organization/linking-keys)** - Connect your wallet to deploy workflows

---

# Creating Your Account
Source: https://docs.chain.link/cre/account/creating-account
Last Updated: 2025-11-04

Before you can use CRE, you need to create an account. An account is required to log in with the CRE CLI and run any CLI commands, including [simulating](/cre/guides/operations/simulating-workflows) workflows.

There are two ways to create an account:

1. **Create a new organization**: Sign up directly on the <a href="https://cre.chain.link" target="_blank" rel="noopener noreferrer">CRE UI</a>. You'll become the *Owner* of a new organization.
2. **Join an existing organization**: Accept an invitation from an existing organization Owner. You'll become a *Member* of that organization automatically after account creation.

This guide walks you through the account creation process for both scenarios.

## Prerequisites

- A valid email address
- Access to your email inbox to receive verification codes (and invitation email, if joining an existing organization)

## Step 1: Navigate to the CRE UI

There are two ways to begin the account creation process:

### Option A: Create a new organization

In this option, you'll create a new organization and become the *Owner* of that organization.

Go to <a href="https://cre.chain.link/login" target="_blank" rel="noopener noreferrer">cre.chain.link</a> and click the **"Create an account"** button.

### Option B: Join an existing organization

In this option, you'll join an existing organization and become a *Member* of that organization.

If you've received an invitation email from an organization Owner, click the **"Accept Invitation"** button in the email. This will redirect you to the account creation page.


<Aside type="note" title="Organization membership">
  Only Owners can invite new members. Learn more about [Understanding Organizations](/cre/organization/understanding-organizations).
</Aside>

After choosing either option, continue with the following steps to complete your account creation.

## Step 2: Enter your information

Fill in the required information:

1. **Email address**: Enter a valid email address (if not already pre-filled)
2. **Country**: Select your country from the dropdown
3. **Terms and policies**: Review and accept the Terms of Service and Privacy Policy

Click **"Continue"** to proceed.

## Step 3: Verify your email

<Aside type="note" title="Organization invitations">
  If you're joining an existing organization via an invitation link, **skip this step**. Your email is already verified
  when you accept the invitation, and you'll proceed directly to setting your password.
</Aside>

Check your email inbox for a message from Chainlink containing a 6-digit verification code. Enter this code in the verification screen and click **Continue**.

<Aside type="tip" title="Didn't receive the code?">
  If you don't see the verification email, check your spam folder. You can also request a new code by clicking "Resend
  Code" on the verification screen.
</Aside>

## Step 4: Set your password

Create a secure password for your account. Your password must meet the security requirements displayed on the screen.

<Aside type="tip" title="Password best practices">
  Use a strong, unique password that you don't use for other services. Consider using a password manager to generate and
  store secure passwords.
</Aside>

## Step 5: Set up two-factor authentication (2FA)

To secure your account, you'll need to set up two-factor authentication. You'll be presented with two authentication method options:

1. **Fingerprint or Face Recognition** - Use biometric authentication on your device
2. **Google Authenticator or similar** - Use an authenticator app

### Using an authenticator app

If you choose the authenticator app option:

1. Click **"Google Authenticator or similar"**
2. Open your preferred authenticator app (such as Google Authenticator, Authy, or 1Password)
3. Scan the QR code displayed on the screen
4. Enter the 6-digit one-time code generated by your authenticator app
5. Click **"Continue"**

<Aside type="tip" title="Trouble scanning?">
  If you're having difficulty scanning the QR code, click "Trouble Scanning?" on the screen for alternative setup
  options.
</Aside>

## Step 6: Save your recovery code

Your recovery code is essential for regaining access to your account if you lose access to your authenticator device.

1. Copy the recovery code displayed on the screen
2. Store it securely in a password manager or offline location
3. Check the box "I have safely recorded this code"
4. Click **"Continue"** to complete account creation

<Aside type="caution" title="Keep your recovery code safe">
  **Never share your recovery code with anyone.** You'll need this code if you ever need to log in without your
  authenticator device. Store it in a secure location.
</Aside>

After completing these steps, you'll be redirected to your CRE dashboard.

## What's next?

Once your account is created:

1. **[Log in to the CRE CLI](/cre/account/cli-login)** - Authenticate your CLI session
2. **If you created a new organization (Owner)**: [Invite team members](/cre/organization/inviting-members) to collaborate on workflows

---

# Logging in with the CLI
Source: https://docs.chain.link/cre/account/cli-login
Last Updated: 2025-11-04

To deploy and manage workflows with the CRE CLI, you need to authenticate your CLI session with your CRE account. This guide walks you through the login process.

## Prerequisites

- [CRE CLI installed](/cre/getting-started/cli-installation) on your machine
- [CRE account created](/cre/account/creating-account)
- Access to your authenticator app for two-factor authentication

## Login process

### Step 1: Initiate login from the terminal

Open your terminal and run the login command:

```bash
cre login
```

This command will automatically open your default web browser to begin the authentication process.

<Aside type="tip" title="Browser doesn't open?">
  If your browser doesn't open automatically, the CLI will display a URL that you can manually copy and paste into your
  browser.
</Aside>

### Step 2: Enter your email address

In the browser window that opens, enter the email address associated with your CRE account and click **"Continue"**.

### Step 3: Enter your password

Enter your account password and click **"Continue"**.

### Step 4: Complete two-factor authentication

You'll be prompted to complete two-factor authentication based on the method you configured during account creation:

- **If you set up an authenticator app**: Open your authenticator app and enter the 6-digit one-time code it generates for your CRE account.
- **If you set up biometric authentication**: Use your fingerprint or face recognition as prompted by your device.

Example of entering a one-time code from an authenticator app:

### Step 5: Confirm successful login

Once authenticated, you'll see a confirmation message in your browser:

You can now close the browser window and return to your terminal. Your terminal will display:

```bash
Login completed successfully
```

Your CLI session is authenticated and ready to use.

---

# Managing Authentication
Source: https://docs.chain.link/cre/account/managing-auth
Last Updated: 2025-11-04

This guide covers how to manage your CLI authentication session, including logging in, checking your status, handling session expiration, and logging out.

## Logging in

To authenticate your CLI with your CRE account, use the `cre login` command. This opens a browser window where you'll enter your credentials and complete two-factor authentication.

For detailed login instructions, see the [Logging in with the CLI](/cre/account/cli-login) guide.

## Session expiration

Your CLI session remains authenticated until you explicitly log out or until your session expires. When your session expires, you'll need to log in again.

If you attempt to run a command with an expired session, you'll see an error:

```bash
Error: failed to attach credentials: failed to load credentials: you are not logged in, try running cre login
```

To resolve this, simply run `cre login` again to re-authenticate.

## Checking authentication status

To verify that you're logged in and view your account details, use the `cre whoami` command:

```bash
cre whoami
```

This command displays your account information:

```bash
Account details retrieved:

Email:           email@domain.com
Organization ID: org_mEMRknbVURM9DWsB
```

If you're not logged in, you'll receive an error message prompting you to run `cre login`.

## Logging out

To explicitly end your CLI session and remove your stored credentials, use the `cre logout` command:

```bash
cre logout
```

After logging out, you'll need to run `cre login` again to authenticate future CLI commands.

---

# Organization
Source: https://docs.chain.link/cre/organization
Last Updated: 2025-11-04

CRE organizations enable teams to collaborate on workflow development and deployment. An organization provides a shared workspace where multiple members can deploy workflows, link wallet addresses, and monitor execution activity together.

## Organization guides

- **[Understanding Organizations](/cre/organization/understanding-organizations)** - Learn how organizations work, including roles, permissions, and shared resources
- **[Linking Wallet Keys](/cre/organization/linking-keys)** - Connect your wallet address to deploy and manage workflows
- **[Inviting Team Members](/cre/organization/inviting-members)** - Add colleagues to your organization (Owner only)

---

# Understanding Organizations
Source: https://docs.chain.link/cre/organization/understanding-organizations
Last Updated: 2025-11-04

## What is an organization?

A CRE organization is a collaborative workspace that allows multiple team members to deploy, manage, and monitor workflows together. When you create a CRE account, you either start a new organization (becoming the *Owner*) or join an existing one (becoming a *Member*).

An organization serves as a container for:

- **Multiple team members** with different roles (Owner and Members)
- **Linked wallet addresses** from all team members
- **Deployed workflows** visible to all organization members
- **Shared monitoring** of workflow executions and activity

Organizations enable teams to collaborate on CRE workflows while maintaining individual control over their own wallet addresses and secrets.

## Organization structure

### Single Owner model

Every organization has exactly **one Owner**—the person who first created the organization account. The Owner role:

- Cannot be transferred or changed
- Has full administrative control
- Can invite new Members to join
- Can link wallet addresses and deploy workflows

### Multiple Members

Members are users invited by the Owner to join the organization. Each member:

- Joins automatically after accepting the invitation and creating their account
- Can link their own wallet addresses to the organization
- Can deploy and manage their own workflows
- Views all organization workflows in the shared dashboard

### Shared visibility

All organization members can:

1. **View linked wallet keys** - Use `cre account list-key` to see all addresses linked to the organization by any member
2. **Monitor all workflows** - Access <a href="https://cre.chain.link/workflows" target="_blank" rel="noopener noreferrer">cre.chain.link/workflows</a> to view:
   - All deployed workflows across the organization
   - Recent execution history
   - Workflow status (Active, Paused, Pending)
   - Execution success/failure metrics
   - Activity graphs and trends

## Organization roles

CRE uses a simple role-based access control system with two roles:

| Role       | Description                                           | Permissions                                                                                                                                                                                                                 | How to obtain                                                                             |
| ---------- | ----------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------- |
| **Owner**  | The person who first created the organization account | <ul><li>Full access to all organization resources and workflows</li><li>Invite new members to the organization</li><li>Manage organization settings</li><li>Deploy, activate, pause, update, and delete workflows</li></ul> | Automatically assigned when you create a new organization by signing up directly          |
| **Member** | Users invited to join an existing organization        | <ul><li>View all organization workflows</li><li>Link wallet addresses to the organization</li><li>Deploy and manage workflows under their linked addresses</li><li>Access workflow execution data</li></ul>                 | Invited by the Owner through the [invitation process](/cre/organization/inviting-members) |

**Key points:**

- There is only one Owner per organization
- Owner permissions cannot be transferred
- Only Owners can invite new Members
- Members automatically join after accepting the invitation and creating an account


<Aside type="note" title="Future role enhancements">
  Additional roles with more granular permissions may be added in future releases to provide more flexible access control for larger teams.
</Aside>

## Learn more

- **[Inviting Team Members](/cre/organization/inviting-members)** - How Owners can add Members to the organization
- **[Linking Wallet Keys](/cre/organization/linking-keys)** - How to link your wallet address to deploy workflows
- **[Creating Your Account](/cre/account/creating-account)** - How to create an account and join or create an organization
- **[Deploying Workflows](/cre/guides/operations/deploying-workflows)** - Deploy your first workflow after linking a key

---

# Linking Wallet Keys
Source: https://docs.chain.link/cre/organization/linking-keys
Last Updated: 2025-11-04

Before you can deploy workflows, you must link a public key address to your CRE organization. This process registers your wallet address onchain in the Workflow Registry contract—the smart contract on Ethereum Mainnet that stores and manages all CRE workflows—associating it with your organization and allowing you to deploy and manage workflows.

## What is key linking?

Key linking is the process of connecting a blockchain wallet address to your CRE organization. Once linked, this address becomes a **workflow owner address** that can deploy, update, and delete workflows in the Workflow Registry.

**Key benefits:**

- Multiple team members can link their own addresses to the same organization
- Each linked address can independently deploy and manage workflows
- Addresses are labeled for easy identification (e.g., "Production Wallet", "Dev Wallet")
- All linked addresses are visible to organization members via cre account list-key

**Important constraints:**

- **One organization per address**: Each wallet address can only be linked to one CRE organization at a time. If you need to use the same address with a different organization, you must first [unlink it](#unlinking-a-key) from the current organization.
- **Maximum 2 keys per organization**: Each organization can link a maximum of 2 web3 keys. If you need to link a third key, you must first [unlink](#unlinking-a-key) one of the existing keys.

However, an organization can have multiple wallet addresses linked to it, allowing team members to use their own addresses or enabling separation between development, staging, and production environments.


<Aside type="note" title="Already linked to another organization?">
  If you try to link an address that's already registered with another organization, the CLI will display an error. To use this address with your current organization, you must first unlink it from the other organization using cre account unlink-key (note: this will delete all workflows registered under that address in the other organization).
</Aside>

## Prerequisites

Before linking a key, ensure you have:

- **CRE CLI installed and authenticated**: See [CLI Installation](/cre/getting-started/cli-installation) and [Logging in with the CLI](/cre/account/cli-login)
- **A CRE project directory**: You must run the command from a project directory that contains a `project.yaml` file
- **Private key in `.env`**: Set `CRE_ETH_PRIVATE_KEY=<your_64_character_hex_key>` (without `0x` prefix) in your `.env` file
- **Funded wallet**: Your wallet must have ETH on Ethereum Mainnet to pay for gas fees (the Workflow Registry contract is deployed on Ethereum Mainnet)
- **Unlinked address**: The wallet address must not already be linked to another CRE organization. Each address can only be associated with one organization at a time.


<Aside type="caution" title="Never commit your .env file">
  Your `.env` file contains sensitive private keys. Ensure it's listed in `.gitignore` and never committed to version control.
</Aside>

## Linking your first key

The easiest way to link a key is to let the deployment process handle it automatically. When you first try to [deploy a workflow](/cre/guides/operations/deploying-workflows), the CLI will detect that your address isn't linked and prompt you to link it.

### Automatic linking during deployment

1. Navigate to your project directory (where your `.env` file is located)

2. Attempt to deploy a workflow:

   ```bash
   cre workflow deploy my-workflow --target production-settings
   ```

3. The CLI will detect that your address isn't linked and prompt you:

   ```bash
   Verifying ownership...
   Workflow owner link status: owner=<your_owner_address>, linked=false
   Owner not linked. Attempting auto-link: owner=<your_owner_address>
   Linking web3 key to your CRE organization
   Target :                 production-settings
   ✔ Using Address :        <your_owner_address>

   ✔ Provide a label for your owner address: █
   ```

4. Enter a descriptive label for your address

5. Review the transaction details and confirm

The CLI will submit the transaction and continue with the deployment once the key is linked.

### Manual linking

You can also link a key manually before attempting to deploy:

```bash
cre account link-key --target production-settings
```

**Interactive flow:**

1. The CLI derives your public address from the private key in `.env`
2. You're prompted to provide a label
3. The CLI checks if the address is already linked
4. Transaction details are displayed (chain, contract address, estimated gas cost)
5. You confirm to execute the transaction
6. The transaction is submitted and you receive a block explorer link

**Example output:**

```bash
Linking web3 key to your CRE organization
Target :                 production-settings
✔ Using Address :        <your_owner_address>

Provide a label for your owner address: <your_owner_label>

Checking existing registrations...
✓ No existing link found for this address
Starting linking: owner=<your_owner_address>, label=<your_owner_label>
Contract address validation passed
Transaction details:
  Chain Name:   ethereum-mainnet
  To:           0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5 # Workflow Registry contract address
  Function:     LinkOwner
  ...
Estimated Cost:
  Gas Price:      0.12450327 gwei
  Total Cost:     0.00001606 ETH
? Do you want to execute this transaction?:
  ▸ Yes
    No
```

After confirming, you'll see:

```bash
Transaction confirmed
View on explorer: https://etherscan.io/tx/<your_transaction_hash>

[OK] web3 address linked to your CRE organization successfully

→ You can now deploy workflows using this address
```

## Viewing linked keys

To see all addresses linked to your organization:

```bash
cre account list-key
```

**Example output:**

```bash
Workflow owners retrieved successfully:

Linked Owners:

  1. JohnProd
     Owner Address:     <public_owner_address>
     Status:            VERIFICATION_STATUS_SUCCESSFULL
     Verified At:       2025-10-21T17:22:24.394249Z
     Chain Selector:    5009297550715157269 # Chain selector for Ethereum Mainnet
     Contract Address:  0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5 # Workflow Registry contract address

  2. JaneProd
     Owner Address:     <public_owner_address>
     Status:            VERIFICATION_STATUS_SUCCESSFULL
     Verified At:       2025-10-21T17:22:24.394249Z
     Chain Selector:    5009297550715157269 # Chain selector for Ethereum Mainnet
     Contract Address:  0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5 # Workflow Registry contract address
```

**Understanding the output:**

- **Label**: The friendly name you provided (e.g., "JohnProd", "JaneProd")
- **Owner Address**: The public address linked to your organization
- **Status**: `VERIFICATION_STATUS_SUCCESSFULL` (linked and verified)
- **Verified At**: Timestamp when the link was confirmed onchain
- **Chain Selector**: The chain identifier where the Workflow Registry contract is deployed
- **Contract Address**: The Workflow Registry contract address


<Aside type="note" title="Organization-wide visibility">
  All members of your organization can see all linked addresses using `cre account list-key`. This helps teams coordinate workflow ownership and deployment.
</Aside>

## Linking multiple addresses

Your organization can link **up to 2 wallet addresses**. Each individual address can only be linked to one organization at a time.

This is useful for:

- **Separation of concerns**: Different addresses for development, staging, and production
- **Team collaboration**: Each team member uses their own address
- **Multi-sig wallets**: Link a multi-sig address alongside individual addresses

To link another address:

1. Update your `.env` file with the new private key
2. Run `cre account link-key --target <target-name>` again
3. Provide a unique label to distinguish this address

## Unlinking a key

If you need to remove a linked address from your organization, you can use the `cre account unlink-key` command. This is useful when:

- Rotating addresses for security reasons
- Removing addresses that are no longer in use
- Cleaning up test or development addresses


<Aside type="caution" title="Destructive operation">
  **Unlinking a key will permanently delete all workflows registered under that address.** This action cannot be undone. Make sure you want to permanently remove all associated workflows before proceeding.
</Aside>

To unlink a key:

1. Ensure your `.env` file contains the private key of the address you want to unlink

2. Run the unlink command:

   ```bash
   cre account unlink-key --target production-settings
   ```

3. Confirm the operation when prompted

The CLI will submit an onchain transaction to remove the address from the Workflow Registry. After the transaction is confirmed, the address and all its associated workflows will be deleted.

## Non-interactive mode

For automation or CI/CD pipelines, use the `--yes` flag to skip confirmation prompts:

```bash
cre account link-key --owner-label "CI Pipeline Wallet" --yes --target production-settings
```

## Using multi-sig wallets

If you're using a multi-sig wallet, you'll need to use the `--unsigned` flag to generate raw transaction data that you can then submit through your multi-sig interface (such as Safe).

### Prerequisites for multi-sig

1. Configure your multi-sig address in `project.yaml` under the `account` section:

   ```yaml
   production-settings:
     account:
       workflow-owner-address: "<your_multisig_address>"
     # ... other settings
   ```

2. Ensure your `.env` file contains the private key of any signer from the multi-sig wallet (used only for signature generation, not for sending transactions)

### Linking a multi-sig address

Run the `link-key` command with the `--unsigned` flag:

```bash
cre account link-key --owner-label "SafeWallet" --target production-settings --unsigned
```

**Example output:**

```bash
Linking web3 key to your CRE organization
Target :                 production-settings
✔ Using Address :        <your_multisig_address>

Checking existing registrations...
✓ No existing link found for this address
Starting linking: owner=<your_multisig_address>, label=SafeWallet
Contract address validation passed
--unsigned flag detected: transaction not sent on-chain.
Generating call data for offline signing and submission in your preferred tool:

Ownership linking initialized successfully!

Next steps:

   1. Submit the following transaction on the target chain:
      Chain:            ethereum-mainnet
      Contract Address: 0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5

   2. Use the following transaction data:

      dc1019690000000000000000000000000000000000000000000000000000000068fd2f9465259a804e880ee30de0fcc2b81ee25d598ee1601e13ace2c2ec10202869706800000000000000000000000000000000000000000000000000000000000000600000000000000000000000000000000000000000000000000000000000000041bd0f40824a1fdce10ee1091703833fb3d4497b3f681f6edee6b159d217326185407ce16eb1c668c90786421b053d4d25401f422aa90d156c35659d7c3e2e13221b00000000000000000000000000000000000000000000000000000000000000

Linked successfully
```

### Submitting through your multi-sig interface

1. **Copy the transaction data** provided in the CLI output
2. **Open your multi-sig interface** (e.g., Safe app at [https://app.safe.global](https://app.safe.global))
3. **Create a new transaction** with:
   - **To address**: 0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5 (Workflow Registry contract)
   - **Value**: 0 (no ETH transfer)
   - **Data**: Paste the transaction data from the CLI output (add 0x prefix if required by your multi-sig interface)
4. **Submit and collect signatures** from the required number of signers
5. **Execute the transaction** once you have enough signatures

**Note**: If your multi-sig interface requires the RegistryWorkflow contract ABI, you can copy it from <a href="https://etherscan.io/address/0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5#code" target="_blank" rel="noopener noreferrer">Etherscan</a>.

### Verifying the multi-sig link

After the multi-sig transaction is executed onchain, you can verify the link status:

```bash
cre account list-key
```

Initially, you'll see the address with a `VERIFICATION_STATUS_PENDING` status:

```bash
Workflow owners retrieved successfully:

Linked Owners:

  1. SafeWallet
     Owner Address:     <your_multisig_address>
     Status:            VERIFICATION_STATUS_PENDING
     Verified At:
     Chain Selector:    5009297550715157269 # Chain selector for Ethereum Mainnet
     Contract Address:  0x4Ac54353FA4Fa961AfcC5ec4B118596d3305E7e5 # Workflow Registry contract address
```

Once the transaction is confirmed onchain, the status will change to `VERIFICATION_STATUS_SUCCESSFULL` and the `Verified At` timestamp will be populated.


<Aside type="note" title="Multi-sig for workflow operations">
  Once your multi-sig address is linked, you'll also need to use the `--unsigned` flag for workflow deployment and management commands (`deploy`, `activate`, `pause`, `delete`). See the [Deploying Workflows guide](/cre/guides/operations/deploying-workflows) for more details on using multi-sig wallets with workflow operations.
</Aside>

## Learn more

- **[Understanding Organizations](/cre/organization/understanding-organizations)** - Learn about organization structure and shared resources
- **[Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets)** - Advanced guide for multi-sig wallet workflows
- **[Account Management CLI Reference](/cre/reference/cli/account)** - Complete reference for `cre account` commands
- **[Deploying Workflows](/cre/guides/operations/deploying-workflows)** - Deploy your first workflow after linking a key

---

# Inviting Team Members
Source: https://docs.chain.link/cre/organization/inviting-members
Last Updated: 2025-11-04

To collaborate with your team on CRE workflows, you can invite members to your organization. This guide walks you through the invitation process.

<Aside type="note" title="Organization ownership">
  The first person to create an account for your organization automatically becomes the **Owner**. Only the Owner can
  invite new members to the organization.
</Aside>

## Prerequisites

- You must be logged in to <a href="https://cre.chain.link" target="_blank" rel="noopener noreferrer">cre.chain.link</a>
- You must be the Owner of your organization
- The email addresses you're inviting must belong to whitelisted domains

## Step 1: Navigate to Organization settings

In the left sidebar, click on **"Organization"**.

## Step 2: Go to the Members tab

Click on the **"Members"** tab to view your organization's members.

## Step 3: Start the invitation process

Click the **"Invite"** button in the top right corner.

## Step 4: Add team member details

Enter the **name** and **email address** of the person you want to invite.

### Inviting multiple members at once

To invite more than one person:

1. Click the **"Add person"** button (visible in the screenshot above)
2. Enter the name and email for each additional person
3. Repeat as needed

## Step 5: Send invitations

Once you've added all the people you want to invite, click the **"Submit"** button in the bottom right corner.

The invited members will receive an email invitation to join your organization.

## What happens next?

After you send invitations:

1. **Email notification**: Each invited person receives an email with instructions to join your organization
2. **Account creation**: If they don't have a CRE account yet, they'll be invited to create one
3. **Automatic membership**: Once they complete account creation, they'll automatically become members of your organization—no additional acceptance step required
4. **Access**: They'll immediately have access to your organization's workflows and resources

<Aside type="note" title="Domain restrictions">
  The email addresses you invite must belong to domains that are whitelisted for CRE access. If an invited member's
  domain is not whitelisted, they won't be able to complete the account creation process.
</Aside>

## Related guides

- [Understanding Organizations](/cre/organization/understanding-organizations) - Learn about organization roles and permissions
- [Linking Wallet Keys](/cre/organization/linking-keys) - Help new members connect their wallets

---

# Capabilities Overview
Source: https://docs.chain.link/cre/capabilities
Last Updated: 2025-11-04

At the core of the Chainlink Runtime Environment (CRE) is the concept of **Capabilities**. A capability is a modular, decentralized service that performs a specific task. Think of them as the individual "bricks" that you can use to build custom workflows.

Each capability is powered by its own independent Decentralized Oracle Network (DON), which is optimized for that specific task, ensuring security and reliable performance.

## Invoking Capabilities via the SDK

As a developer, you do not interact with these capability DONs directly. Instead, you invoke them through the developer-friendly interfaces provided by the **CRE SDKs** ([Go](/cre/reference/sdk/core-go) and [TypeScript](/cre/reference/sdk/core-ts)), such as the [`evm.Client`](/cre/reference/sdk/evm-client) or the [`http.Client`](/cre/reference/sdk/http-client). The SDK handles the low-level complexity of communicating with the correct DON and processing the consensus-verified result, allowing you to focus on your business logic.

## Available Capabilities

This section provides a high-level, conceptual overview of the capabilities currently available in CRE.

- **[Triggers](/cre/capabilities/triggers)**: Event sources that start your workflow executions.
- **[HTTP](/cre/capabilities/http)**: Fetch and post data from external APIs with decentralized consensus.
- **[EVM Read & Write](/cre/capabilities/evm-read-write)**: Interact with smart contracts on EVM-compatible blockchains with decentralized consensus.

All execution capabilities (HTTP, EVM) automatically use [built-in consensus](/cre/concepts/consensus-computing) to validate results across multiple nodes, ensuring security and reliability.

---

# The Trigger Capability
Source: https://docs.chain.link/cre/capabilities/triggers
Last Updated: 2025-11-04

**Triggers** are a special type of capability that initiate the execution of your workflow. They are event-driven services that constantly watch for a specific condition to be met. When the condition occurs, the trigger fires and instructs CRE to run the callback function you have registered for that event. Learn more about the [trigger-and-callback model](/cre/#the-trigger-and-callback-model).

## Trigger types

CRE provides several types of triggers to start your workflows:

- **Time-based:** The `Cron` trigger fires at a specific time or on a recurring schedule (e.g., "every 5 minutes").
- **Request-based:** The `HTTP` trigger fires when an external system makes an HTTP request to your workflow's endpoint. HTTP triggers require authorization keys when deployed to ensure only authorized addresses can trigger your workflow.
- **Onchain Events:** The `EVM Log` trigger fires when a specific event is emitted by a smart contract on a supported blockchain.

<Aside type="note" title="Triggers in simulation vs. deployed workflows">
  **In simulation** (`cre workflow simulate`), triggers are manually selected and executed immediately to test your workflow logic without waiting for real events. This allows rapid iteration during development.

  **In deployed workflows**, the Workflow DON continuously monitors for trigger events and automatically executes your callback when conditions are met. Time-based triggers run on schedule, HTTP triggers respond to incoming requests, and EVM log triggers fire when smart contract events occur.

  See the [Simulating Workflows guide](/cre/guides/operations/simulating-workflows) for details on testing triggers locally.
</Aside>

## Learn more

- **[Using Triggers Guides](/cre/guides/workflow/using-triggers/overview)**: Learn how to use the SDK to register handlers for the different trigger types.
- **[Triggers SDK Reference](/cre/reference/sdk/triggers/overview)**: See the detailed API reference for trigger configurations and payloads.

---

# The HTTP Capability
Source: https://docs.chain.link/cre/capabilities/http
Last Updated: 2025-11-04

The **HTTP** capability is a decentralized service that allows your workflow to securely interact with any external, offchain API. It can be used to both **fetch data from** and **send data to** other systems.

## Why use a Capability for HTTP Requests?

When your workflow needs to interact with an external API, the HTTP capability provides decentralized execution across multiple independent nodes.

When you use the SDK's `http.Client` to make a request (like a `GET` or a `POST`), you are invoking this capability. CRE instructs each node in a dedicated DON to make the same API request. The nodes' individual responses are then **validated through a consensus protocol**, which ensures they agree on the result before returning it to your workflow.

This provides cryptographically verified, tamper-proof execution for your offchain data operations.

<Aside type="note" title="HTTP in simulation vs. deployed workflows">
  **In local simulation** (`cre workflow simulate`), HTTP requests are made from your local machine to test your API integrations. The simulator processes responses using single-node consensus.

  **In deployed workflows**, the HTTP capability DON executes your API requests across multiple nodes, and responses are validated through Byzantine Fault Tolerant (BFT) consensus before being returned to your workflow.
</Aside>

## Learn more

- **[API Interactions Guide](/cre/guides/workflow/using-http-client)**: Learn how to use the SDK to invoke the HTTP capability.
- **[HTTP Client SDK Reference](/cre/reference/sdk/http-client)**: See the detailed API reference for the `http.Client`.

---

# The EVM Read & Write Capabilities
Source: https://docs.chain.link/cre/capabilities/evm-read-write
Last Updated: 2025-11-04

The **EVM Read & Write** capabilities provide a secure and reliable service for your workflow to interact with smart contracts on any EVM-compatible blockchain.

- **EVM Read:** Allows your workflow to call `view` and `pure` functions on a smart contract to read its state.
- **EVM Write:** Allows your workflow to call state-changing functions on a smart contract to write data to the blockchain.

## How it works

When you use the SDK's [`evm.Client`](/cre/reference/sdk/evm-client) to interact with a contract, you are invoking these underlying capabilities. This provides a simpler and more reliable developer experience compared to crafting raw RPC calls.

The SDKs simplify contract interactions with type-safe ABI handling (Go uses generated bindings, TypeScript uses viem), while the underlying CRE infrastructure manages consensus and transaction submission.

### Key features

- **Type-safe interactions**: SDKs provide type safety for contract calls (Go bindings offer compile-time safety, TypeScript uses viem for runtime type checking)
- **Decentralized execution**: Read and write operations are executed across multiple nodes in a DON with cryptographic verification
- **Decentralized consensus**: Multiple DON nodes independently verify read results and validate write operations before onchain submission
- **Chain selector support**: Target specific blockchains using Chainlink's chain selector system
- **Block number flexibility**: Read from finalized, latest, or a specific block number
- **Error handling**: Comprehensive error reporting for failed calls and transactions

<Aside type="note" title="EVM in simulation vs. deployed workflows">
  **In local simulation**, the simulator makes real RPC calls to your configured endpoint.

  **In deployed workflows**, the EVM capability DON executes contract calls across multiple nodes with decentralized consensus.
</Aside>

### Understanding EVM Write operations

For **EVM Write** operations, your workflow doesn't write directly to your contract. Instead, the data follows a secure multi-step flow:

1. **Your workflow** generates a cryptographically signed report with your ABI-encoded data
2. **The EVM Write capability** submits this report to a Chainlink-managed `KeystoneForwarder` contract
3. **The forwarder** validates the report's cryptographic signatures
4. **The forwarder** calls your consumer contract's `onReport(bytes metadata, bytes report)` function to deliver the data

This architecture ensures decentralized consensus, cryptographic verification, and accountability. Your consumer contract must implement the `IReceiver` interface to receive data from the forwarder.

Learn more in the [Onchain Write guide](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts).

## Learn more

- **[EVM Chain Interactions Guide](/cre/guides/workflow/using-evm-client/overview)**: Learn how to read from and write to smart contracts
- **[EVM Client SDK Reference](/cre/reference/sdk/evm-client)**: Detailed API reference for the `evm.Client`

---

# Consensus Computing in CRE
Source: https://docs.chain.link/cre/concepts/consensus-computing
Last Updated: 2025-11-04

**Consensus computing** is the foundational computing paradigm that makes CRE secure and reliable. It ensures that every operation your workflow performs—whether fetching data from an API or reading from a blockchain—is verified by multiple independent nodes before producing a final result.

## What is consensus computing?

Consensus computing is when a decentralized network of nodes must form consensus as part of executing code and storing information. Unlike traditional computing where you trust a single server or service, consensus computing provides unique guarantees:

- **Tamper-resistance**: No single node can manipulate results
- **High availability**: The network continues operating even if individual nodes fail
- **Trust minimization**: You don't need to trust any single entity
- **Verifiability**: All results are cryptographically verified

Blockchains pioneered consensus computing for maintaining asset ledgers and executing smart contracts. CRE extends this paradigm to **any offchain operation**—API calls, computations, and more.

## How CRE uses consensus

In CRE, **every execution capability automatically includes decentralized consensus**. Here's how it works:

1. **Independent execution**: When your workflow invokes a capability (like `http.Client` or `evm.Client`), each node in the dedicated capability DON performs the operation independently
2. **Result collection**: Each node produces its own result based on what it observed
3. **Consensus protocol**: The DON applies a Byzantine Fault Tolerant (BFT) consensus protocol to validate and aggregate the individual results
4. **Verified output**: A single, consensus-verified result is returned to your workflow

This process happens automatically for every capability call. You don't need to write any special code—consensus is built into the CRE runtime environment.

## Why this matters for your workflows

### Protection against node failures and manipulation

CRE's consensus model protects against individual node failures and malicious behavior. When multiple independent nodes execute the same operation:

- **Node-level resilience**: If some nodes fail or go offline, the network continues operating
- **Byzantine Fault Tolerance**: Even if some nodes are compromised and return incorrect results, the honest majority ensures the correct outcome
- **Execution consistency**: All nodes must execute your workflow logic identically, preventing manipulation by individual operators

### Validated and verified results

Every result your workflow receives has been cryptographically verified and validated across multiple nodes. This provides strong guarantees that:

- The operation was executed correctly
- The result matches what independent observers agreed upon
- No single node operator can manipulate your workflow's execution or outputs

### Unified security model

With CRE, your **entire institutional-grade smart contract**—not just the onchain parts—benefits from consensus computing. This means:

- **API responses** are validated across multiple nodes before your workflow uses them
- **Blockchain reads** are verified by multiple nodes
- **Blockchain writes** are validated by multiple nodes before being submitted onchain
- **Computation results** within your workflow are executed consistently across all nodes

Your workflow inherits the same security and reliability guarantees as blockchain transactions, but for any offchain operation.

## Consensus in practice

### HTTP capability

When your workflow makes an API request using `http.Client`, the HTTP capability DON executes your request across multiple nodes. Their responses are validated through a consensus protocol before returning a result to your workflow.

This ensures:

- Execution consistency across all nodes
- Protection against individual node compromise or failure
- Detection of inconsistent API responses (e.g., due to load balancing or timing)

See the [HTTP Capability](/cre/capabilities/http) page for details.

### EVM Read & Write capability

When your workflow reads from or writes to a blockchain using `evm.Client`, the EVM capability DON performs the operation across multiple nodes:

- **For reads**: Multiple nodes independently query the blockchain, and consensus validates their responses match
- **For writes**: Multiple nodes agree on the transaction data before submitting it onchain

This ensures:

- Execution consistency across all nodes
- Protection against individual node compromise or failure
- Validated blockchain data before use in your workflow

See the [EVM Read & Write Capability](/cre/capabilities/evm-read-write) page for details.

## Consensus in simulation vs. deployed workflows

<Aside type="note" title="Understanding consensus during development">
  **In local simulation** (`cre workflow simulate`), consensus uses a single-node model to test your workflow's logic quickly. The "consensus" step still occurs, but involves a single node wrapping its result in a standardized report structure. This allows you to build and test against the final SDK interfaces without the overhead of multi-node consensus.

  **In deployed workflows**, true multi-node Byzantine Fault Tolerant (BFT) consensus is performed across all capability DON operations, providing full decentralized security guarantees.
</Aside>

## Learn more

Learn more about how to use CRE capabilities with built-in consensus:

- **[Capabilities Overview](/cre/capabilities)**: Explore all available capabilities
- **[API Interactions](/cre/guides/workflow/using-http-client)**: Learn how to use the HTTP capability with built-in consensus
- **[EVM Chain Interactions](/cre/guides/workflow/using-evm-client/overview)**: Learn how to use the EVM capability with built-in consensus

---

# TypeScript Runtime Environment
Source: https://docs.chain.link/cre/concepts/typescript-wasm-runtime
Last Updated: 2025-11-04

Your TypeScript workflows are compiled to <a href="https://webassembly.org/" target="_blank" rel="noopener noreferrer">WebAssembly (WASM)</a> to run in the CRE execution environment. Understanding this compilation process helps you write compatible code and avoid common pitfalls.

## How TypeScript becomes WASM

CRE uses [Javy](https://github.com/bytecodealliance/javy) to compile your TypeScript workflows into WASM binaries. The compilation pipeline works as follows:

1. **TypeScript → JavaScript**: Your `.ts` files are transpiled to JavaScript
2. **JavaScript → WASM**: Javy bundles the JavaScript code with an embedded QuickJS engine into a WASM binary
3. **Execution**: The WASM binary runs in the CRE runtime environment

## QuickJS vs Node.js

Javy uses [QuickJS](https://bellard.org/quickjs), a lightweight JavaScript engine embedded in the WASM binary. **QuickJS is not Node.js**—it provides core JavaScript (ECMAScript) functionality but does not include the full Node.js API surface.

**What is supported:**

- Standard JavaScript features like `Map`, `Set`, and most ES6+ syntax are fully supported
- All CRE SDK capabilities, including EVM client, HTTP client, and consensus functions

**Key limitation:**

Not all Node.js built-in modules are available. For example, `node:crypto` is not supported. Before using third-party NPM packages, verify they don't rely on unsupported Node.js APIs.

<Aside type="note" title="Note on async/await with SDK capabilities">
  While JavaScript `Promise` and `async/await` are supported by QuickJS, **SDK capabilities do not use them**. The CRE
  TypeScript SDK uses a custom `.result()` pattern instead. See the [Core SDK
  reference](/cre/reference/sdk/core-ts#understanding-the-result-pattern) for details on why this pattern exists and how
  to use it.
</Aside>

## Checking library compatibility

Before using third-party NPM packages in your workflows, verify they don't rely on unsupported Node.js APIs. The [QuickJS Node.js compatibility documentation](https://sebastianwessel.github.io/quickjs/docs/module-resolution/node-compatibility.html) lists which Node.js modules are available.

**The best way to verify compatibility** is to test with `cre workflow simulate`. Simulation runs your workflow in the same WASM environment as production, so any compatibility issues should surface during simulation.

## Why WASM?

WebAssembly provides several benefits for CRE workflows:

- **Sandboxed execution**: WASM runs in an isolated environment, providing security boundaries between your workflow code and the CRE runtime
- **Deterministic execution**: WASM ensures workflows produce consistent results across different nodes in the DON
- **Performance**: WASM binaries are compact and execute efficiently

---

# CRE Templates
Source: https://docs.chain.link/cre/templates
Last Updated: 2025-11-21

Templates are workflow examples you can copy and customize for your own projects. They are curated and maintained by Chainlink, offering patterns you can use as references or starting points for your own workflows.


<Aside type="caution" title="Educational Example Disclaimer">
  This page includes educational examples to use a Chainlink system, product, or service and is provided to
  demonstrate how to interact with Chainlink's systems, products, and services to integrate them into your own. This
  template is provided "AS IS" and "AS AVAILABLE" without warranties of any kind, it has not been audited, and it may be
  missing key checks or error handling to make the usage of the system, product or service more clear. Do not use the
  code in this example in a production environment without completing your own audits and application of best practices.
  Neither Chainlink Labs, the Chainlink Foundation, nor Chainlink node operators are responsible for unintended outputs
  that are generated due to errors in code.
</Aside>

## Building Blocks and Starter Templates

Templates are organized into two categories:

### 1. Building Blocks

[Building Blocks](https://github.com/smartcontractkit/cre-templates/tree/main/building-blocks) are small, focused examples that teach **one concept at a time**. Each Building Block is self-contained and demonstrates a specific CRE capability or pattern. They feature minimal code, clear configuration, and are runnable locally with `cre workflow simulate`.

**Available Building Blocks:**

1. **`kv-store`** (Key-Value Store with AWS S3)
   - Reads a value from an AWS S3 object, increments it, and writes it back
   - Demonstrates: SigV4-signed HTTP requests, CRE secrets (`AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`), and consensus read → single write flow
   - Use case: Learn offchain write patterns with secrets and consensus

2. **`read-data-feeds`** (Chainlink Data Feeds)
   - Reads `decimals()` and `latestAnswer()` from <a href="https://docs.chain.link/data-feeds" target="_blank" rel="noopener noreferrer">Chainlink Data Feeds</a> on a cron schedule
   - Demonstrates: Contract ABIs, Go bindings generation, RPC configuration, and onchain reads
   - Use case: Learn how to read onchain data via contract calls

**When to use Building Blocks:**

- You want to learn a specific feature quickly (e.g., secrets + HTTP signing, reading a data feed, cron triggers)
- You need a focused code snippet to copy into your project
- You're exploring a new capability before integrating it into a larger workflow

### 2. Starter Templates

[Starter Templates](https://github.com/smartcontractkit/cre-templates/tree/main/starter-templates) are complete, end-to-end workflows that combine multiple capabilities and mirror real-world use cases. These templates are more comprehensive than Building Blocks and include production-like configuration, optional precompiled smart contracts, and generated bindings. They can be adapted directly into your own projects.

**Available Starter Templates:**

1. **`custom-data-feed`** (Custom Data Feed)
   - Periodically fetches offchain data via HTTP and pushes updates onchain
   - Demonstrates: Cron scheduling, secrets management, contract bindings, and blockchain writes
   - Use case: Build data feeds that combine offchain APIs with onchain smart contracts

2. **`bring-your-own-data`** (BYOD - NAV & PoR)
   - End-to-end examples for publishing Net Asset Value (NAV) and Proof of Reserve (PoR) data onchain
   - Demonstrates: Complete workflow with demo contracts for publishing institutional data
   - Use case: Publish verified financial or reserve data to smart contracts

3. **`multi-chain-token-manager`** (Multi-Chain Token Manager)
   - Orchestrates token operations and state across multiple blockchain networks
   - Demonstrates: Multi-chain RPC configuration, bindings, and cross-chain coordination patterns
   - Use case: Manage token operations across different EVM chains from a single workflow

**When to use Starter Templates:**

- You want a runnable reference architecture that shows production-ready patterns
- You're starting a new project and need a solid foundation
- You want to see how multiple CRE capabilities integrate in a real workflow

### How to access templates

All templates are available on GitHub at [github.com/smartcontractkit/cre-templates](https://github.com/smartcontractkit/cre-templates)

- Browse all Building Blocks and Starter Templates
- Clone or fork templates to customize them for your use case
- View individual READMEs for detailed usage instructions

<Aside type="tip" title="Quick Start: Custom Data Feed via CLI">
  The **Custom Data Feed** starter template is also available through `cre init` for a guided, interactive setup
  experience. We provide a step-by-step guide: [Running a Demo Workflow](/cre/templates/running-demo-workflow).
</Aside>

## When to use what

Choose the right template based on where you are in your CRE journey:

| Your Situation                                                           | Recommended Action                                                               |
| ------------------------------------------------------------------------ | -------------------------------------------------------------------------------- |
| Just finished the [Getting Started guide](/cre/getting-started/overview) | Run the **Custom Data Feed** template to see a more complex, real-world example  |
| Want to learn offchain writes with secrets and consensus                 | Clone the **`kv-store`** Building Block (AWS S3 example)                         |
| Need to read Chainlink Data Feeds in your workflow                       | Clone the **`read-data-feeds`** Building Block (onchain reads with ABIs)         |
| Building a custom data feed for your protocol                            | Fork the **`custom-data-feed`** Starter Template and customize it                |
| Publishing NAV or PoR data onchain                                       | Fork the **`bring-your-own-data`** Starter Template (includes demo contracts)    |
| Managing tokens across multiple chains                                   | Fork the **`multi-chain-token-manager`** Starter Template (cross-chain patterns) |

## Next steps

Ready to explore templates?

- **[Run the Custom Data Feed Template](/cre/templates/running-demo-workflow)** - Walk through this template with our step-by-step guide (available in Go and TypeScript)

- **[Browse all templates on GitHub](https://github.com/smartcontractkit/cre-templates)** - Explore the full collection of Building Blocks and Starter Templates

<Aside type="note" title="Contributing">
  The CRE Templates repository is open source under the MIT license. If you've built a useful workflow pattern and want
  to share it with the community, consider contributing.
</Aside>

---

# AI-Powered Prediction Market
Source: https://docs.chain.link/cre/demos/prediction-market
Last Updated: 2025-11-21

This demo showcases a complete, end-to-end prediction market application that integrates CRE with external services. Unlike [templates](/cre/templates) that are meant to be copied and customized, this demo is designed to be explored and studied to understand what's possible with CRE.


<Aside type="caution" title="Educational Example Disclaimer">
  This page includes educational examples to use a Chainlink system, product, or service and is provided to
  demonstrate how to interact with Chainlink's systems, products, and services to integrate them into your own. This
  template is provided "AS IS" and "AS AVAILABLE" without warranties of any kind, it has not been audited, and it may be
  missing key checks or error handling to make the usage of the system, product or service more clear. Do not use the
  code in this example in a production environment without completing your own audits and application of best practices.
  Neither Chainlink Labs, the Chainlink Foundation, nor Chainlink node operators are responsible for unintended outputs
  that are generated due to errors in code.
</Aside>

## GitHub Repository

The [repository](https://github.com/smartcontractkit/cre-gcp-prediction-market-demo) contains complete setup instructions, including quick start guides and full end-to-end deployment steps.

## What this demo does

This demo showcases a fully automated prediction market where:

1. **Users create markets** by asking binary (Yes/No) questions onchain
2. **Users stake ERC-20 tokens** (USDC) to make predictions
3. **After the market closes**, anyone can request settlement
4. **CRE automatically triggers** when it detects the settlement request event
5. **Gemini AI determines the outcome** using Google search grounding for factual verification
6. **CRE submits a cryptographically signed settlement** report back onchain
7. **Settlement data is stored in Firestore** for audit trails and frontend display
8. **Winners claim their proportional share** of the total pool

## Key integrations

This demo demonstrates several important CRE integration patterns:

### Event-driven workflows

The CRE workflow listens for `SettlementRequested` events onchain using CRE's [EVM log trigger capability](/cre/guides/workflow/using-triggers/evm-log-trigger). When a settlement is requested, the workflow automatically triggers and begins the settlement process.

### External AI service integration

The workflow sends the market question to Google's Gemini AI with search grounding enabled. Gemini searches the web to determine the factual outcome and returns a result with a confidence score.

### Data persistence

Settlement results, including the AI's response and confidence level, are stored in [Firebase Firestore](https://firebase.google.com/docs/firestore). This creates a trail and enables the frontend to display settlement history.

### Cryptographic signing

The workflow generates cryptographically signed settlement reports that the smart contract can verify. This ensures only authorized, consensus-approved data reaches the contract.

### Full-stack architecture

The demo includes:

- **Smart contracts** (Solidity + Foundry): `SimpleMarket.sol` for managing prediction markets
- **CRE workflow** (TypeScript): Event-driven settlement logic with AI integration
- **Database** (Firebase/Firestore): Audit trail and data persistence
- **Frontend** (Next.js): User interface for viewing settlement history

## What you'll learn

Exploring this demo will teach you:

1. **Event-driven workflow patterns**: How to use CRE's log trigger to react to onchain events in real-time
2. **External service integration**: Best practices for integrating AI services like Gemini with your workflows
3. **Error handling and validation**: How to validate external API responses before using them onchain
4. **Data persistence patterns**: Techniques for storing workflow results in external databases
5. **Full-stack dApp architecture**: How CRE fits into a complete application with smart contracts, workflows, and frontends
6. **Cryptographic signing**: How CRE generates and submits cryptographically signed reports

## Getting started

The repository includes two quick start options:

### Option 1: Test CRE Workflow Only (Fastest)

Test the workflow using a pre-deployed contract and transaction. This lets you see the workflow in action without deploying anything yourself.

### Option 2: Full End-to-End Test

Deploy your own contracts, create markets, request settlements, and run the workflow end-to-end. This gives you complete hands-on experience with all components.

See the [repository README](https://github.com/smartcontractkit/cre-gcp-prediction-market-demo) for detailed prerequisites and setup instructions.

## Next steps

- **[Explore the repository](https://github.com/smartcontractkit/cre-gcp-prediction-market-demo)** - Clone the demo and follow the setup instructions
- **[Learn about log triggers](/cre/guides/workflow/using-triggers/evm-log-trigger)** - Understand how to build event-driven workflows
- **[Explore HTTP capabilities](/cre/guides/workflow/using-http-client)** - Learn more about integrating external APIs
- **[Browse templates](/cre/templates)** - Check out workflow templates you can copy and customize

---

# CLI Reference
Source: https://docs.chain.link/cre/reference/cli
Last Updated: 2025-11-20

<Aside type="note" title="Required CLI Version: v1.0.2">
  To ensure compatibility with the guides and examples in this documentation, please use version `v1.0.2` of the CRE
  CLI. You can check your installed version by running `cre version`. Refer to the [CLI
  Installation](/cre/getting-started/cli-installation/macos-linux) guide for more information.
</Aside>

The CRE Command Line Interface (CLI) is your primary tool for developing, testing, deploying, and managing workflows. It handles project setup, contract binding generation (Go workflows only), local simulation, and workflow lifecycle management.

## Global flags

These flags can be used with any `cre` command.

| Flag                 | Description                                                                                                                                                                                |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `-h, --help`         | Displays help information for any command                                                                                                                                                  |
| `-e, --env`          | Specifies the path to your `.env` file (default: `".env"`)                                                                                                                                 |
| `-T, --target`       | Sets the target environment from your configuration files                                                                                                                                  |
| `-R, --project-root` | Specifies the path to the project root directory. By default, the CLI automatically finds the project root by searching for `project.yaml` in the current directory and parent directories |
| `-v, --verbose`      | Enables verbose logging to print `DEBUG` level logs                                                                                                                                        |

## Commands overview

### Authentication

Manage your authentication and account credentials.

- **`cre login`** — Authenticate with the CRE UI and save credentials locally
- **`cre logout`** — Revoke authentication tokens and remove local credentials
- **`cre whoami`** — Show your current account details

[View authentication commands →](/cre/reference/cli/authentication)

***

### Project Setup

Initialize projects and generate contract bindings (Go only).

- **`cre init`** — Initialize a new CRE project with an interactive setup guide
- **`cre generate-bindings`** (Go only) — Generate Go bindings from contract ABI files for type-safe contract interactions

[View project setup commands →](/cre/reference/cli/project-setup)

***

### Account Management

Manage your linked public key addresses for workflow operations.

- **`cre account link-key`** — Link a public key address to your account
- **`cre account list-key`** — List workflow owners linked to your organization
- **`cre account unlink-key`** — Unlink a public key address from your account

[View account management commands →](/cre/reference/cli/account)

***

### Workflow Commands

Manage workflows throughout their entire lifecycle.

- **`cre workflow simulate`** — Compile and execute workflows in a local simulation environment
- **`cre workflow deploy`** — Deploy a workflow to the Workflow Registry contract
- **`cre workflow activate`** — Activate a workflow on the Workflow Registry contract
- **`cre workflow pause`** — Pause a workflow on the Workflow Registry contract
- **`cre workflow delete`** — Delete all versions of a workflow from the Workflow Registry

[View workflow commands →](/cre/reference/cli/workflow)

***

### Secrets Management

Manage secrets stored in the Vault DON for use in your workflows.

- **`cre secrets create`** — Create new secrets from a YAML file
- **`cre secrets update`** — Update existing secrets
- **`cre secrets delete`** — Delete secrets
- **`cre secrets list`** — List secret identifiers in a namespace

[View secrets management commands →](/cre/reference/cli/secrets)

***

### Utilities

Additional utility commands.

- **`cre update`** — Update the CRE CLI to the latest version
- **`cre version`** — Print the current version of the CRE CLI

[View utility commands →](/cre/reference/cli/utilities)

## Typical development workflow

The typical workflow development process uses these commands in sequence:

1. **`cre init`** — Initialize your project
2. **`cre generate-bindings`** — Generate contract bindings from ABIs (Go workflows only, if interacting with contracts)
3. **`cre workflow simulate`** — Test your workflow locally
4. **`cre workflow deploy`** — Deploy your workflow to the registry
5. **`cre workflow activate`** / **`cre workflow pause`** — Control workflow execution

## Learn more

- **[CLI Installation](/cre/getting-started/cli-installation)** — How to install and set up the CRE CLI
- **[Getting Started](/cre/getting-started)** — Step-by-step tutorials using the CLI
- **[Project Configuration](/cre/reference/project-configuration)** — Understanding project structure and configuration files

---

# Authentication Commands
Source: https://docs.chain.link/cre/reference/cli/authentication
Last Updated: 2025-11-04

The authentication commands manage your CRE credentials and account information.

<Aside type="note" title="Global flags">
  All `cre` commands support [global flags](/cre/reference/cli#global-flags) like `--env`, `--target`, `--project-root`,
  and `--verbose`.
</Aside>

## `cre login`

Starts the authentication flow. This command opens your browser for user login and saves your credentials locally.

**Usage:**

```bash
cre login
```

**Authentication steps:**

1. The CLI opens your default web browser to the login page
2. Enter your account email address
3. Enter your account password
4. Enter your one-time password (OTP) from your authenticator app
5. The CLI automatically captures and saves your credentials locally

## `cre logout`

Revokes authentication tokens and removes local credentials. This invalidates the current authentication tokens and deletes stored credentials from your machine.

**Usage:**

```bash
cre logout
```

## `cre whoami`

Shows your current account details. This command fetches and displays your account information, including your email and organization ID.

**Usage:**

```bash
cre whoami
```

<Aside type="note" title="Verify authentication">
  Use `cre whoami` to verify you're logged in and check which account is currently active. This is useful before
  deploying workflows or performing other authenticated operations.
</Aside>

## Authentication workflow

The typical authentication flow:

1. **`cre login`** — Authenticate with the CRE UI (browser-based)
2. **`cre whoami`** — Verify your authentication and account details
3. **Perform CLI operations** — Deploy, manage workflows, etc.
4. **`cre logout`** — (Optional) Revoke credentials when done

## Learn more

- [Getting Started](/cre/getting-started) — Includes authentication setup in the initial steps
- [Account Management](/cre/reference/cli/account) — Link keys after authentication
- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Requires authentication

---

# Account Management
Source: https://docs.chain.link/cre/reference/cli/account
Last Updated: 2025-11-04

The `cre account` commands manage your linked public key addresses for workflow operations. These commands allow you to link wallet addresses to your CRE account, list linked addresses, and unlink them when needed.


<Aside type="note" title="Global flags">
  All `cre` commands support [global flags](/cre/reference/cli#global-flags) like `--env`, `--target`, `--project-root`, and `--verbose`.
</Aside>

## `cre account link-key`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Links a public key address to your account for workflow operations. This command reads your private key from the `.env` file (for EOA) or uses your configuration (for multi-sig), derives the public address, and registers it onchain in the Workflow Registry contract.

For a complete step-by-step guide with examples, see [Linking Wallet Keys](/cre/organization/linking-keys).


<Aside type="caution" title="Prerequisites for link-key">
  - **Must run from a project directory** with a `project.yaml` file
  - **`.env` must contain `CRE_ETH_PRIVATE_KEY`** — Your wallet's private key
  - **Wallet must be funded** with ETH on Ethereum Mainnet to pay gas fees
  - **RPC endpoint configured** — Add an `ethereum-mainnet` RPC URL in your `project.yaml` under the `rpcs` section for your target (e.g., `https://ethereum-rpc.publicnode.com` or your preferred provider)
  - **For multi-sig wallets**: Also specify `workflow-owner-address` in your `project.yaml` under the `account` section. See [Linking Wallet Keys](/cre/organization/linking-keys#using-multi-sig-wallets) for details.
</Aside>


<Aside type="note" title="Transaction is sent to Ethereum Mainnet">
  This command always submits a transaction to **Ethereum Mainnet**, regardless of your workflow deployment environment. Ensure your wallet has sufficient ETH for gas fees and your `project.yaml` includes an RPC configuration for `ethereum-mainnet`:

  ```yaml
  rpcs:
    - chain-name: ethereum-mainnet
      url: https://ethereum-rpc.publicnode.com # Or your preferred RPC provider
  ```
</Aside>

**Usage:**

```bash
cre account link-key [flags]
```

**Flags:**

| Flag                | Description                                                     |
| ------------------- | --------------------------------------------------------------- |
| `-l, --owner-label` | Label for the workflow owner                                    |
| `--unsigned`        | Return the raw transaction instead of sending it to the network |
| `--yes`             | Skip the confirmation prompt and proceed with the operation     |

**Interactive flow:**

When you run this command, the CLI will:

1. Extract your public address from the private key in your `.env` file
2. Prompt you to provide a label for this address (e.g., "Production Wallet")
3. Check if the address is already linked
4. Display transaction details (chain, contract, estimated gas cost)
5. Ask for confirmation to execute the transaction
6. Submit the transaction and provide a block explorer link

**Examples:**

- Interactive mode (recommended)

  ```bash
  cre account link-key
  ```

- Non-interactive mode with label

  ```bash
  cre account link-key --owner-label "My Production Wallet" --yes
  ```

## `cre account list-key`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Lists all public key addresses linked to your organization. This shows the verification status, chain information, and contract addresses for each linked key.

For a complete guide on linking and managing keys, see [Linking Wallet Keys](/cre/organization/linking-keys).

**Usage:**

```bash
cre account list-key
```

**Example output:**

```bash
Workflow owners retrieved successfully:

Linked Owners:

  1. my-production-wallet
     Owner Address:     <your_public_address>
     Status:            VERIFICATION_STATUS_SUCCESSFULL
     Verified At:       <timestamp>
     Chain Selector:    <chain_selector>
     Contract Address:  <Workflow_Registry_address>
```

## `cre account unlink-key`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Unlinks a previously linked public key address from your account. This is a destructive operation that removes the address from the Workflow Registry contract and deletes all workflows registered under that address.

For a complete guide on linking and unlinking keys, see [Linking Wallet Keys](/cre/organization/linking-keys).


<Aside type="caution" title="Destructive operation">
  **Unlinking will delete all workflows registered under this address.** This action cannot be undone. Make sure you want to permanently remove all workflows associated with this address before proceeding.
</Aside>


<Aside type="note" title="Prerequisites for unlink-key">
  - Must run from a project directory with a `project.yaml` file
  - **`.env` must contain `CRE_ETH_PRIVATE_KEY`** — The private key of the address you want to unlink
  - Wallet must be funded with ETH on Ethereum Mainnet to pay gas fees
</Aside>

**Usage:**

```bash
cre account unlink-key [flags]
```

**Flags:**

| Flag         | Description                                                     |
| ------------ | --------------------------------------------------------------- |
| `--unsigned` | Return the raw transaction instead of sending it to the network |
| `--yes`      | Skip the confirmation prompt and proceed with the operation     |

**Interactive flow:**

When you run this command, the CLI will:

1. Extract your public address from the private key in your `.env` file (for EOA) or use your configuration (for multi-sig)
2. **Display a destructive action warning** about deleting all workflows
3. Ask for first confirmation to proceed with unlinking
4. Display transaction details (chain, contract, estimated gas cost)
5. Ask for second confirmation to execute the transaction
6. Submit the transaction and provide a block explorer link

---

# Workflow Commands
Source: https://docs.chain.link/cre/reference/cli/workflow
Last Updated: 2025-11-04

The `cre workflow` commands manage workflows throughout their entire lifecycle, from local testing to deployment and ongoing management.

## `cre workflow simulate`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Compiles your workflow to WASM and executes it in a local simulation environment. This is the core command for testing and debugging your workflow.

**Usage:**

```bash
cre workflow simulate <workflow-name-or-path> [flags]
```

**Arguments:**

- `<workflow-name-or-path>` — (Required) Workflow folder name (e.g., `my-workflow`) or path (e.g., `./my-workflow`). When run from the project root, you can use just the folder name. The CLI looks for a `workflow.yaml` file in the workflow directory.

**Flags:**

| Flag                      | Description                                                                                        |
| ------------------------- | -------------------------------------------------------------------------------------------------- |
| `--broadcast`             | Broadcast onchain write transactions (default: `false`). Without this flag, a dry run is performed |
| `-g, --engine-logs`       | Enable non-fatal engine logging                                                                    |
| `--non-interactive`       | Run without prompts; requires `--trigger-index` and inputs for the selected trigger type           |
| `--trigger-index <int>`   | Index of the trigger to run (0-based). Required when using `--non-interactive`                     |
| `--http-payload <string>` | HTTP trigger payload as JSON string or path to JSON file (with or without `@` prefix)              |
| `--evm-tx-hash <string>`  | EVM trigger transaction hash (`0x...`). For EVM log triggers                                       |
| `--evm-event-index <int>` | EVM trigger log index (0-based). For EVM log triggers                                              |

**Examples:**

- Basic simulation

  ```bash
  cre workflow simulate ./my-workflow --target local-simulation
  ```

- Broadcast real onchain transactions

  ```bash
  cre workflow simulate ./my-workflow --broadcast --target local-simulation
  ```


<Aside type="note" title="Dry run by default">
  By default, `cre workflow simulate` performs a **dry run** for onchain write operations. It simulates the transaction to confirm it would succeed, but does not broadcast it to the network. This results in a successful log with an empty transaction hash (`0x`). To send a real transaction, use the `--broadcast` flag.
</Aside>

## `cre workflow deploy`


<Aside type="note" title="Deployment access required">
  Deploying workflows requires Early Access approval. If you don't have deployment access yet, <a href="https://cre.chain.link/request-access" target="_blank" rel="noopener noreferrer">request it here</a>.

  **While you wait:** Continue building and simulating workflows using [`cre workflow simulate`](/cre/guides/operations/simulating-workflows).
</Aside>

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Deploys a workflow to the Workflow Registry contract. This command compiles your workflow, uploads the artifacts to the CRE Storage Service, and registers the workflow onchain.

**Usage:**

```bash
cre workflow deploy <workflow-name-or-path> [flags]
```

**Arguments:**

- `<workflow-name-or-path>` — (Required) Workflow folder name (e.g., `my-workflow`) or path (e.g., `./my-workflow`)

**Flags:**

| Flag           | Description                                                                                    |
| -------------- | ---------------------------------------------------------------------------------------------- |
| `-o, --output` | Output file for the compiled WASM binary encoded in base64 (default: `"./binary.wasm.br.b64"`) |
| `--unsigned`   | Return the raw transaction instead of sending it to the network                                |
| `--yes`        | Skip the confirmation prompt and proceed with the operation                                    |

**Examples:**

- Deploy workflow

  ```bash
  cre workflow deploy my-workflow --target production-settings
  ```

- Deploy and save the compiled binary to a custom location

  ```bash
  cre workflow deploy my-workflow --output ./dist/workflow.wasm.br.b64
  ```

For more details, see [Deploying Workflows](/cre/guides/operations/deploying-workflows).

## `cre workflow activate`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Changes the workflow status to active on the Workflow Registry contract. Active workflows can respond to their configured triggers.

**Usage:**

```bash
cre workflow activate <workflow-name-or-path> [flags]
```

**Arguments:**

- `<workflow-name-or-path>` — (Required) Workflow folder name (e.g., `my-workflow`) or path (e.g., `./my-workflow`)

**Flags:**

| Flag         | Description                                                     |
| ------------ | --------------------------------------------------------------- |
| `--unsigned` | Return the raw transaction instead of sending it to the network |
| `--yes`      | Skip the confirmation prompt and proceed with the operation     |

**Example:**

```bash
cre workflow activate ./my-workflow --target production-settings
```

For more details, see [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows).

## `cre workflow pause`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Changes the workflow status to paused on the Workflow Registry contract. Paused workflows will not respond to triggers.

**Usage:**

```bash
cre workflow pause <workflow-name-or-path> [flags]
```

**Arguments:**

- `<workflow-name-or-path>` — (Required) Workflow folder name (e.g., `my-workflow`) or path (e.g., `./my-workflow`)

**Flags:**

| Flag         | Description                                                     |
| ------------ | --------------------------------------------------------------- |
| `--unsigned` | Return the raw transaction instead of sending it to the network |
| `--yes`      | Skip the confirmation prompt and proceed with the operation     |

**Example:**

```bash
cre workflow pause ./my-workflow --target production-settings
```


<Aside type="caution" title="Pausing stops trigger execution">
  When you pause a workflow, it will no longer respond to any triggers. Ensure this is intentional before pausing production workflows.
</Aside>

For more details, see [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows).

## `cre workflow delete`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Deletes a workflow from the Workflow Registry.

**Usage:**

```bash
cre workflow delete <workflow-name-or-path> [flags]
```

**Arguments:**

- `<workflow-name-or-path>` — (Required) Workflow folder name (e.g., `my-workflow`) or path (e.g., `./my-workflow`)

**Flags:**

| Flag         | Description                                                     |
| ------------ | --------------------------------------------------------------- |
| `--unsigned` | Return the raw transaction instead of sending it to the network |
| `--yes`      | Skip the confirmation prompt and proceed with the operation     |

**Example:**

```bash
cre workflow delete ./my-workflow --target production-settings
```


<Aside type="caution" title="Destructive operation">
  This command **permanently deletes the workflow**. The `--yes` flag bypasses the confirmation prompt for this destructive operation. Use with caution.
</Aside>

For more details, see [Deleting Workflows](/cre/guides/operations/deleting-workflows).

## Workflow lifecycle

The typical workflow lifecycle uses these commands in sequence:

1. **Develop locally** — Write and iterate on your workflow code
2. **`cre workflow simulate`** — Test your workflow in a local simulation environment
3. **`cre workflow deploy`** — Deploy your workflow to the registry
4. **`cre workflow pause`** / **`cre workflow activate`** — Control workflow execution as needed
5. **`cre workflow deploy`** (again) — Deploy updates (replaces the existing workflow)
6. **`cre workflow delete`** — Remove the workflow when no longer needed

## Learn more

- [Deploying Workflows](/cre/guides/operations/deploying-workflows) — Detailed deployment guide
- [Activating & Pausing Workflows](/cre/guides/operations/activating-pausing-workflows) — Managing workflow state
- [Updating Deployed Workflows](/cre/guides/operations/updating-deployed-workflows) — Version management
- [Deleting Workflows](/cre/guides/operations/deleting-workflows) — Cleanup and removal

---

# Secrets Management Commands
Source: https://docs.chain.link/cre/reference/cli/secrets
Last Updated: 2025-11-04

<Aside type="note" title="Authentication required">
  Running the commands on this page requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to
  verify you're logged in, or run `cre login` to authenticate. See [Creating Your
  Account](/cre/account/creating-account) and [Logging in with the CLI](/cre/account/cli-login) for further details.
</Aside>

The `cre secrets` commands manage secrets stored in the Vault DON (Decentralized Oracle Network) for deployed workflows. These commands allow you to create, update, delete, and list secrets that your workflows can access at runtime.

## Namespaces

Secrets are organized into **namespaces**, which act as logical groupings (e.g., `"main"`, `"staging"`, `"production"`). All secrets are stored in the `"main"` namespace by default. Currently, `create`, `update`, and `delete` commands only support the default namespace. Custom namespace support may be added in future CLI versions.


<Aside type="note" title="Prerequisites">
  - You must be logged in with `cre login`
  - Your `workflow-owner-address` must be configured in your project
  - For comprehensive guides on using secrets, see [Managing Secrets](/cre/guides/workflow/secrets)
</Aside>


<Aside type="note" title="Global flags">
  All `cre` commands support [global flags](/cre/reference/cli#global-flags) like `--env`, `--target`, `--project-root`, and `--verbose`.
</Aside>

## cre secrets create

Creates new secrets in the Vault DON from a YAML file.

### Usage

```bash
cre secrets create [SECRETS_FILE_PATH] [flags]
```

### Arguments

- `SECRETS_FILE_PATH` — (Required) Path to a YAML file containing the secrets to create

### Flags

| Flag         | Type     | Default | Description                                                     |
| ------------ | -------- | ------- | --------------------------------------------------------------- |
| `--timeout`  | duration | `48h`   | Timeout for the operation (e.g., `30m`, `2h`, `48h`). Max: `7d` |
| `--unsigned` | boolean  | `false` | Generate raw transaction data for multi-sig wallets             |

### Input file format

YAML file with `secretsNames` structure:

```yaml
secretsNames:
  API_KEY:
    - API_KEY_VALUE

  DATABASE_URL:
    - DATABASE_URL_VALUE
```

- `secretsNames` — Top-level key containing all secrets
- Each secret key (e.g., `API_KEY`) maps to an array containing an environment variable name
- Secret values are read from environment variables or `.env` file


<Aside type="note" title="Default namespace">
  Secrets are stored in the `"main"` namespace automatically. Custom namespace support may be added in future CLI versions.
</Aside>

### Examples

- Create secrets from YAML file

  ```bash
  cre secrets create my-secrets.yaml --target production-settings
  ```

- Create secrets with custom timeout

  ```bash
  cre secrets create my-secrets.yaml --timeout 1h
  ```

- Create secrets for multi-sig wallets

  ```bash
  cre secrets create my-secrets.yaml --unsigned
  ```

## cre secrets update

Updates existing secrets in the Vault DON from a YAML file.

### Usage

```bash
cre secrets update [SECRETS_FILE_PATH] [flags]
```

### Arguments

- `SECRETS_FILE_PATH` — (Required) Path to a YAML file containing the secrets to update

### Flags

| Flag         | Type     | Default | Description                                                     |
| ------------ | -------- | ------- | --------------------------------------------------------------- |
| `--timeout`  | duration | `48h`   | Timeout for the operation (e.g., `30m`, `2h`, `48h`). Max: `7d` |
| `--unsigned` | boolean  | `false` | Generate raw transaction data for multi-sig wallets             |

### Input file format

Same YAML format as `create`.

### Examples

- Update secrets

  ```bash
  cre secrets update my-secrets.yaml --target production-settings
  ```

- Update secrets with custom timeout

  ```bash
  cre secrets update my-secrets.yaml --timeout 6h
  ```


<Aside type="note" title="Update behavior">
  Only modifies secrets that already exist. To create new secrets, use `cre secrets create`.
</Aside>

## cre secrets delete

Deletes secrets from the Vault DON based on a YAML file.

### Usage

```bash
cre secrets delete [SECRETS_FILE_PATH] [flags]
```

### Arguments

- `SECRETS_FILE_PATH` — (Required) Path to a YAML file containing the secrets to delete

### Flags

| Flag         | Type     | Default | Description                                                     |
| ------------ | -------- | ------- | --------------------------------------------------------------- |
| `--timeout`  | duration | `48h`   | Timeout for the operation (e.g., `30m`, `2h`, `48h`). Max: `7d` |
| `--unsigned` | boolean  | `false` | Generate raw transaction data for multi-sig wallets             |

### Input file format

YAML file with a simple list of secret identifiers to delete:

```yaml
secretsNames:
  - API_KEY
  - OLD_SECRET
```


<Aside type="note" title="Deletion format">
  The `delete` command uses a simpler format than `create`/`update`: just a list of secret IDs. All secrets are deleted from the `"main"` namespace.
</Aside>

### Example

```bash
cre secrets delete secrets-to-delete.yaml --target production-settings
```


<Aside type="caution" title="Permanent deletion">
  Deleting secrets is permanent and cannot be undone.
</Aside>

## cre secrets list

Lists all secret identifiers for your owner address in a specific namespace.

### Usage

```bash
cre secrets list [flags]
```

### Flags

| Flag          | Type     | Default  | Description                                                     |
| ------------- | -------- | -------- | --------------------------------------------------------------- |
| `--namespace` | string   | `"main"` | Namespace to list secrets from                                  |
| `--timeout`   | duration | `48h`    | Timeout for the operation (e.g., `30m`, `2h`, `48h`). Max: `7d` |
| `--unsigned`  | boolean  | `false`  | Generate raw transaction data for multi-sig wallets             |

### Example

- List secrets in default namespace

  ```bash
  cre secrets list --target production-settings
  ```

- List secrets in specific namespace

  ```bash
  cre secrets list --namespace production
  ```

### Output

Returns secret identifiers (not values) for the specified namespace:

```
Secret identifiers in namespace 'main':
  - API_KEY
  - DATABASE_URL
  - WEBHOOK_SECRET
```


<Aside type="note" title="Security">
  Only returns secret identifiers, never the actual values. Secret values are only accessible to workflows at runtime.
</Aside>

## Using with multi-sig wallets

All commands support the `--unsigned` flag for multi-sig operations:

```bash
cre secrets create my-secrets.yaml --unsigned
```

When `--unsigned` is used:

1. CLI generates raw transaction data instead of broadcasting
2. Transaction payload is returned for submission through your multi-sig interface
3. After multi-sig confirmation, the secrets operation proceeds

For details, see [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets).

## Learn more

- [Managing Secrets](/cre/guides/workflow/secrets) — Overview and decision tree for secrets management
- [Using Secrets in Simulation](/cre/guides/workflow/secrets/using-secrets-simulation) — For local development
- [Using Secrets with Deployed Workflows](/cre/guides/workflow/secrets/using-secrets-deployed) — Complete guide with examples
- [Managing Secrets with 1Password](/cre/guides/workflow/secrets/managing-secrets-1password) — Best practice for secure management
- [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets) — Multi-sig configuration

---

# Utility Commands
Source: https://docs.chain.link/cre/reference/cli/utilities
Last Updated: 2025-11-20

Utility commands provide helpful information and troubleshooting capabilities.


<Aside type="note" title="Global flags">
  All `cre` commands support [global flags](/cre/reference/cli#global-flags) like `--env`, `--target`, `--project-root`, and `--verbose`.
</Aside>

## `cre update`

Updates the CRE CLI to the latest version. This command automatically downloads and installs the newest release, making it easy to stay up to date.

**Usage:**

```bash
cre update
```

**Behavior:**

- Checks for the latest available version on <a href="https://github.com/smartcontractkit/cre-cli" target="_blank" rel="noopener noreferrer">GitHub</a>
- Compares it with your currently installed version
- Automatically downloads and installs the update if a newer version is available
- Downloads the appropriate binary for your operating system and architecture
- Replaces the existing CLI binary with the new version


<Aside type="tip" title="Automatic version checks">
  The CLI automatically checks if your version is outdated when you run certain commands. If a newer version is available, you'll see a warning message encouraging you to run `cre update`.
</Aside>

## `cre version`

Prints the current version of the CRE CLI.

**Usage:**

```bash
cre version
```

**Example output:**

```bash
cre version v1.0.2
```


<Aside type="note" title="Version compatibility">
  Always check that your CLI version matches the version recommended in the documentation. The current recommended version is **v1.0.2**. See the [CLI Installation guide](/cre/getting-started/cli-installation) for more information.
</Aside>

## Learn more

- [CLI Installation](/cre/getting-started/cli-installation) — How to install and update the CRE CLI
- [CLI Reference](/cre/reference/cli) — Complete CLI command reference

---

# Finality and Confidence Levels
Source: https://docs.chain.link/cre/concepts/finality-ts
Last Updated: 2025-12-10

Finality determines when a blockchain transaction is considered irreversible. Until a block is finalized, it could be reorganized (replaced by a different block if the chain temporarily forks), which means any data you read from it might change.

Different blockchains achieve finality in different ways and at different speeds. CRE abstracts these differences through **confidence levels**, allowing you to specify your finality requirements without needing to know the underlying chain-specific implementation.

## Understanding CRE's finality model

CRE provides three confidence levels for reading blockchain data and monitoring events. These levels work consistently across all supported chains.

### The three confidence levels

| Confidence Level | Description                                                                         | Use Case                                                                         |
| ---------------- | ----------------------------------------------------------------------------------- | -------------------------------------------------------------------------------- |
| **`LATEST`**     | The most recent block. No finality guarantees—the block could still be reorganized. | Non-critical, time-sensitive operations where speed matters more than certainty. |
| **`SAFE`**       | A block that is unlikely to be reorganized, but not yet fully finalized.            | A balance between speed and security for most operations.                        |
| **`FINALIZED`**  | A block that is considered irreversible. This is the safest option.                 | Critical operations where you need absolute certainty the data won't change.     |

**Choosing the right level:**

- **`FINALIZED`** — Use for financial transactions, critical state updates, or when incorrect data could cause significant issues
- **`SAFE`** — Use when you need reasonable confidence without waiting for full finality (good for most monitoring/alerting)
- **`LATEST`** — Use for real-time dashboards or displays where speed matters more than guaranteed accuracy

### How confidence levels map to chains

**SAFE and LATEST:**

CRE uses the chain's native `safe` and `latest` block tags respectively for all supported chains.

**FINALIZED:**

When you specify `FINALIZED` in your workflow, CRE automatically maps this to its finality method—whether it uses the native finality tag or block depth (with the number of blocks specified for block depth).

| Chain                           | Finality Method          |
| ------------------------------- | ------------------------ |
| Arbitrum One / Arbitrum Sepolia | Native `finalized` tag   |
| Avalanche / Avalanche Fuji      | Native `finalized` tag   |
| Base / Base Sepolia             | Native `finalized` tag   |
| BNB Chain / BNB Testnet         | Native `finalized` tag   |
| Ethereum / Ethereum Sepolia     | Native `finalized` tag   |
| OP Mainnet / OP Sepolia         | Native `finalized` tag   |
| Polygon / Polygon Amoy          | Block depth (500 blocks) |

## Finality for chain reads

Chain read operations ([`CallContract`](/cre/reference/sdk/evm-client-ts#callcontract), [`BalanceAt`](/cre/reference/sdk/evm-client-ts#balanceat), [`FilterLogs`](/cre/reference/sdk/evm-client-ts#filterlogs), etc.) support confidence levels and custom block depths.

### Using confidence levels

For most read operations, use the standard confidence levels by passing [`LATEST_BLOCK_NUMBER`](/cre/reference/sdk/evm-client-ts#latest_block_number) or [`LAST_FINALIZED_BLOCK_NUMBER`](/cre/reference/sdk/evm-client-ts#last_finalized_block_number) as the `blockNumber` parameter. If you don't specify a block number, CRE defaults to `LATEST`.

**Note:** The `SAFE` confidence level is not available for chain reads—only `LATEST` and `FINALIZED` are supported.

### Custom block depths

For use cases requiring fixed confirmation thresholds or historical state verification, you can specify **any explicit block number** instead of using the predefined confidence levels. This enables you to:

- Implement custom finality rules tailored to your risk profile (e.g., "always wait 1,000 blocks")
- Meet regulatory requirements that mandate fixed, auditable confirmation thresholds
- Query historical state at specific past block heights for analysis or verification

**When to use custom block depths:**

- **Regulatory compliance** — Your interactions require provable, fixed confirmation thresholds for auditors
- **Changing chain parameters** — The chain's finality definition may change, but your security model must remain constant
- **Historical verification** — You need to verify state at a specific moment

**Implementation:**

Block numbers must be provided as `BigIntJson` objects. See [Onchain Read](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths) for the conversion pattern and examples.

## Finality for chain writes

Chain write operations return a [`WriteReportReply`](/cre/reference/sdk/evm-client-ts#writereportreply) when the transaction is included in a block, not when it reaches finality.

### What a successful response means

When [`WriteReportReply`](/cre/reference/sdk/evm-client-ts#writereportreply) returns with `txStatus` equal to `TX_STATUS_SUCCESS`:

- Your transaction was **included in a block**
- The transaction is **not necessarily finalized** yet

The reply is returned as soon as the transaction appears in a block, not when the block reaches finality. This is important for time-sensitive workflows, but it means the transaction could still be reorganized.

### Reorg handling

If a block containing your transaction is reorganized:

- CRE's Transaction Manager (TXM) automatically resubmits your transaction
- Gas bumping is applied as needed to ensure the transaction is included
- **Important:** The transaction hash may change during resubmission
- You are not automatically notified if the hash changes

<Aside type="caution" title="For mission-critical applications">
  If you need absolute certainty that your write transaction reached finality, implement post-write verification by
  reading the blockchain state after a custom number of confirmations. Do not rely solely on `WriteReportReply` for
  finality confirmation.
</Aside>

## Finality for event triggers

EVM Log Triggers must use the three confidence levels (`LATEST`, `SAFE`, or `FINALIZED`). Custom block depths are not supported for triggers.

By default, triggers use `SAFE` if no confidence level is specified. For details on configuring trigger confidence levels, see [EVM Log Trigger](/cre/reference/sdk/triggers/evm-log-trigger-ts#configuration).

---

# Avoiding Non-Determinism in Workflows
Source: https://docs.chain.link/cre/concepts/non-determinism-ts
Last Updated: 2025-11-04

<Aside type="note" title="TL;DR">
  In DON mode, all nodes must execute identical code paths to reach consensus. Non-deterministic code causes nodes to
  generate different request IDs, breaking consensus. This guide shows common pitfalls and how to avoid them.
</Aside>

## The problem: Why determinism matters

When your workflow runs in DON mode, multiple nodes execute the same code independently. These nodes must reach consensus on the results before proceeding. **If nodes execute different code paths, they generate different request IDs for capability calls, and consensus fails.**

The failure pattern: Code diverges → Different request IDs → No quorum → Workflow fails

## Quick reference: Common pitfalls

| Don't Use                    | Use Instead                                  |
| ---------------------------- | -------------------------------------------- |
| Unordered object iteration   | Sort keys first, then iterate                |
| `Promise.race()`             | Call `.result()` in deterministic order      |
| `Date.now()` or `new Date()` | `runtime.now()`                              |
| LLM free-text responses      | Structured output with field-level consensus |

## 1. Object and Map iteration

JavaScript objects do not guarantee key order by specification. While modern engines preserve insertion order, relying on this behavior can cause subtle bugs, especially across different runtimes or JSON serialization.

### Objects: Order is not guaranteed

**Bad: Implicit iteration order**

```typescript
const obj = { b: 2, a: 1 }

// Order may vary across runtimes or serialization
for (const key in obj) {
  console.log(key) // Could be "b", "a" or "a", "b"
}
```

**Good: Explicit iteration with `Object.keys()`**

```typescript
const obj = { b: 2, a: 1 }

// Preserves insertion order
for (const key of Object.keys(obj)) {
  console.log(key, obj[key]) // "b", "a" (insertion order)
}
```

**Best: Deterministic sorted iteration**

```typescript
const obj = { b: 2, a: 1 }

// Guaranteed alphabetical order
for (const key of Object.keys(obj).sort()) {
  console.log(key, obj[key]) // "a", "b" (alphabetically sorted)
}
```

### Maps and Sets: Order is guaranteed

Maps and Sets preserve insertion order by specification, making them safe for deterministic iteration.

**Good: Map preserves insertion order**

```typescript
const map = new Map<string, number>()
map.set("b", 2)
map.set("a", 1)

for (const [key, value] of map) {
  console.log(key, value) // Always "b", then "a"
}
```

**Good: Set preserves insertion order**

```typescript
const set = new Set(["b", "a"])

for (const value of set) {
  console.log(value) // Always "b", then "a"
}
```

<Aside type="tip" title="When to use Maps vs Objects">
  If iteration order matters for your workflow logic, use `Map` instead of plain objects. Maps guarantee insertion order
  by specification.
</Aside>

## 2. Promise handling and the `.result()` pattern

SDK capabilities use the [`.result()` pattern](/cre/reference/sdk/core-ts#understanding-the-result-pattern) instead of traditional `async/await`. When working with multiple operations, the order in which you call `.result()` must be deterministic.

### Avoid non-deterministic Promise methods

**Bad: Promise.race() introduces non-determinism**

```typescript
// Different nodes may "win" the race
const fastest = await Promise.race([fetchFromAPI1(), fetchFromAPI2()])
```

**Bad: Promise.any() picks first success**

```typescript
// Different nodes may succeed with different sources
const firstSuccess = await Promise.any([fetchFromAPI1(), fetchFromAPI2()])
```

**Good: Deterministic order with `.result()`**

```typescript
import { cre, type Runtime, type NodeRuntime, consensusMedianAggregation } from "@chainlink/cre-sdk"

// Fetch from API 1, then API 2, in a fixed order
const fetchPrice = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()

  // Try first API
  const response1 = httpClient
    .sendRequest(nodeRuntime, {
      url: "https://api1.example.com/price",
    })
    .result()

  // If first API succeeds, use it; otherwise try second API
  if (response1.statusCode === 200) {
    return parsePriceFromResponse(response1)
  }

  // Try second API as fallback (deterministic order)
  const response2 = httpClient
    .sendRequest(nodeRuntime, {
      url: "https://api2.example.com/price",
    })
    .result()

  return parsePriceFromResponse(response2)
}

// In your DON mode handler
const onTrigger = (runtime: Runtime<Config>): MyResult => {
  // Run the fetch logic in node mode with consensus
  const price = runtime.runInNodeMode(fetchPrice, consensusMedianAggregation<bigint>())().result()

  return { price }
}
```

The key is to call `.result()` in a **fixed, deterministic order** (API 1, then API 2 if needed), not racing them.

<Aside type="note" title="Custom code vs SDK capabilities">
  You can use `async/await` and Promises in your own custom functions. However, all SDK capabilities (HTTP requests,
  blockchain interactions, secrets) use the `.result()` pattern. See the [Core SDK
  reference](/cre/reference/sdk/core-ts#understanding-the-result-pattern) for details.
</Aside>

## 3. Time and dates

Never use JavaScript's built-in time functions in DON mode. Nodes may have slightly different system clocks, causing divergence.

**Bad: Using JavaScript's time functions**

```typescript
const now = Date.now() // Different on each node
const timestamp = new Date() // Different on each node
```

**Good: Use runtime.now()**

```typescript
const now = runtime.now() // Same timestamp across all nodes
runtime.log(`Current time: ${now.toISOString()}`)
```

The `runtime.now()` method returns a `Date` object representing DON Time—a consensus-derived timestamp that all nodes agree on. See [Time in CRE](/cre/concepts/time-in-cre) for more details.

## 4. Working with LLMs

Large Language Models (LLMs) generate different responses for the same prompt, even with temperature set to 0. This inherent non-determinism breaks consensus in workflows.

**The problem:** Free-text responses from LLMs will vary across nodes, making it impossible to reach agreement on the output.

**The solution:** Request **structured output** from the LLM (such as JSON with specific fields) rather than free-form text. Then use consensus aggregation on the structured fields. This approach allows nodes to agree on the key data points even if the exact text varies slightly.

## Best practices summary

### Do:

- Sort object keys before iteration
- Use Maps and Sets when insertion order matters
- Call `.result()` in a fixed, deterministic order
- Use `runtime.now()` for all time operations
- Request structured output from LLMs

### Don't:

- Iterate objects with `for...in` without sorting keys
- Use `Promise.race()`, `Promise.any()`, or unpredictable `Promise.all()` patterns
- Use `Date.now()` or `new Date()` for timestamps
- Rely on free-text LLM responses

## Related concepts

- **[Time in CRE](/cre/concepts/time-in-cre)**: Learn about DON Time and why `runtime.now()` is required
- **[Consensus Computing](/cre/concepts/consensus-computing)**: Deep dive into how nodes reach agreement
- **[Core SDK Reference](/cre/reference/sdk/core-ts)**: Details on `Runtime`, `NodeRuntime`, and the `.result()` pattern

---

# Part 1: Project Setup & Simulation
Source: https://docs.chain.link/cre/getting-started/part-1-project-setup-ts
Last Updated: 2025-11-04

<Aside type="note" title="SDK Language: TypeScript">
  You're viewing the **TypeScript** version of this guide. If you prefer Go, use the language selector in the right
  sidebar to switch to the Go version.
</Aside>

In this first part, you'll go from an empty directory to a fully initialized CRE project and [simulate](/cre/guides/operations/simulating-workflows) your first, minimal workflow. The goal is to get a quick "win" and familiarize yourself with the core project structure and development loop.

## What you'll do

- Initialize a new project using `cre init`.
- Explore the generated project structure and workflow code.
- Configure your workflow for simulation.
- Run your first local simulation with `cre workflow simulate`.

## Prerequisites

Before you begin, ensure you have the following:

- **CRE CLI**: See the [Installation Guide](/cre/getting-started/cli-installation/macos-linux) for details.
- **CRE account & authentication**: You must have a CRE account and be logged in with the CLI. See [Create your account](/cre/account/creating-account) and [Log in with the CLI](/cre/account/cli-login) for instructions.
- **Bun**: You must have <a href="https://bun.com/docs" target="blank">Bun</a> version 1.2.21 or higher installed. Check your version with bun --version. See [Install Bun](https://bun.sh/) for instructions.
- **Funded Sepolia Account**: An account with Sepolia ETH to pay for transaction gas fees. Go to <a href="https://faucets.chain.link" target="blank">faucets.chain.link</a> to get some Sepolia ETH.

## Step 1: Verify your authentication

Before initializing your project, verify that you're logged in to the CRE CLI:

```bash
cre whoami
```

**Expected output:**

- If you're authenticated, you'll see your account details:

  ```bash
  Account details retrieved:

  Email:           email@domain.com
  Organization ID: org_AbCdEfGhIjKlMnOp
  ```

- If you're not logged in, you'll receive an error message prompting you to run `cre login`:

  ```bash
  Error: failed to attach credentials: failed to load credentials: you are not logged in, try running cre login
  ```

  Run the login command and follow the prompts:

  ```bash
  cre login
  ```

  See [Logging in with the CLI](/cre/account/cli-login) for detailed instructions if you need help.

## Step 2: Initialize your project

The CRE CLI provides an `init` command to scaffold a new project. It's an interactive process that will ask you for a project name, a workflow template, and a name for your first workflow.

1. **In your terminal, navigate to a parent directory where you want your new CRE project to live.**

2. **Run the `init` command.** The CLI will guide you through the setup process:

   ```bash
   cre init
   ```

3. **Provide the following details when prompted:**
   - **Project name**: onchain-calculator
   - **Language**: Select `Typescript` and press Enter.
   - **Pick a workflow template**: Use the arrow keys to select `Helloworld: Typescript Hello World example` and press Enter. We are starting from scratch to learn all the configuration steps.
   - **Workflow name**: my-calculator-workflow

The CLI will then create a new `onchain-calculator` directory and initialize your first workflow within it.

## Step 3: Explore the generated files

The `init` command creates a directory with a standard structure and generates your first workflow code. Let's explore what was created.

### Project structure

Your new project has the following structure:

```
onchain-calculator/
├── my-calculator-workflow/
│   ├── config.production.json
│   ├── config.staging.json
│   ├── main.ts
│   ├── package.json
│   ├── README.md
│   ├── tsconfig.json
│   └── workflow.yaml
├── .env
├── .gitignore
├── project.yaml
└── secrets.yaml
```

- **Project**: The top-level directory (e.g., `onchain-calculator/`).
  - It contains project-wide files like `project.yaml`, which holds shared configurations for all workflows within the project.
  - A project can contain multiple workflows, each in its own subdirectory.
- **Workflow**: A subdirectory (e.g., `my-calculator-workflow/`) that contains the source code and configuration for a single workflow.
  - Each workflow has its own `package.json` and `tsconfig.json`, making workflows independent and portable.

Here are the key files and their roles:

| File                         | Role                                                                                                                                                                                                                                    |
| ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `project.yaml`               | The global configuration file. Contains shared settings like RPC URLs for different environments (called `targets`).                                                                                                                    |
| `secrets.yaml`               | Stores references to secrets.                                                                                                                                                                                                           |
| `.env`                       | Stores secrets and environment variables, like your private key. Never commit this file to version control.                                                                                                                             |
| `.gitignore`                 | Prevents sensitive files (like `.env`) from being committed to version control.                                                                                                                                                         |
| `my-calculator-workflow/`    | A directory containing the source code and configuration for a single workflow.                                                                                                                                                         |
| `├── main.ts`                | The heart of your workflow where you'll write your TypeScript logic. This is the entry point that gets compiled to WASM.                                                                                                                |
| `├── package.json`           | Manages dependencies for this specific workflow. Each workflow can have its own dependencies, allowing for flexibility and isolation.                                                                                                   |
| `├── tsconfig.json`          | TypeScript configuration for this workflow. Controls how TypeScript compiles your code.                                                                                                                                                 |
| `├── workflow.yaml`          | Contains configurations specific to this workflow, such as its name and workflow artifacts (entry point path, config file path, secrets file path). The `workflow-artifacts` section tells the CLI where to find your workflow's files. |
| `├── config.staging.json`    | Contains parameters for your workflow when using the `staging-settings` target, which can be accessed in your code via the `config` object.                                                                                             |
| `└── config.production.json` | Contains parameters for your workflow when using the `production-settings` target, which can be accessed in your code via the `config` object.                                                                                          |

<Aside type="note" title="Learn More About Configuration">
  For a comprehensive guide on how `project.yaml`, `workflow.yaml`, targets, and secrets work together, see [**Project
  Configuration**](/cre/reference/project-configuration).
</Aside>

You don't need to understand every file and directory right now—this guide is designed to introduce each concept when
you actually need it. For now, let's look at the workflow code that was generated.

### The workflow code

The `init` command created a `main.ts` file with a minimal "Hello World!" workflow. Let's examine and understand this code.

This code defines a `Config` type to hold parameters from our config file. It then configures a [cron trigger](/cre/reference/sdk/triggers/cron-trigger) to run on the schedule provided in the config, and registers a simple callback that logs a message.

Open `onchain-calculator/my-calculator-workflow/main.ts` to see its contents:

Code snippet for onchain-calculator/my-calculator-workflow/main.ts:

```typescript
import { cre, Runner, type Runtime } from "@chainlink/cre-sdk"

type Config = {
  schedule: string
}

const onCronTrigger = (runtime: Runtime<Config>): string => {
  runtime.log("Hello world! Workflow triggered.")
  return "Hello world!"
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

**Key components:**

- **`Config` type**: Defines the shape of your configuration. TypeScript ensures type safety throughout your workflow.
- **`onCronTrigger`**: The callback function that executes when the cron trigger fires. It receives the `runtime` object and returns a string result. This is where you write your business logic.
- **`initWorkflow`**: Creates the workflow by registering handlers (trigger-callback pairs). This is where you define what events your workflow responds to.
- **`main`**: The entry point that creates a `Runner`, passes your config type, and runs the workflow.

## Step 4: Configure your workflow

Now that you've explored the generated files, let's configure your workflow for simulation. You'll need to adjust a few configuration files.

### Review config files

The CLI generates separate config files for each target environment. Your workflow code can access the parameters from whichever config file corresponds to the target you're using.

The `cre init` command already created these files with a schedule. Open `my-calculator-workflow/config.staging.json` to see its contents:

```json
{
  "schedule": "*/30 * * * * *"
}
```

This cron schedule means "run every 30 seconds". No changes are needed here for now. Since we'll be using the `staging-settings` target for this guide, you only need to be aware of `config.staging.json`. The `config.production.json` file contains the same schedule but is used when targeting the production environment.

### Review `workflow.yaml`

This file tells the CLI where to find your workflow files. The `cre init` command created this file with default values. Open `my-calculator-workflow/workflow.yaml` and you'll see:

```yaml
# ==========================================================================
staging-settings:
  user-workflow:
    workflow-name: "my-calculator-workflow-staging"
  workflow-artifacts:
    workflow-path: "./main.ts"
    config-path: "./config.staging.json"
    secrets-path: ""
# ==========================================================================
production-settings:
  user-workflow:
    workflow-name: "my-calculator-workflow-production"
  workflow-artifacts:
    workflow-path: "./main.ts"
    config-path: "./config.production.json"
    secrets-path: ""
```

**Understanding the sections:**

- **Target names** (`staging-settings`, `production-settings`): These are environment configuration sets. The `cre init` command pre-populates your `workflow.yaml` with these two common targets as a starting point, but you can name targets whatever you want (e.g., `dev`, `test`, `prod`). When running CLI commands, you specify which target to use with the `--target` flag.
- **`workflow-name`**: Each target has its own workflow name with a suffix (e.g., `-staging`, `-production`). This allows you to deploy the same workflow to different environments with distinct identities.
- **`workflow-path: "./main.ts"`**: The entry point for your TypeScript code
- **`config-path`**: Each target points to its own config file (`config.staging.json` or `config.production.json`)
- **`secrets-path: ""`**: The location of your secrets file (empty for now; you'll learn about secrets in more advanced guides)

For this guide, we'll use `staging-settings` for local simulation. When you run `cre workflow simulate my-calculator-workflow --target staging-settings`, the CLI reads the configuration from the `staging-settings` section of this file.

### Set up your private key

The simulator requires a private key to initialize its environment, even for workflows that don't interact with the blockchain yet. This key will be used in later parts of this guide to read from and send transactions to the Sepolia testnet.

1. Open the `.env` file located in your `onchain-calculator/` directory.
2. Add your funded Sepolia account's private key:

   ```bash
   # Replace with your own private key for your funded Sepolia account
   CRE_ETH_PRIVATE_KEY=YOUR_64_CHARACTER_PRIVATE_KEY_HERE
   ```

   <Aside type="caution" title="Use the Raw Key">
     Your private key must be the 64-character hexadecimal string. Do **not** include the `0x` prefix.
   </Aside>

<Aside type="caution" title="Never Commit .env Files">
  The `.gitignore` file included in the project already prevents `.env` files from being committed to version control.
  **Never** share your private keys or commit them to Git.
</Aside>

<Aside type="note" title="Best Practice: Use a Secrets Manager">
  While using a plaintext `.env` file is convenient for initial testing, the recommended best practice is to use a
  dedicated secrets manager. See our guide on [Managing Secrets with 1Password
  CLI](/cre/guides/workflow/secrets/managing-secrets-1password) to learn how to inject secrets securely at runtime.
</Aside>

## Step 5: Install dependencies

Before you can simulate the workflow, you need to install the workflow's dependencies.

1. Navigate to your workflow directory:

   ```bash
   cd onchain-calculator/my-calculator-workflow
   ```

2. Install the dependencies:

   ```bash
   bun install
   ```

   This command reads your `package.json` and installs all required dependencies, including the CRE SDK. The template comes with a `postinstall` script that automatically runs `bunx cre-setup` to set up the WebAssembly compilation tools.

   **Expected output:**

   ```bash
   bun install v1.2.23 (cf136713)

   $ bunx cre-setup
   [cre-sdk-javy-plugin] Detected platform: darwin, arch: arm64
   [cre-sdk-javy-plugin] Using cached binary: /Users/<user>/.cache/javy/v5.0.4/darwin-arm64/javy
   ✅ CRE TS SDK is ready to use.

   + @types/bun@1.2.21
   + @chainlink/cre-sdk@1.0.0

   30 packages installed [4.71s]
   ```

3. Return to the project root:

   ```bash
   cd ..
   ```

<Aside type="note" title="Per-Workflow Dependencies">
  Each TypeScript workflow has its own `package.json`, allowing you to have different SDK versions or additional dependencies per workflow. This makes workflows portable and easier to manage in large projects.
</Aside>

## Step 6: Run your first simulation

Now that your workflow is configured and dependencies are installed, you can run the simulation. [Workflow simulation](/cre/guides/operations/simulating-workflows) is a local execution environment that compiles your code to WebAssembly and runs it on your machine, allowing you to test and debug before deploying to a live network.

Run the `simulate` command from your project root directory (the `onchain-calculator/` folder):

```bash
cre workflow simulate my-calculator-workflow --target staging-settings
```

This command compiles your TypeScript code to WebAssembly, uses the `staging-settings` target configuration from `workflow.yaml`, and spins up a local simulation environment.

## Step 7: Review the output

After the workflow compiles, the simulator detects the single trigger you defined in your code and immediately runs the workflow.

```bash
Workflow compiled
2025-11-03T19:04:21Z [SIMULATION] Simulator Initialized

2025-11-03T19:04:21Z [SIMULATION] Running trigger trigger=cron-trigger@1.0.0
2025-11-03T19:04:21Z [USER LOG] Hello world! Workflow triggered.

Workflow Simulation Result:
 "Hello world!"

2025-11-03T19:04:21Z [SIMULATION] Execution finished signal received
2025-11-03T19:04:21Z [SIMULATION] Skipping WorkflowEngineV2
```

- **`[USER LOG]`**: This is the output from your own code—in this case, the `runtime.log()` call. This is where you will look for your custom log messages.
- **`[SIMULATION]`**: These are system-level messages from the simulator, showing its internal state (initialization, trigger execution, completion).
- **`Workflow Simulation Result: "Hello world!"`**: This is the final return value of your workflow. In TypeScript, the workflow returns whatever your handler function returns.

Congratulations! You've built and simulated your first CRE workflow in TypeScript.

## Key TypeScript concepts

Before moving to the next part, let's review some key TypeScript-specific concepts:

### Type safety with TypeScript

The TypeScript SDK provides full type safety:

```typescript
type Config = {
  schedule: string
}

const runner = await Runner.newRunner<Config>()
```

By passing the `Config` type to `Runner.newRunner<Config>()`, TypeScript ensures your workflow receives the correct configuration shape.

### Optional: Runtime validation

For runtime validation of your configuration, you can use schema validation libraries. The CRE SDK supports any library that implements the [Standard Schema](https://github.com/standard-schema/standard-schema) interface, such as [Zod](https://zod.dev/) or [ArkType](https://arktype.io/).

Here's an example using Zod:

```typescript
import { z } from "zod"

const configSchema = z.object({
  schedule: z.string(),
})

type Config = z.infer<typeof configSchema>

const runner = await Runner.newRunner<Config>({ configSchema })
```

This provides both compile-time type checking and runtime validation. We'll use Zod in later parts of this guide.

## Next steps

In the next section, you'll build on this foundation by modifying the workflow to fetch real data from an external API.

- **[Part 2: Fetching Offchain Data](/cre/getting-started/part-2-fetching-data)**

---

# Part 2: Fetching Offchain Data
Source: https://docs.chain.link/cre/getting-started/part-2-fetching-data-ts
Last Updated: 2025-11-04

In Part 1, you successfully built and ran a minimal workflow. Now, it's time to connect it to the outside world. In this section, you will modify your workflow to fetch data from a public API using the CRE SDK's [`HTTPClient`](/cre/reference/sdk/http-client).

## What you'll do

- Add a new URL to your workflow's config file.
- Learn about the `runInNodeMode` helper for offchain operations.
- Write a new function to fetch data from the public [`api.mathjs.org`](https://api.mathjs.org/) API.
- Integrate the offchain data into your main workflow logic.

## Step 1: Update your configuration

First, you need to add the API endpoint to your workflow's configuration. This allows you to easily change the URL without modifying your TypeScript code.

Open the `config.staging.json` file in your `my-calculator-workflow` directory and add the `apiUrl` key. Your file should now look like this:

```json
{
  "schedule": "*/30 * * * * *",
  "apiUrl": "https://api.mathjs.org/v4/?expr=randomInt(1,101)"
}
```

This URL calls the public mathjs.org API and uses its `randomInt(min, max)` function to return a random integer between 1 and 100. Note that the upper bound is exclusive, so we use `101` to get values up to 100. The API returns the number as a raw string in the response body.

## Step 2: Understand the `runInNodeMode` pattern

Many offchain data sources are **non-deterministic**, meaning different nodes calling the same API might get slightly different answers due to timing, load balancing, or other factors. The `api.mathjs.org` API with the `randomInt` function is a perfect example—each call will return a different random number.

The CRE SDK solves this with `runtime.runInNodeMode`, a helper function that transforms these potentially varied results into a single, highly reliable result. It works like a "map-reduce" for the DON:

1. **Map**: You provide a function (e.g., `fetchMathResult`) that will be executed by every node in the DON independently. Each node "maps" the offchain world by fetching its own version of the data.
2. **Reduce**: You provide a consensus algorithm (e.g., [`consensusMedianAggregation`](/cre/reference/sdk/consensus#consensusmedianaggregationt)) that takes all the individual results and "reduces" them into a single, trusted outcome.

This pattern is fundamental to securely and reliably bringing offchain data into your workflow.

<Aside type="note" title="Learn More">
  The `runInNodeMode` helper enables node-level execution. To learn more about the distinction between the DON-level
  `Runtime` and node-level execution, see the [Runtime](/cre/reference/sdk/core/#runtime-and-noderuntime) section in the
  SDK Reference.
</Aside>

## Step 3: Add the HTTP fetch logic

Now, let's modify your `main.ts` file. You will add a new function, `fetchMathResult`, that contains the logic for calling the API. You'll also update the `onCronTrigger` function to call the `runInNodeMode` helper.

Replace the entire content of `onchain-calculator/my-calculator-workflow/main.ts` with the following code.

Code snippet for onchain-calculator/my-calculator-workflow/main.ts:

```typescript
import { cre, consensusMedianAggregation, Runner, type NodeRuntime, type Runtime } from "@chainlink/cre-sdk"

type Config = {
  schedule: string
  apiUrl: string
}

type MyResult = {
  result: bigint
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

// fetchMathResult is the function passed to the runInNodeMode helper.
// It contains the logic for making the request and parsing the response.
const fetchMathResult = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()

  const req = {
    url: nodeRuntime.config.apiUrl,
    method: "GET" as const,
  }

  // Send the request using the HTTP client
  const resp = httpClient.sendRequest(nodeRuntime, req).result()

  // The mathjs.org API returns the result as a raw string in the body.
  // We need to parse it into a bigint.
  const bodyText = new TextDecoder().decode(resp.body)
  const val = BigInt(bodyText.trim())

  return val
}

const onCronTrigger = (runtime: Runtime<Config>): MyResult => {
  runtime.log("Hello, Calculator! Workflow triggered.")
  // Use runInNodeMode to execute the offchain fetch.
  // The API returns a random number, so each node can get a different result.
  // We use median consensus to find a single, trusted value.
  const result = runtime.runInNodeMode(fetchMathResult, consensusMedianAggregation())().result()

  runtime.log(`Successfully fetched and aggregated math result: ${result}`)

  return {
    result,
  }
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```


<Aside type="tip" title="Optional: Add runtime validation">
  If you added Zod validation in Part 1, you can continue using it here by replacing the `Config` type definition with a Zod schema and passing `configSchema` to `Runner.newRunner<Config>({ configSchema })`. See [Part 1: Optional Runtime Validation](/cre/getting-started/part-1-project-setup#optional-runtime-validation) for details.
</Aside>

<Aside type="note" title="The .result() method">
  You'll notice that capability calls like `httpClient.sendRequest()` and `runtime.runInNodeMode()` use `.result()` to
  get the response. This is because traditional `async/await` doesn't work in the WebAssembly environment where CRE
  workflows run. The `.result()` method provides a way to handle asynchronous operations within WASM's synchronous
  execution model. For a deeper explanation, see [Understanding the `.result()`
  Pattern](/cre/reference/sdk/core-ts#understanding-the-result-pattern).
</Aside>

<Aside type="note" title="Consensus & Aggregation">
  Because the `api.mathjs.org/v4/?expr=randomInt(1,101)` endpoint is non-deterministic, each node in the DON will receive a different random number. To create a single, reliable result from these varied responses, we use [`consensusMedianAggregation`](/cre/reference/sdk/consensus#consensusmedianaggregationt).

  This algorithm collects the numerical results from all nodes, sorts them, and selects the median value. This approach is robust against outliers and provides a trustworthy, aggregated number for your workflow to use, even when the underlying data source is not consistent across calls.

  **In local simulation**: The simulator uses a single-node model for faster testing. The "consensus" step still occurs, but involves one node wrapping its result in a standardized report structure. When deployed, true multi-node Byzantine Fault Tolerant (BFT) consensus is performed across all capability operations.

  To learn more about how consensus works in CRE, see [Consensus Computing](/cre/concepts/consensus-computing) and the [Consensus & Aggregation reference](/cre/reference/sdk/consensus).
</Aside>

## Step 4: Run the simulation and review the output

Run the `simulate` command from your project root directory (the `onchain-calculator/` folder). Because there is only one trigger, the simulator runs it automatically.

```bash
cre workflow simulate my-calculator-workflow --target staging-settings
```

The output shows the new user logs from your workflow, followed by the final `Workflow Simulation Result`.

```bash
Workflow compiled
2025-11-03T19:05:41Z [SIMULATION] Simulator Initialized

2025-11-03T19:05:41Z [SIMULATION] Running trigger trigger=cron-trigger@1.0.0
2025-11-03T19:05:41Z [USER LOG] Hello, Calculator! Workflow triggered.
2025-11-03T19:05:41Z [USER LOG] Successfully fetched and aggregated math result: 60

Workflow Simulation Result:
 {
  "result": 60
}

2025-11-03T19:05:41Z [SIMULATION] Execution finished signal received
2025-11-03T19:05:41Z [SIMULATION] Skipping WorkflowEngineV2
```

- **`[USER LOG]`**: You can now see both of your `runtime.log()` calls in the output. The second log shows the fetched value (e.g., `60`), confirming that the API call and consensus worked correctly.
- **`[SIMULATION]`**: These are system-level messages from the simulator showing its internal state.
- **`Workflow Simulation Result`**: This is the final return value of your workflow. The `result` field now contains the aggregated value from the API as a number.

Your workflow can now fetch and process data from an external source.

## Next Steps

Next, you'll learn how to interact with a smart contract to read data from the blockchain and combine it with this offchain result.

- **[Part 3: Reading an Onchain Value](/cre/getting-started/part-3-reading-onchain-value)**

---

# Part 3: Reading an Onchain Value
Source: https://docs.chain.link/cre/getting-started/part-3-reading-onchain-value-ts
Last Updated: 2025-11-04

In the previous part, you successfully fetched data from an offchain API. Now, you will complete the "Onchain Calculator" by reading a value from a smart contract and combining it with your offchain result.

This part of the guide introduces EVM interactions using the TypeScript SDK's [`EVMClient`](/cre/reference/sdk/evm-client) and <a href="https://viem.sh/" target="_blank" rel="noopener noreferrer">Viem</a> for type-safe contract interactions.

## What you'll do

- Configure your project with a Sepolia RPC URL.
- Use the EVM client with Viem to read a value from a deployed smart contract.
- Integrate the onchain value into your main workflow logic.

## Step 1: The smart contract

For this guide, we will interact with a simple `Storage` contract that has already been deployed to the Sepolia testnet. All it does is store a single `uint256` value.

Here is the Solidity source code for the contract:

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

contract Storage {
  uint256 public value;

  constructor(uint256 initialValue) {
    value = initialValue;
  }

  function get() public view returns (uint256) {
    return value;
  }
}
```

A version of this contract has been deployed to Sepolia at `0xa17CF997C28FF154eDBae1422e6a50BeF23927F4` with an `initialValue` of `22`.

## Step 2: Configure your environment

To interact with a contract on Sepolia, your workflow needs EVM chain details.

1. **Contract address and chain name**: Add the deployed contract's address and chain name to your `config.staging.json` file. We use an `evms` array to hold the configuration, which makes it easy to add more contracts (or chains) later.

   ```json
   {
     "schedule": "*/30 * * * * *",
     "apiUrl": "https://api.mathjs.org/v4/?expr=randomInt(1,101)",
     "evms": [
       {
         "storageAddress": "0xa17CF997C28FF154eDBae1422e6a50BeF23927F4",
         "chainName": "ethereum-testnet-sepolia"
       }
     ]
   }
   ```


   <Aside type="note" title="About Chain Identifiers">
     A **chain selector** is a unique identifier used by Chainlink to specify a blockchain.

     Chain selectors can be referenced in multiple formats:

     - **String name** (used in config files and `project.yaml`): `"ethereum-testnet-sepolia"`
     - **Numeric ID** (used internally by the SDK, returned by `getNetwork()`): `16015286601757825753n`

     You only use the string name in your configuration files. The SDK's `getNetwork()` helper converts the string name to the numeric BigInt ID automatically.

     For a complete reference showing all supported chains, see the [Chain Selectors reference](/cre/reference/sdk/evm-client#chain-selectors).
   </Aside>

2. **RPC URL**: For your workflow to interact with the blockchain, it needs an RPC endpoint. The `cre init` command has already configured a public Sepolia RPC URL in your `project.yaml` file for convenience. Let's take a look at what was generated:

   Open your `project.yaml` file at the root of your project. Your `staging-settings` target should look like this:

   ```yaml
   # in onchain-calculator/project.yaml
   staging-settings:
     rpcs:
       - chain-name: ethereum-testnet-sepolia
         url: https://ethereum-sepolia-rpc.publicnode.com
   ```

   This public RPC endpoint is sufficient for testing and following this guide. However, for production use or higher reliability, you should consider using a dedicated RPC provider like [Infura](https://infura.io/), [Alchemy](https://www.alchemy.com/), or [QuickNode](https://www.quicknode.com/).

   <Aside type="caution" title="Protect Your RPC URL">
     When you use a dedicated RPC provider, your RPC URLs will contain API keys. To avoid exposing them, you should add
     your `project.yaml` file to your project's `.gitignore` file.
   </Aside>

## Step 3: Create the contract ABI file

To interact with the `Storage` contract in a type-safe and maintainable way, you'll create an ABI file that defines the contract's interface.

The TypeScript SDK uses <a href="https://viem.sh/" target="_blank" rel="noopener noreferrer">Viem</a> for EVM interactions, which provides excellent TypeScript type inference when you define ABIs as TypeScript modules.

1. **Create the ABI directory**: From your project root (`onchain-calculator/`), create the `contracts/abi` directory:

   ```bash
   mkdir -p contracts/abi
   ```

2. **Add the Storage contract ABI**: Create a new file called `Storage.ts` in the `contracts/abi` directory:

   ```bash
   touch contracts/abi/Storage.ts
   ```

   Open `contracts/abi/Storage.ts` and paste the following ABI definition:

   ```typescript
   export const Storage = [
     {
       inputs: [],
       name: "get",
       outputs: [{ internalType: "uint256", name: "", type: "uint256" }],
       stateMutability: "view",
       type: "function",
     },
   ] as const
   ```

   The `as const` assertion is important—it tells TypeScript to infer the most specific type possible, which enables Viem's type-safe contract interactions.

3. **Create an index file**: To make imports cleaner, create an `index.ts` file that exports all your ABIs:

   ```bash
   touch contracts/abi/index.ts
   ```

   Open `contracts/abi/index.ts` and add:

   ```typescript
   export { Storage } from "./Storage"
   ```

   This allows you to import ABIs using: `import { Storage } from "../contracts/abi"`.

## Step 4: Update your workflow logic

Now that you have the ABI defined, you can import and use it in your workflow. Replace the entire content of `onchain-calculator/my-calculator-workflow/main.ts` with the version below.

**Note:** Lines highlighted in green indicate new or modified code compared to Part 2.

Code snippet for onchain-calculator/my-calculator-workflow/main.ts:

```typescript
import {
  cre,
  consensusMedianAggregation,
  Runner,
  type NodeRuntime,
  type Runtime,
  getNetwork,
  LAST_FINALIZED_BLOCK_NUMBER,
  encodeCallMsg,
  bytesToHex,
} from "@chainlink/cre-sdk"
import { encodeFunctionData, decodeFunctionResult, zeroAddress } from "viem"
import { Storage } from "../contracts/abi"


// EvmConfig defines the configuration for a single EVM chain.
type EvmConfig = {
  storageAddress: string
  chainName: string
}


type Config = {
  schedule: string
  apiUrl: string
  evms: EvmConfig[]
}

type MyResult = {
  finalResult: bigint
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

// fetchMathResult is the function passed to the runInNodeMode helper.
const fetchMathResult = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()

  const req = {
    url: nodeRuntime.config.apiUrl,
    method: "GET" as const,
  }

  const resp = httpClient.sendRequest(nodeRuntime, req).result()
  const bodyText = new TextDecoder().decode(resp.body)
  const val = BigInt(bodyText.trim())

  return val
}

const onCronTrigger = (runtime: Runtime<Config>): MyResult => {
  // Step 1: Fetch offchain data (from Part 2)
  const offchainValue = runtime.runInNodeMode(fetchMathResult, consensusMedianAggregation())().result()

  runtime.log(`Successfully fetched offchain value: ${offchainValue}`)


  // Get the first EVM configuration from the list.
  const evmConfig = runtime.config.evms[0]

  // Step 2: Read onchain data using the EVM client
  // Convert the human-readable chain name to a chain selector
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: evmConfig.chainName,
    isTestnet: true,
  })
  if (!network) {
    throw new Error(`Unknown chain name: ${evmConfig.chainName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // Encode the function call using the Storage ABI
  const callData = encodeFunctionData({
    abi: Storage,
    functionName: "get",
  })

  // Call the contract
  const contractCall = evmClient
    .callContract(runtime, {
      call: encodeCallMsg({
        from: zeroAddress,
        to: evmConfig.storageAddress as `0x${string}`,
        data: callData,
      }),
      blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
    })
    .result()

  // Decode the result
  const onchainValue = decodeFunctionResult({
    abi: Storage,
    functionName: "get",
    data: bytesToHex(contractCall.data),
  }) as bigint

  runtime.log(`Successfully read onchain value: ${onchainValue}`)

  // Step 3: Combine the results
  const finalResult = onchainValue + offchainValue
  runtime.log(`Final calculated result: ${finalResult}`)


  return {
    finalResult,
  }
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```


<Aside type="tip" title="Optional: Add runtime validation">
  If you added Zod validation in Part 1, you can continue using it here. The `Config` type now includes the `evms` array, so update your Zod schema accordingly and pass `configSchema` to `Runner.newRunner<Config>({ configSchema })`. See [Part 1: Optional Runtime Validation](/cre/getting-started/part-1-project-setup#optional-runtime-validation) for details.
</Aside>

**Key TypeScript SDK features:**

- **`getNetwork()`**: Converts a human-readable chain name to a numeric chain selector
- **`EVMClient`**: The EVM capability client for interacting with blockchains
- **`encodeFunctionData()`**: From Viem, encodes a function call with type-safe parameters
- **`callContract()`**: EVMClient method for calling view/pure functions on a contract
- **`bytesToHex()`**: Converts `Uint8Array` response data to hex string for Viem
- **`decodeFunctionResult()`**: From Viem, decodes the contract call response with type inference
- **`LAST_FINALIZED_BLOCK_NUMBER`**: Constant for reading from the most recent finalized block
- **Native `bigint`**: TypeScript's built-in big integer type (no external library needed)

## Step 5: Run the simulation and review the output

Run the simulation from your project root directory (the `onchain-calculator/` folder). Because there is only one trigger defined, the simulator runs it automatically.

```bash
cre workflow simulate my-calculator-workflow --target staging-settings
```

The simulation logs will show the end-to-end execution of your workflow.

```bash
Workflow compiled
2025-11-03T19:06:56Z [SIMULATION] Simulator Initialized

2025-11-03T19:06:56Z [SIMULATION] Running trigger trigger=cron-trigger@1.0.0
2025-11-03T19:06:56Z [USER LOG] Successfully fetched offchain value: 55
2025-11-03T19:06:56Z [USER LOG] Successfully read onchain value: 22
2025-11-03T19:06:56Z [USER LOG] Final calculated result: 77

Workflow Simulation Result:
 {
  "finalResult": 77
}

2025-11-03T19:06:56Z [SIMULATION] Execution finished signal received
2025-11-03T19:06:56Z [SIMULATION] Skipping WorkflowEngineV2
```

- **`[USER LOG]`**: You can now see all three of your `runtime.log()` calls, showing the offchain value (`55`), the onchain value (`22`), and the final combined result (`77`).
- **`[SIMULATION]`**: These are system-level messages from the simulator showing its internal state.
- **`Workflow Simulation Result`**: This is the final, JSON-formatted return value of your workflow. The `finalResult` field contains the sum of the offchain and onchain values (55 + 22 = 77).

You have successfully built a complete CRE workflow that combines offchain and onchain data.

## Next Steps

You have successfully read a value from a smart contract and combined it with offchain data. The final step is to write this new result back to the blockchain.

- **[Part 4: Writing Onchain](/cre/getting-started/part-4-writing-onchain)**: Learn how to execute an onchain write transaction from your workflow to complete the project.

---

# Part 4: Writing Onchain
Source: https://docs.chain.link/cre/getting-started/part-4-writing-onchain-ts
Last Updated: 2025-12-09

In the previous parts, you successfully fetched offchain data and read from a smart contract. Now, you'll complete the "Onchain Calculator" by writing your computed result back to the blockchain.

## What you'll do

- Use the `CalculatorConsumer` contract to receive workflow results
- Modify your workflow to write data to the blockchain using the EVM capability
- Execute your first onchain write transaction through CRE
- Verify your result on the blockchain

## Step 1: The consumer contract

To write data onchain, your workflow needs a target smart contract (a "consumer contract"). For this guide, we have pre-deployed a simple `CalculatorConsumer` contract on the Sepolia testnet. This contract is designed to receive and store the calculation results from your workflow.

Here is the source code for the contract so you can see how it works:

<Aside type="note" title="Note">
  Don't worry if you don't understand every line of this contract right now. We're showing it to you for context, but
  the key takeaway is that it's designed to securely receive data from a CRE workflow. We'll cover the important details
  of how this works in a later guide.
</Aside>

```sol
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.19;

import {ReceiverTemplate} from "./ReceiverTemplate.sol";

/**
 * @title CalculatorConsumer (Testing Version)
 * @notice This contract receives reports from a CRE workflow and stores the results of a calculation onchain.
 * @dev Inherits from ReceiverTemplate which provides security checks. The forwarder address must be
 * configured at deployment. Additional security checks (workflowId, workflowName, author) can be enabled via setter functions.
 */
contract CalculatorConsumer is ReceiverTemplate {
  // Struct to hold the data sent in a report from the workflow
  struct CalculatorResult {
    uint256 offchainValue;
    int256 onchainValue;
    uint256 finalResult;
  }

  // --- State Variables ---
  CalculatorResult public latestResult;
  uint256 public resultCount;
  mapping(uint256 => CalculatorResult) public results;

  // --- Events ---
  event ResultUpdated(uint256 indexed resultId, uint256 finalResult);

  /**
   * @notice Constructor requires the forwarder address for security
   * @param _forwarderAddress The address of the Chainlink Forwarder contract (for testing: MockForwarder)
   * @dev The forwarder address enables the first layer of security - only the forwarder can call onReport.
   * Additional security checks can be configured after deployment using setter functions.
   */
  constructor(
    address _forwarderAddress
  ) ReceiverTemplate(_forwarderAddress) {}

  /**
   * @notice Implements the core business logic for processing reports.
   * @dev This is called automatically by ReceiverTemplate's onReport function after security checks.
   */
  function _processReport(
    bytes calldata report
  ) internal override {
    // Decode the report bytes into our CalculatorResult struct
    CalculatorResult memory calculatorResult = abi.decode(report, (CalculatorResult));

    // --- Core Logic ---
    // Update contract state with the new result
    resultCount++;
    results[resultCount] = calculatorResult;
    latestResult = calculatorResult;

    emit ResultUpdated(resultCount, calculatorResult.finalResult);
  }

  // This function is a "dry-run" utility. It allows an offchain system to check
  // if a prospective result is an outlier before submitting it for a real onchain update.
  // It is also used to guide the binding generator to create a method that accepts the CalculatorResult struct.
  function isResultAnomalous(
    CalculatorResult memory _prospectiveResult
  ) public view returns (bool) {
    // A result is not considered anomalous if it's the first one.
    if (resultCount == 0) {
      return false;
    }

    // Business logic: Define an anomaly as a new result that is more than double the previous result.
    // This is just one example of a validation rule you could implement.
    return _prospectiveResult.finalResult > (latestResult.finalResult * 2);
  }
}
```

The contract is already deployed for you on Sepolia at the following address: <a href="https://sepolia.etherscan.io/address/0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb#code" target="_blank" rel="noopener noreferrer">`0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb`</a>. You will use this address in your configuration file.

## Step 2: Update your workflow configuration

Add the `CalculatorConsumer` contract address to your `config.staging.json`:

```json
{
  "schedule": "*/30 * * * * *",
  "apiUrl": "https://api.mathjs.org/v4/?expr=randomInt(1,101)",
  "evms": [
    {
      "storageAddress": "0xa17CF997C28FF154eDBae1422e6a50BeF23927F4",
      "calculatorConsumerAddress": "0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb",
      "chainName": "ethereum-testnet-sepolia",
      "gasLimit": "500000"
    }
  ]
}
```

## Step 3: Update your workflow logic

Now modify your workflow to write the final result to the contract. Writing onchain involves a two-step process:

1. **Generate a signed report**: Use `runtime.report()` to create a cryptographically signed report from your workflow data
2. **Submit the report**: Use `evmClient.writeReport()` to submit the signed report to the consumer contract

The TypeScript SDK uses Viem's `encodeAbiParameters` to properly encode the struct data according to the contract's ABI before generating the report.

<Aside type="note" title="Configuring Gas Limit">
  Notice that in the code below, we pass the `gasLimit` from our config file to the `writeReport` function. Explicitly
  setting a sufficient gas limit is crucial for write operations to prevent them from failing due to "out of gas"
  errors.
</Aside>

Replace the entire content of `onchain-calculator/my-calculator-workflow/main.ts` with this final version.

**Note:** Lines highlighted in green indicate new or modified code compared to Part 3.

Code snippet for onchain-calculator/my-calculator-workflow/main.ts:

```typescript
import {
  cre,
  consensusMedianAggregation,
  Runner,
  type NodeRuntime,
  type Runtime,
  getNetwork,
  LAST_FINALIZED_BLOCK_NUMBER,
  encodeCallMsg,
  bytesToHex,
  hexToBase64,
} from "@chainlink/cre-sdk"
import { encodeAbiParameters, parseAbiParameters, encodeFunctionData, decodeFunctionResult, zeroAddress } from "viem"
import { Storage } from "../contracts/abi"

type EvmConfig = {
  chainName: string
  storageAddress: string
  calculatorConsumerAddress: string
  gasLimit: string
}

type Config = {
  schedule: string
  apiUrl: string
  evms: EvmConfig[]
}


// MyResult struct now holds all the outputs of our workflow.
type MyResult = {
  offchainValue: bigint
  onchainValue: bigint
  finalResult: bigint
  txHash: string
}


const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

const onCronTrigger = (runtime: Runtime<Config>): MyResult => {
  const evmConfig = runtime.config.evms[0]

  // Convert the human-readable chain name to a chain selector
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: evmConfig.chainName,
    isTestnet: true,
  })
  if (!network) {
    throw new Error(`Unknown chain name: ${evmConfig.chainName}`)
  }

  // Step 1: Fetch offchain data
  const offchainValue = runtime.runInNodeMode(fetchMathResult, consensusMedianAggregation())().result()

  runtime.log(`Successfully fetched offchain value: ${offchainValue}`)

  // Step 2: Read onchain data using the EVM client
  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const callData = encodeFunctionData({
    abi: Storage,
    functionName: "get",
  })

  const contractCall = evmClient
    .callContract(runtime, {
      call: encodeCallMsg({
        from: zeroAddress,
        to: evmConfig.storageAddress as `0x${string}`,
        data: callData,
      }),
      blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
    })
    .result()

  const onchainValue = decodeFunctionResult({
    abi: Storage,
    functionName: "get",
    data: bytesToHex(contractCall.data),
  }) as bigint

  runtime.log(`Successfully read onchain value: ${onchainValue}`)


  // Step 3: Calculate the final result
  const finalResultValue = onchainValue + offchainValue

  runtime.log(`Final calculated result: ${finalResultValue}`)

  // Step 4: Write the result to the consumer contract
  const txHash = updateCalculatorResult(
    runtime,
    network.chainSelector.selector,
    evmConfig,
    offchainValue,
    onchainValue,
    finalResultValue
  )

  // Step 5: Log and return the final, consolidated result.
  const finalWorkflowResult: MyResult = {
    offchainValue,
    onchainValue,
    finalResult: finalResultValue,
    txHash,
  }

  runtime.log(
    `Workflow finished successfully! offchainValue: ${offchainValue}, onchainValue: ${onchainValue}, finalResult: ${finalResultValue}, txHash: ${txHash}`
  )

  return finalWorkflowResult
}


const fetchMathResult = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()

  const req = {
    url: nodeRuntime.config.apiUrl,
    method: "GET" as const,
  }

  const resp = httpClient.sendRequest(nodeRuntime, req).result()
  const bodyText = new TextDecoder().decode(resp.body)
  const val = BigInt(bodyText.trim())

  return val
}


// updateCalculatorResult handles the logic for writing data to the CalculatorConsumer contract.
function updateCalculatorResult(
  runtime: Runtime<Config>,
  chainSelector: bigint,
  evmConfig: EvmConfig,
  offchainValue: bigint,
  onchainValue: bigint,
  finalResult: bigint
): string {
  runtime.log(`Updating calculator result for consumer: ${evmConfig.calculatorConsumerAddress}`)

  const evmClient = new cre.capabilities.EVMClient(chainSelector)

  // Encode the CalculatorResult struct according to the contract's ABI
  const reportData = encodeAbiParameters(
    parseAbiParameters("uint256 offchainValue, int256 onchainValue, uint256 finalResult"),
    [offchainValue, onchainValue, finalResult]
  )

  runtime.log(
    `Writing report to consumer contract - offchainValue: ${offchainValue}, onchainValue: ${onchainValue}, finalResult: ${finalResult}`
  )

  // Step 1: Generate a signed report using the consensus capability
  const reportResponse = runtime
    .report({
      encodedPayload: hexToBase64(reportData),
      encoderName: "evm",
      signingAlgo: "ecdsa",
      hashingAlgo: "keccak256",
    })
    .result()

  // Step 2: Submit the report to the consumer contract
  const writeReportResult = evmClient
    .writeReport(runtime, {
      receiver: evmConfig.calculatorConsumerAddress,
      report: reportResponse,
      gasConfig: {
        gasLimit: evmConfig.gasLimit,
      },
    })
    .result()

  runtime.log("Waiting for write report response")

  const txHash = bytesToHex(writeReportResult.txHash || new Uint8Array(32))
  runtime.log(`Write report transaction succeeded: ${txHash}`)
  runtime.log(`View transaction at https://sepolia.etherscan.io/tx/${txHash}`)
  return txHash
}


export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```


<Aside type="note" title="Optional: Add runtime validation">
  If you added Zod validation in Part 1, you can continue using it here. The `Config` type now includes `gasLimit` and `calculatorConsumerAddress` fields, so update your Zod schema accordingly. See [Part 1: Optional Runtime Validation](/cre/getting-started/part-1-project-setup#optional-runtime-validation) for details.
</Aside>

**Key TypeScript SDK features for writing:**

- **`encodeAbiParameters()`**: From Viem, encodes structured data according to a contract's ABI
- **`parseAbiParameters()`**: From Viem, defines the parameter types for encoding
- **`runtime.report()`**: Generates a signed report using the consensus capability
- **`writeReport()`**: EVMClient method for submitting the signed report to a consumer contract
- **`txHash`**: The transaction hash returned after a successful write operation


<Aside type="tip" title="Convenience helpers for cleaner code">
  Once you're comfortable with these core concepts, the SDK provides convenience helpers for more concise code:

  - **HTTP helpers**: The [`HTTPSendRequester`](/cre/reference/sdk/http-client-ts#using-sendrequester) type includes [`ok()`](/cre/reference/sdk/http-client-ts#ok), [`text()`](/cre/reference/sdk/http-client-ts#text), and [`json()`](/cre/reference/sdk/http-client-ts#json) functions for cleaner response handling
  - **Report helper**: [`prepareReportRequest()`](/cre/reference/sdk/evm-client-ts#preparereportrequest) automatically sets default encoding parameters (`ecdsa`, `keccak256`, `evm`) for report generation

  These helpers are great for production code, but we use the explicit approach here for educational clarity.
</Aside>

<Aside type="note" title="Organizing ABIs for reusability">
  Notice that we encoded the `CalculatorResult` struct inline using `parseAbiParameters()`. This works great for one-off
  workflows. However, if you're building workflows that interact with the same consumer contract multiple times or want
  better code organization, you can define the ABI parameters in a dedicated file (like we did with the `Storage`
  contract in [Part 3](/cre/getting-started/part-3-reading-onchain-value-ts#step-3-create-the-contract-abi-file)). To
  learn more about organizing ABIs for write operations, see [Organizing ABIs for reusable data
  structures](/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain#organizing-abis-for-reusable-data-structures).
</Aside>

## Step 4: Run the simulation and review the output


<Aside type="note" title="Funding Your Account">
  This step submits an onchain transaction, which requires gas. Before running the simulation, verify that the account
  associated with the private key from [Part
  1](/cre/getting-started/part-1-project-setup-ts#set-up-your-private-key) is funded with sufficient Sepolia ETH.
  An unfunded account will cause the transaction to fail, often with an error message like `gas required exceeds
    allowance`.

  If you need more Sepolia ETH, go to <a href="https://faucets.chain.link" target="blank">faucets.chain.link</a> to get some Sepolia ETH.
</Aside>

<Aside type="caution" title="Broadcasting Your Transaction">
  By default, `cre workflow simulate` performs a **dry run** for onchain write operations. It will simulate the transaction and return a successful response, but will **not** broadcast it to the network, resulting in an empty transaction hash (`0x`).

  To execute a real transaction, you must add the `--broadcast` flag to the command.
</Aside>

Run the simulation from your project root directory (the `onchain-calculator/` folder). Because there is only one trigger, the simulator runs it automatically.

```bash
cre workflow simulate my-calculator-workflow --target staging-settings --broadcast
```

Your workflow will now show the complete end-to-end execution, including the final log of the `MyResult` object containing the transaction hash.

```bash
Workflow compiled
2026-01-09T17:52:05Z [SIMULATION] Simulator Initialized

2026-01-09T17:52:05Z [SIMULATION] Running trigger trigger=cron-trigger@1.0.0
2026-01-09T17:52:06Z [USER LOG] Successfully fetched offchain value: 68
2026-01-09T17:52:06Z [USER LOG] Successfully read onchain value: 22
2026-01-09T17:52:06Z [USER LOG] Final calculated result: 90
2026-01-09T17:52:06Z [USER LOG] Updating calculator result for consumer: 0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb
2026-01-09T17:52:06Z [USER LOG] Writing report to consumer contract - offchainValue: 68, onchainValue: 22, finalResult: 90
2026-01-09T17:52:12Z [USER LOG] Waiting for write report response
2026-01-09T17:52:12Z [USER LOG] Write report transaction succeeded: 0x6346d9eeca2f2875131d38aa9903a216f16e3cc7188f0ac6a1d5cd1fcbfbf9e6
2026-01-09T17:52:12Z [USER LOG] View transaction at https://sepolia.etherscan.io/tx/0x6346d9eeca2f2875131d38aa9903a216f16e3cc7188f0ac6a1d5cd1fcbfbf9e6
2026-01-09T17:52:12Z [USER LOG] Workflow finished successfully! offchainValue: 68, onchainValue: 22, finalResult: 90, txHash: 0x6346d9eeca2f2875131d38aa9903a216f16e3cc7188f0ac6a1d5cd1fcbfbf9e6

Workflow Simulation Result:
 {
  "finalResult": 90,
  "offchainValue": 68,
  "onchainValue": 22,
  "txHash": "0x6346d9eeca2f2875131d38aa9903a216f16e3cc7188f0ac6a1d5cd1fcbfbf9e6"
}

2026-01-09T17:52:12Z [SIMULATION] Execution finished signal received
2026-01-09T17:52:12Z [SIMULATION] Skipping WorkflowEngineV2
```

- **`[USER LOG]`**: You can see all of your `logger.info()` calls showing the complete workflow execution, including the offchain value (`68`), onchain value (`22`), final calculation (`90`), and the transaction hash.
- **`[SIMULATION]`**: These are system-level messages from the simulator showing its internal state.
- **`Workflow Simulation Result`**: This is the final return value of your workflow. The `MyResult` object contains all the values (68 + 22 = 90) and the transaction hash confirming the write operation succeeded.

## Step 5: Verify the result onchain

### **1. Check the Transaction**

In your terminal output, you'll see a clickable URL to view the transaction on Sepolia Etherscan:

```
[USER LOG] View transaction at https://sepolia.etherscan.io/tx/0x...
```

Click the URL (or copy and paste it into your browser) to see the full details of the transaction your workflow submitted.

**What are you seeing on a blockchain explorer?**

You'll notice the transaction's `to` address is not the `CalculatorConsumer` contract you intended to call. Instead, it's to a **Forwarder** contract. Your workflow sends a secure report to the Forwarder, which then verifies the request and makes the final call to the `CalculatorConsumer` on your workflow's behalf. To learn more, see the [Onchain Write guide](/cre/guides/workflow/using-evm-client/onchain-write/overview).

### **2. Check the contract state**

While your wallet interacted with the Forwarder, the `CalculatorConsumer` contract's state was still updated. You can verify this change directly on Etherscan:

- Navigate to the `CalculatorConsumer` contract address: <a href="https://sepolia.etherscan.io/address/0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb#readContract" target="_blank" rel="noopener noreferrer">`0x95e10BaC2B89aB4D8508ccEC3f08494FcB3D23cb`</a>.
- Expand the `latestResult` function and click **Query**. The values should match the `finalResult`, `offchainValue`, and `onchainValue` from your workflow logs.

This completes the end-to-end loop: triggering a workflow, fetching data, reading onchain state, and verifiably writing the result back to a public blockchain.

To learn more about implementing consumer contracts and the secure write process, see these guides:

- **[Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts)**: Learn how to create your own secure consumer contracts with proper validation.
- **[Onchain Write Guide](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts)**: Dive deeper into the write patterns.

## Next steps

You've now mastered the complete CRE development workflow!

- **[Conclusion & Next Steps](/cre/getting-started/conclusion)**: Review what you've learned and find resources for advanced topics.

---

# Using Secrets in Simulation
Source: https://docs.chain.link/cre/guides/workflow/secrets/using-secrets-simulation-ts
Last Updated: 2025-11-04

This guide explains how to use secrets during **local development and simulation**. When you're simulating a workflow on your local machine with `cre workflow simulate`, secrets are provided via environment variables or a `.env` file.


<Aside type="note" title="For deployed workflows">
  If you're deploying workflows, you'll need to store secrets in the **Vault DON** instead. See [Using Secrets with Deployed Workflows](/cre/guides/workflow/secrets/using-secrets-deployed) for details.
</Aside>

At a high level, the process follows a simple, three-step pattern:

1. **Declare**: You declare the logical names of your secrets in a `secrets.yaml` file.
2. **Provide**: You provide the actual secret values in a `.env` file or as environment variables.
3. **Use**: You access the secrets in your workflow code using the SDK's secret management API.

This separation of concerns ensures that your workflow code is portable and your secrets are never hard-coded.

<Aside type="note" title="Best Practices for Storing Secrets">
  While this guide shows secrets being provided via a plaintext `.env` file and environment variables, the recommended
  best practice for security is to use a dedicated secrets manager. See our guide on [Managing Secrets with 1Password
  CLI](/cre/guides/workflow/secrets/managing-secrets-1password) to learn how to inject secrets securely at runtime.
</Aside>

## Step-by-step guide

### Step 1: Declare your secrets (`secrets.yaml`)

The first step is to create a `secrets.yaml` file in the root of your project. This file acts as a manifest, defining the "logical names" or "IDs" for the secrets your workflow will use.

In this file, you map a logical name (which you'll use in your workflow code) to one environment variable name that will hold the actual secret value.

**Example `secrets.yaml`:**

```yaml
# in project-root/secrets.yaml
secretsNames:
  # This is the logical ID you will use in your workflow code
  SECRET_ADDRESS:
    # This is the environment variable the CLI will look for
    - SECRET_ADDRESS_ALL
```

### Step 2: Provide the secret values

Next, you need to provide the actual values for the secrets. The `cre` CLI can read these values in two primary ways.

#### Method 1: Using shell environment variables (Recommended)

You can provide secrets as standard environment variables directly in your shell.

For example, in your terminal:

```bash
export SECRET_ADDRESS_ALL="0x1234567890abcdef1234567890abcdef12345678"
```

When you run the `cre workflow simulate` command in the same terminal session, the CLI will have access to this variable.

#### Method 2: Using a `.env` file

Create a `.env` file in your project's root directory. The `cre` CLI automatically finds this file and loads the variables defined within it into the environment for your simulation. The variable names here must match those you declared in `secrets.yaml`.

**Example `.env` file:**

```bash
# in project-root/.env

# The variable name matches the one in secrets.yaml
SECRET_ADDRESS_ALL="0x1234567890abcdef1234567890abcdef12345678"
```

<Aside type="caution" title="Never Commit Your Secrets">
  The project's `.gitignore` file is already configured to ignore `.env` files. **Never** commit this file to version
  control.
</Aside>

### Step 3: Use the secret in your workflow

Now you can access the secret in your workflow code. The SDK provides a method to retrieve secrets using the logical ID you defined in `secrets.yaml`.

The following code shows a complete, runnable workflow that triggers on a schedule, fetches a secret, and logs its value.

**Example workflow:**

Code snippet for Fetching Single Secret (TypeScript):

```typescript
import { cre, Runner, type Runtime } from "@chainlink/cre-sdk"

// Config can be an empty object if you don't need any parameters from config.json
type Config = Record<string, never>

// Define the logical name of the secret as a constant for clarity
const SECRET_NAME = "SECRET_ADDRESS"

// onCronTrigger is the callback function that gets executed when the cron trigger fires
// This is where you use the secret
const onCronTrigger = (runtime: Runtime<Config>): string => {
  // Call runtime.getSecret with the secret's logical ID
  const secret = runtime.getSecret({ id: SECRET_NAME }).result()

  // Use the secret's value
  const secretAddress = secret.value
  runtime.log(`Successfully fetched a secret! Address: ${secretAddress}`)

  // ... now you can use the secretAddress in your logic ...
  return "Success"
}

// initWorkflow is the entry point for the workflow
const initWorkflow = () => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: "0 */10 * * * *" }), onCronTrigger)]
}

// main is the entry point for the WASM binary
export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

### Step 4: Configure secrets path in `workflow.yaml`

Before simulating, you need to tell the CLI where to find your secrets file. This is configured in your `workflow.yaml` file under `workflow-artifacts.secrets-path`.

Open your `workflow.yaml` file and set the `secrets-path`:

```yaml
local-simulation:
  user-workflow:
    workflow-name: "my-workflow"
    workflow-artifacts:
      workflow-path: "./main.ts"
      config-path: "./config.json"
      secrets-path: "../secrets.yaml" # Path to your secrets file
```

Notice the path `../secrets.yaml`. Because the workflow artifacts are relative to the workflow directory, you need to point to the `secrets.yaml` file located one level up in the project root.

### Step 5: Run the simulation

Now you can simulate your workflow:

```bash
cre workflow simulate my-workflow --target staging-settings
```

The CLI will automatically read the `secrets-path` from your `workflow.yaml` and load the secrets from your `.env` file or environment variables you provided in your terminal session.

## Fetching multiple secrets

You can fetch multiple secrets by calling the secret retrieval method multiple times within your workflow.


<Aside type="caution" title="Fetch secrets sequentially">
  The WASM host for the CRE runtime does not support parallel `runtime.getSecret()` requests. Always fetch secrets **sequentially**: call `getSecret()`, get the result with `.result()`, then call `getSecret()` again for the next secret. Do not attempt to fetch multiple secrets in parallel.
</Aside>

The following example builds on the previous one. First, update your `secrets.yaml` to declare two secrets:

```yaml
secretsNames:
  SECRET_ADDRESS:
    - SECRET_ADDRESS_ALL
  API_KEY:
    - API_KEY_ALL
```

Then provide the values in your `.env` file or export them as environment variables in your terminal session:

```bash
export SECRET_ADDRESS_ALL="0x1234567890abcdef1234567890abcdef12345678"
export API_KEY_ALL="your-api-key-here"
```

Now you can fetch both secrets in your workflow code:

Code snippet for Fetching Multiple Secrets (TypeScript):

```typescript
import { cre, Runner, type Runtime } from "@chainlink/cre-sdk"

// Config can be an empty object if you don't need any parameters from config.json
type Config = Record<string, never>

const SECRET_ADDRESS_NAME = "SECRET_ADDRESS"
const API_KEY_NAME = "API_KEY"

const onCronTrigger = (runtime: Runtime<Config>): string => {
  // 1. Request the first secret
  const secretAddress = runtime.getSecret({ id: SECRET_ADDRESS_NAME }).result()

  // 2. Request the second secret
  const apiKey = runtime.getSecret({ id: API_KEY_NAME }).result()

  // 3. Use your secrets
  runtime.log(`Successfully fetched secrets! Address: ${secretAddress.value}, API Key: ${apiKey.value}`)

  return "Success"
}

// initWorkflow is the entry point for the workflow
const initWorkflow = () => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: "0 */10 * * * *" }), onCronTrigger)]
}

// main is the entry point for the WASM binary
export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

---

# Onchain Read
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/onchain-read-ts
Last Updated: 2025-12-10

This guide explains how to read data from a smart contract from within your CRE workflow. The TypeScript SDK uses [viem](https://viem.sh/) for ABI handling and the SDK's [`EVMClient`](/cre/reference/sdk/evm-client-ts) to create a type-safe developer experience.

## The read pattern

Reading from a contract follows this pattern:

1. **Define your contract ABI**: Create a TypeScript file with your contract's ABI using <a href="https://viem.sh/docs/abi/parseAbi" target="_blank">viem's `parseAbi`</a> (inline) or store it in `contracts/abi/` for complex workflows
2. **Get network information**: Use the SDK's `getNetwork()` helper to look up chain selector and other network details
3. **Instantiate the EVM Client**: Create an `EVMClient` instance with the chain selector
4. **Encode the function call**: Use <a href="https://viem.sh/docs/contract/encodeFunctionData#encodefunctiondata" target="_blank">viem's `encodeFunctionData()`</a> to ABI-encode your function call
5. **Encode the call message**: Use `encodeCallMsg()` to create a properly formatted call message with `from`, `to`, and `data`
6. **Call the contract**: Use `callContract(runtime, {...})` to execute the read operation
7. **Decode the result**: Use <a href="https://viem.sh/docs/contract/decodeFunctionResult#decodefunctionresult" target="_blank">viem's `decodeFunctionResult()`</a> to decode the returned data
8. **Await the result**: Call `.result()` on the returned object to get the consensus-verified result

## Step-by-step example

Let's read a value from a simple `Storage` contract with a `get() view returns (uint256)` function.

### 1. Define the contract ABI

For simple contracts, you can define the ABI inline using viem's `parseAbi`:

```typescript
import { parseAbi } from "viem"

const storageAbi = parseAbi(["function get() view returns (uint256)"])
```

For complex workflows with multiple contracts, it's recommended to create separate ABI files in a `contracts/abi/` directory. See [Part 3 of the Getting Started guide](/cre/getting-started/part-3-reading-onchain-value-ts#step-3-create-the-contract-abi-file) for an example of this pattern.

### 2. The workflow logic

Here's a complete example of reading from a Storage contract:

```typescript
import {
  cre,
  getNetwork,
  encodeCallMsg,
  bytesToHex,
  LAST_FINALIZED_BLOCK_NUMBER,
  type Runtime,
  Runner,
} from "@chainlink/cre-sdk"
import { type Address, encodeFunctionData, decodeFunctionResult, parseAbi, zeroAddress } from "viem"
import { z } from "zod"

// Define config schema with Zod
const configSchema = z.object({
  contractAddress: z.string(),
  chainSelectorName: z.string(),
})

type Config = z.infer<typeof configSchema>

// Define the Storage contract ABI
const storageAbi = parseAbi(["function get() view returns (uint256)"])

const onCronTrigger = (runtime: Runtime<Config>): string => {
  // Get network information
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: runtime.config.chainName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${runtime.config.chainSelectorName}`)
  }

  // Create EVM client with chain selector
  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // Encode the function call
  const callData = encodeFunctionData({
    abi: storageAbi,
    functionName: "get",
    args: [], // No arguments for this function
  })

  // Call the contract
  const contractCall = evmClient
    .callContract(runtime, {
      call: encodeCallMsg({
        from: zeroAddress,
        to: runtime.config.contractAddress as Address,
        data: callData,
      }),
      blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
    })
    .result()

  // Decode the result (convert Uint8Array to hex string for viem)
  const storedValue = decodeFunctionResult({
    abi: storageAbi,
    functionName: "get",
    data: bytesToHex(contractCall.data),
  })

  runtime.log(`Successfully read storage value: ${storedValue.toString()}`)
  return storedValue.toString()
}

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: "*/10 * * * * *", // Every 10 seconds
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Understanding the components

<Aside type="note" title="Type-safe addresses">
  Notice the `as Address` type assertion when passing `contractAddress` to `encodeCallMsg`. This tells TypeScript the
  string is a valid Ethereum address, which is required by viem's `encodeCallMsg` function. The `Address` type is
  imported from `viem`.
</Aside>

### Network lookup with `getNetwork()`

The SDK provides a `getNetwork()` helper that looks up network information by name:

```typescript
const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})

// Returns network info including:
// - chainSelector.selector (numeric ID)
// - name
// - chainType
```

See the [EVM Client SDK Reference](/cre/reference/sdk/evm-client-ts#chain-selectors) for all available networks.

### Block number options

When calling `callContract()`, you can specify which block to read from:

- **`LAST_FINALIZED_BLOCK_NUMBER`**: Read from the last finalized block (recommended for production)
- **`LATEST_BLOCK_NUMBER`**: Read from the latest block
- **Custom block number**: Use a `BigIntJson` object for custom finality depths or historical queries

```typescript
import { LAST_FINALIZED_BLOCK_NUMBER, LATEST_BLOCK_NUMBER } from "@chainlink/cre-sdk"

// Read from finalized block (most common)
const contractCall = evmClient.callContract(runtime, {
  call: encodeCallMsg({...}),
  blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
}).result()

// Or read from latest block
const contractCall = evmClient.callContract(runtime, {
  call: encodeCallMsg({...}),
  blockNumber: LATEST_BLOCK_NUMBER,
}).result()
```

#### Custom block depths

For use cases requiring fixed confirmation thresholds (e.g., regulatory compliance) or historical state verification, you can specify an exact block number.

**Example 1 - Read from a specific historical block**:

```typescript
import { blockNumber } from '@chainlink/cre-sdk'

const historicalBlock = 9767655n
const contractCall = evmClient.callContract(runtime, {
  call: encodeCallMsg({...}),
  blockNumber: blockNumber(historicalBlock),
}).result()
```

**Example 2 - Read from 500 blocks ago for custom finality**:

```typescript
import { protoBigIntToBigint, blockNumber } from '@chainlink/cre-sdk'

// Get the latest block number
const latestHeader = evmClient.headerByNumber(runtime, {}).result()
if (!latestHeader.header?.blockNumber) {
  throw new Error("Failed to get latest block number")
}

// Convert protobuf BigInt to native bigint and calculate custom block
const latestBlockNum = protoBigIntToBigint(latestHeader.header.blockNumber)
const customBlock = latestBlockNum - 500n

// Call the contract at the custom block height
const contractCall = evmClient.callContract(runtime, {
  call: encodeCallMsg({...}),
  blockNumber: blockNumber(customBlock),
}).result()
```

**Helper functions:**

The SDK provides two helper functions for working with block numbers:

- **`protoBigIntToBigint(pb)`** — Converts a protobuf `BigInt` (returned by SDK methods like `headerByNumber`) to a native JavaScript `bigint`. Use this when you need to perform arithmetic on block numbers.

- **`blockNumber(n)`** — Converts a native `bigint`, `number`, or `string` to the protobuf `BigInt` JSON format required by SDK methods. This is an alias for `bigintToProtoBigInt`.

See [Finality and Confidence Levels](/cre/concepts/finality-ts) for more details on when to use custom block depths.

### Encoding call messages with `encodeCallMsg()`

The `encodeCallMsg()` helper converts your hex-formatted call data into the base64 format required by the EVM capability:

```typescript
import { encodeCallMsg } from "@chainlink/cre-sdk"
import { zeroAddress } from "viem"

const callMsg = encodeCallMsg({
  from: zeroAddress, // Caller address (typically zeroAddress for view functions)
  to: "0xYourContractAddress", // Contract address
  data: callData, // ABI-encoded function call from encodeFunctionData()
})
```

This helper is required because the underlying EVM capability expects addresses and data in base64 format, not hex.

### ABI encoding/decoding with viem

The TypeScript SDK relies on viem for all ABI operations:

- **`encodeFunctionData()`**: Encodes a function call into bytes
- **`decodeFunctionResult()`**: Decodes the returned bytes into TypeScript types
- **`parseAbi()`**: Parses human-readable ABI strings into typed ABI objects

### The `.result()` pattern

All CRE capability calls return objects with a `.result()` method. Calling `.result()` blocks execution synchronously (within the WASM environment) and waits for the consensus-verified result.

```typescript
// This returns an object with a .result() method
const callObject = evmClient.callContract(runtime, {
  call: encodeCallMsg({...}),
  blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
})

// This blocks and returns the actual result
const contractCall = callObject.result()
```

This pattern is consistent across all SDK capabilities (EVM, HTTP, etc.).

## Solidity-to-TypeScript type mappings

Viem automatically handles type conversions:

| Solidity Type            | TypeScript Type |
| ------------------------ | --------------- |
| `uint8`, `uint256`, etc. | `bigint`        |
| `int8`, `int256`, etc.   | `bigint`        |
| `address`                | `string`        |
| `bool`                   | `boolean`       |
| `string`                 | `string`        |
| `bytes`, `bytes32`, etc. | `Uint8Array`    |

## Complete example with configuration

Here's a full runnable workflow with external configuration:

### Main workflow file (`main.ts`)

```typescript
import {
  cre,
  getNetwork,
  encodeCallMsg,
  bytesToHex,
  LAST_FINALIZED_BLOCK_NUMBER,
  type Runtime,
  Runner,
} from "@chainlink/cre-sdk"
import { type Address, encodeFunctionData, decodeFunctionResult, parseAbi, zeroAddress } from "viem"
import { z } from "zod"

const configSchema = z.object({
  contractAddress: z.string(),
  chainSelectorName: z.string(),
})

type Config = z.infer<typeof configSchema>

const storageAbi = parseAbi(["function get() view returns (uint256)"])

const onCronTrigger = (runtime: Runtime<Config>): string => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: runtime.config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${runtime.config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const callData = encodeFunctionData({
    abi: storageAbi,
    functionName: "get",
    args: [],
  })

  const contractCall = evmClient
    .callContract(runtime, {
      call: encodeCallMsg({
        from: zeroAddress,
        to: runtime.config.contractAddress as Address,
        data: callData,
      }),
      blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
    })
    .result()

  const storedValue = decodeFunctionResult({
    abi: storageAbi,
    functionName: "get",
    data: bytesToHex(contractCall.data),
  })

  runtime.log(`Storage value: ${storedValue.toString()}`)
  return storedValue.toString()
}

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: "*/10 * * * * *",
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

### Configuration file (`config.json`)

```json
{
  "contractAddress": "0xa17CF997C28FF154eDBae1422e6a50BeF23927F4",
  "chainSelectorName": "ethereum-testnet-sepolia"
}
```

<Aside type="note" title="Chain Names">
  The `chainSelectorName` should match one of the supported networks in the SDK. Use the chain selector name format like
  `"ethereum-testnet-sepolia"`. See the [EVM Client SDK Reference](/cre/reference/sdk/evm-client-ts#chain-selectors) for
  all available networks.
</Aside>

## Working with complex ABIs

For workflows with multiple contracts or complex ABIs, organize them in separate files:

### Contract ABI file (`contracts/abi/Storage.ts`)

```typescript
import { parseAbi } from "viem"

export const Storage = parseAbi(["function get() view returns (uint256)", "function set(uint256 value) external"])
```

### Export file (`contracts/abi/index.ts`)

```typescript
export { Storage } from "./Storage"
```

### Import in workflow

```typescript
import { Storage } from "../contracts/abi"

const callData = encodeFunctionData({
  abi: Storage,
  functionName: "get",
  args: [],
})
```

This pattern provides better organization, reusability, and type safety across your workflow.

## Next steps

- Learn how to [write data to contracts](/cre/guides/workflow/using-evm-client/onchain-write/overview)
- Explore the [EVM Client SDK Reference](/cre/reference/sdk/evm-client-ts) for all available methods
- See [Part 3](/cre/getting-started/part-3-reading-onchain-value) and [Part 4](/cre/getting-started/part-4-writing-onchain) of the Getting Started guide for more examples

---

# Onchain Write
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/onchain-write/overview-ts
Last Updated: 2025-11-04

This overview explains how writing data onchain works in CRE and how the TypeScript SDK handles it.

- [Understanding how CRE writes work](#understanding-how-cre-writes-work) - The secure write flow
- [What you need: A consumer contract](#what-you-need-a-consumer-contract) - Contract requirements
- [The TypeScript write process](#the-typescript-write-process) - Two-step approach overview
- [Next steps](#next-steps) - Where to go from here

## Understanding how CRE writes work

Before diving into code, it's important to understand how CRE handles onchain writes differently than traditional web3 applications.

### Why CRE doesn't write directly to your contract

In a traditional web3 app, you'd create a transaction and send it directly to your smart contract. **CRE uses a different, more secure approach** for three key reasons:

1. **Decentralization**: Multiple nodes in the Decentralized Oracle Network (DON) need to agree on what data to write
2. **Verification**: The blockchain needs cryptographic proof that the data came from a trusted Chainlink network
3. **Accountability**: There must be a verifiable trail showing which workflow and owner created the data

### The secure write flow (4 steps)

Here's the journey your workflow's data takes to reach the blockchain:

1. **Report generation**: Your workflow generates a ***report***—your data is ABI-encoded and wrapped in a cryptographically signed "package"
2. **DON consensus**: The DON reaches consensus on the report's contents
3. **Forwarder submission**: A designated node submits the report to a Chainlink `KeystoneForwarder` contract
4. **Delivery to your contract**: The Forwarder validates the report's signatures and calls your consumer contract's `onReport()` function with the data

In your workflow code, this process involves two steps: calling `runtime.report()` to generate the signed report, then calling `evmClient.writeReport()` to submit it to the blockchain.

## What you need: A consumer contract

Before you can write data onchain, you need a **consumer contract**. This is the smart contract that will receive your workflow's data.

**What is a consumer contract?**

A consumer contract is **your smart contract** that implements the `IReceiver` interface. This interface defines an `onReport()` function that the Chainlink Forwarder calls to deliver your workflow's data.

Think of it as a mailbox that's designed to receive packages (reports) from Chainlink's secure delivery service (the Forwarder contract).

**Key requirement:**

Your contract must implement the `IReceiver` interface. This single requirement ensures your contract has the necessary `onReport(bytes metadata, bytes report)` function that the Chainlink Forwarder calls to deliver data.

**Getting started:**

- **Don't have a consumer contract yet?** Follow the [Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts) guide to create one.
- **Already have one deployed?** Great! Make sure you have its address and ABI ready for encoding your data.

## The TypeScript write process

The TypeScript SDK uses a simple, two-step process for writing data onchain:

### Step 1: Generate a signed report

Use `runtime.report()` to:

1. ABI-encode your data using <a href="https://viem.sh/docs/abi/encodeAbiParameters" target="_blank">viem's `encodeAbiParameters()`</a>
2. Convert the encoded data to base64 format
3. Generate a cryptographically signed report

### Step 2: Submit the report

Use `evmClient.writeReport()` to submit the signed report to your consumer contract address.

**Key features:**

- **Use viem** directly for ABI operations
- **Manual but flexible** - Full control over encoding and submission
- **Type-safe** - TypeScript and viem ensure compile-time safety
- **Works for any data** - Single values, structs, arrays, etc.

<Aside type="note" title="Already familiar with the Getting Started tutorial?">
  The approach covered in [Part 4: Writing Onchain](/cre/getting-started/part-4-writing-onchain) uses this same two-step
  pattern. This section provides the conceptual foundation for that tutorial.
</Aside>

## Next steps

Now that you understand the concepts, follow these guides to implement onchain writes:

1. **[Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts)** - Create a Solidity contract to receive your workflow's data
2. **[Writing Data Onchain](/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain)** - Complete step-by-step guide with examples for single values and structs

**Additional resources:**

- **[EVM Client Reference](/cre/reference/sdk/evm-client-ts)** - Complete API documentation
- **[Onchain Read](/cre/guides/workflow/using-evm-client/onchain-read-ts)** - Reading data from smart contracts

---

# EVM Chain Interactions
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/overview-ts
Last Updated: 2025-11-04

The `EVMClient` is the TypeScript SDK's interface for interacting with EVM-compatible blockchains. It provides a simple, powerful, and type-safe way to read data from and write data to your onchain contracts through the underlying **[EVM Read & Write Capabilities](/cre/capabilities/evm-read-write)**.

## How it works

The TypeScript SDK uses **manual ABI definitions** with <a href="https://viem.sh/" target="_blank">viem</a> for contract interactions. This approach provides:

- Type-safe ABI encoding/decoding with viem
- No code generation required—just define ABIs as TypeScript constants
- Full TypeScript type inference for contract calls
- Synchronous-looking async operations with `.result()`
- Built-in helpers like `getNetwork()`, `bytesToHex()`, and chain selectors

This approach gives you the flexibility of direct ABI handling with the type safety of TypeScript and viem's powerful utilities.

## Guides

- **[Onchain Read](/cre/guides/workflow/using-evm-client/onchain-read-ts)**: Learn how to call `view` and `pure` functions on your smart contracts to read onchain state
- **[Onchain Write](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts)**: Learn how to call state-changing functions on your smart contracts to write data to the blockchain
  - **[Building Consumer Contracts](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts)**: Learn how to write your own consumer contracts that can receive reports from your workflow

---

# Supported Networks
Source: https://docs.chain.link/cre/guides/workflow/using-evm-client/supported-networks-ts
Last Updated: 2025-11-20

This page lists the EVM-compatible networks supported by CRE workflows, along with their chain names (for configuration) and forwarder contract addresses (for consumer contract validation).

## How to Use This Page

This reference provides three key pieces of information for each network:

1. **Network Name**: The human-readable network identifier (click to view the forwarder contract on the block explorer)
2. **Chain Name**: The value to use in your [`project.yaml`](/cre/reference/project-configuration-ts#31-global-configuration-projectyaml) configuration and [EVM Client code](/cre/reference/sdk/evm-client-ts#chain-selectors)
3. **Forwarder Address**: The contract address for optional consumer contract validation (click to copy)

## Understanding Forwarder Addresses

Forwarder addresses are relevant **only if you want to add security validation to your consumer contracts**. Your workflow code does not interact with forwarders directly—the EVM capability handles report delivery automatically. Learn more: [Onchain Write Overview](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts).

**Optional security layer**: You can configure your consumer contract's `onReport()` function to accept calls only from the trusted forwarder address. See [Configuring Permissions](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts#34-configuring-permissions) for implementation details.

### Simulation vs Production Addresses

**Important**: Forwarder contracts differ between local simulation and production:

| Environment      | Contract Type           | Section                                         |
| ---------------- | ----------------------- | ----------------------------------------------- |
| Local simulation | `MockKeystoneForwarder` | [Simulation Forwarders](#simulation-forwarders) |
| Production       | `KeystoneForwarder`     | [Production Forwarders](#production-forwarders) |

If you configure forwarder validation in your consumer contract, **remember to update the forwarder address** when deploying to production. Learn more: [Working with Simulation](/cre/guides/workflow/using-evm-client/onchain-write/building-consumer-contracts#4-working-with-simulation).

## Simulation Forwarders

These `MockKeystoneForwarder` addresses are used when running `cre workflow simulate` with the `--broadcast` flag. Use these addresses **only** during local development and testing.

### Simulation Mainnets

| Network                                                                                                                                               | Chain Name                    | Mock Forwarder Address                     |
| ----------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ------------------------------------------ |
| <a href="https://arbiscan.io/address/0xd770499057619c9a76205fd4168161cf94abc532" target="_blank" rel="noopener noreferrer">Arbitrum One</a>           | ethereum-mainnet-arbitrum-1   | 0xd770499057619c9a76205fd4168161cf94abc532 |
| <a href="https://snowscan.xyz/address/0xdc21e279934ff6721cadfdd112dafb3261f09a2c" target="_blank" rel="noopener noreferrer">Avalanche</a>             | avalanche-mainnet             | 0xdc21e279934ff6721cadfdd112dafb3261f09a2c |
| <a href="https://basescan.org/address/0x5e342a8438b4f5d39e72875fcee6f76b39cce548" target="_blank" rel="noopener noreferrer">Base</a>                  | ethereum-mainnet-base-1       | 0x5e342a8438b4f5d39e72875fcee6f76b39cce548 |
| <a href="https://bscscan.com/address/0x6f3239bbb26e98961e1115aba83f8a282e5508c8" target="_blank" rel="noopener noreferrer">BNB Smart Chain</a>        | binance_smart_chain-mainnet | 0x6f3239bbb26e98961e1115aba83f8a282e5508c8 |
| <a href="https://etherscan.io/address/0xa3d1ad4ac559a6575a114998affb2fb2ec97a7d9" target="_blank" rel="noopener noreferrer">Ethereum Mainnet</a>      | ethereum-mainnet              | 0xa3d1ad4ac559a6575a114998affb2fb2ec97a7d9 |
| <a href="https://optimistic.etherscan.io/address/0x9119a1501550ed94a3f2794038ed9258337afa18" target="_blank" rel="noopener noreferrer">OP Mainnet</a> | ethereum-mainnet-optimism-1   | 0x9119a1501550ed94a3f2794038ed9258337afa18 |
| <a href="https://polygonscan.com/address/0xf458d621885e29a5003ea9bbba5280d54e19b1ce" target="_blank" rel="noopener noreferrer">Polygon</a>            | polygon-mainnet               | 0xf458d621885e29a5003ea9bbba5280d54e19b1ce |

### Simulation Testnets

| Network                                                                                                                                                     | Chain Name                          | Mock Forwarder Address                     |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------------------------------ |
| <a href="https://sepolia.arbiscan.io/address/0xd41263567ddfead91504199b8c6c87371e83ca5d" target="_blank" rel="noopener noreferrer">Arbitrum Sepolia</a>     | ethereum-testnet-sepolia-arbitrum-1 | 0xd41263567ddfead91504199b8c6c87371e83ca5d |
| <a href="https://testnet.snowscan.xyz/address/0x2e7371a5d032489e4f60216d8d898a4c10805963" target="_blank" rel="noopener noreferrer">Avalanche Fuji</a>      | avalanche-testnet-fuji              | 0x2e7371a5d032489e4f60216d8d898a4c10805963 |
| <a href="https://sepolia.basescan.org/address/0x82300bd7c3958625581cc2f77bc6464dcecdf3e5" target="_blank" rel="noopener noreferrer">Base Sepolia</a>        | ethereum-testnet-sepolia-base-1     | 0x82300bd7c3958625581cc2f77bc6464dcecdf3e5 |
| <a href="https://testnet.bscscan.com/address/0xa238e42cb8782808dbb2f37e19859244ec4779b0" target="_blank" rel="noopener noreferrer">BSC Testnet</a>          | binance_smart_chain-testnet       | 0xa238e42cb8782808dbb2f37e19859244ec4779b0 |
| <a href="https://sepolia.etherscan.io/address/0x15fC6ae953E024d975e77382eEeC56A9101f9F88" target="_blank" rel="noopener noreferrer">Ethereum Sepolia</a>    | ethereum-testnet-sepolia            | 0x15fC6ae953E024d975e77382eEeC56A9101f9F88 |
| <a href="https://sepolia-optimism.etherscan.io/address/0xa2888380dff3704a8ab6d1cd1a8f69c15fea5ee3" target="_blank" rel="noopener noreferrer">OP Sepolia</a> | ethereum-testnet-sepolia-optimism-1 | 0xa2888380dff3704a8ab6d1cd1a8f69c15fea5ee3 |
| <a href="https://amoy.polygonscan.com/address/0x3675a5eb2286a3f87e8278fc66edf458a2e3bb74" target="_blank" rel="noopener noreferrer">Polygon Amoy</a>        | polygon-testnet-amoy                | 0x3675a5eb2286a3f87e8278fc66edf458a2e3bb74 |

## Production Forwarders

These `KeystoneForwarder` addresses are used by deployed workflows. Use these addresses when configuring your consumer contracts for production.

### Mainnets

| Network                                                                                                                                               | Chain Name                    | Forwarder Address                          |
| ----------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ------------------------------------------ |
| <a href="https://arbiscan.io/address/0xF8344CFd5c43616a4366C34E3EEE75af79a74482" target="_blank" rel="noopener noreferrer">Arbitrum One</a>           | ethereum-mainnet-arbitrum-1   | 0xF8344CFd5c43616a4366C34E3EEE75af79a74482 |
| <a href="https://snowscan.xyz/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">Avalanche</a>             | avalanche-mainnet             | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://basescan.org/address/0xF8344CFd5c43616a4366C34E3EEE75af79a74482" target="_blank" rel="noopener noreferrer">Base</a>                  | ethereum-mainnet-base-1       | 0xF8344CFd5c43616a4366C34E3EEE75af79a74482 |
| <a href="https://bscscan.com/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">BNB Smart Chain</a>        | binance_smart_chain-mainnet | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://etherscan.io/address/0x0b93082D9b3C7C97fAcd250082899BAcf3af3885" target="_blank" rel="noopener noreferrer">Ethereum Mainnet</a>      | ethereum-mainnet              | 0x0b93082D9b3C7C97fAcd250082899BAcf3af3885 |
| <a href="https://optimistic.etherscan.io/address/0xF8344CFd5c43616a4366C34E3EEE75af79a74482" target="_blank" rel="noopener noreferrer">OP Mainnet</a> | ethereum-mainnet-optimism-1   | 0xF8344CFd5c43616a4366C34E3EEE75af79a74482 |
| <a href="https://polygonscan.com/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">Polygon</a>            | polygon-mainnet               | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |

### Testnets

| Network                                                                                                                                                     | Chain Name                          | Forwarder Address                          |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------- | ------------------------------------------ |
| <a href="https://sepolia.arbiscan.io/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">Arbitrum Sepolia</a>     | ethereum-testnet-sepolia-arbitrum-1 | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://testnet.snowscan.xyz/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">Avalanche Fuji</a>      | avalanche-testnet-fuji              | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://sepolia.basescan.org/address/0xF8344CFd5c43616a4366C34E3EEE75af79a74482" target="_blank" rel="noopener noreferrer">Base Sepolia</a>        | ethereum-testnet-sepolia-base-1     | 0xF8344CFd5c43616a4366C34E3EEE75af79a74482 |
| <a href="https://testnet.bscscan.com/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">BSC Testnet</a>          | binance_smart_chain-testnet       | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://sepolia.etherscan.io/address/0xF8344CFd5c43616a4366C34E3EEE75af79a74482" target="_blank" rel="noopener noreferrer">Ethereum Sepolia</a>    | ethereum-testnet-sepolia            | 0xF8344CFd5c43616a4366C34E3EEE75af79a74482 |
| <a href="https://sepolia-optimism.etherscan.io/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">OP Sepolia</a> | ethereum-testnet-sepolia-optimism-1 | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |
| <a href="https://amoy.polygonscan.com/address/0x76c9cf548b4179F8901cda1f8623568b58215E62" target="_blank" rel="noopener noreferrer">Polygon Amoy</a>        | polygon-testnet-amoy                | 0x76c9cf548b4179F8901cda1f8623568b58215E62 |

---

# Making GET Requests
Source: https://docs.chain.link/cre/guides/workflow/using-http-client/get-request-ts
Last Updated: 2025-12-16

The `HTTPClient` is the SDK's interface for the underlying [HTTP Capability](/cre/capabilities/http). It allows your workflow to fetch data from any external API.

All HTTP requests are wrapped in a consensus mechanism to provide a single, reliable result. The SDK provides two ways to do this:

- **`sendRequest`:** (Recommended) A high-level helper method that simplifies making requests.
- **`runInNodeMode`:** The lower-level pattern for more complex scenarios.

## Prerequisites

This guide assumes you have a basic understanding of CRE. If you are new, we strongly recommend completing the [Getting Started tutorial](/cre/getting-started/overview) first.


<Aside type="caution" title="Redirects are not supported">
  HTTP requests to URLs that return redirects (3xx status codes) will fail. Ensure the URL you provide is the final destination and does not redirect to another URL.
</Aside>

## Choosing your approach

### Use `sendRequest` (Section 1) when:

- Making a **single HTTP GET request**
- Your logic is straightforward: make request → parse response → return result
- You want **simple, clean code** with minimal boilerplate

This is the recommended approach for most use cases.

### Use `runInNodeMode` (Section 2) when:

- You need **multiple HTTP requests** with logic between them
- You need **conditional execution** (if/else based on runtime conditions)
- You need **custom retry logic** or complex error handling
- You need **complex data transformation** (fetching from multiple APIs and combining results)

If you're unsure, start with Section 1. You can always migrate to Section 2 later if your requirements become more complex.

## 1. The `sendRequest` Pattern (Recommended)

The high-level `sendRequest()` method is the simplest and recommended way to make HTTP calls. It automatically handles the `runInNodeMode` pattern for you.

### How it works

The pattern involves two key components:

1. **A Fetching Function**: You create a function (e.g., `fetchAndParse`) that receives a `sendRequester` object and additional arguments (like `config`). This function contains your core logic—making the request, parsing the response, and returning a clean data object.
2. **Your Main Handler**: Your main trigger callback calls `httpClient.sendRequest()`, which returns a function that you then call with your additional arguments. For a full list of supported consensus methods, see the [Consensus & Aggregation reference](/cre/reference/sdk/consensus-ts).

This separation keeps your code clean and focused.

### Step-by-step example

This example shows a complete workflow that fetches the price of an asset, parses it into a typed object, and aggregates the results using field-based consensus.

#### Step 1: Configure your workflow

Add the API URL to your `config.json` file.

```json
{
  "schedule": "0 */5 * * * *",
  "apiUrl": "https://some-price-api.com/price?ids=ethereum"
}
```

#### Step 2: Define the response types

Define TypeScript types for the API response and your internal data model.

```typescript
import { cre, type Runtime, type HTTPSendRequester, Runner } from "@chainlink/cre-sdk"
import { z } from "zod"

// Config schema
const configSchema = z.object({
  schedule: z.string(),
  apiUrl: z.string(),
})

type Config = z.infer<typeof configSchema>

// PriceData is the clean, internal type that our workflow will use
type PriceData = {
  price: number
  lastUpdated: Date
}

// ExternalApiResponse is used to parse the nested JSON from the external API
type ExternalApiResponse = {
  ethereum: {
    usd: number
    last_updated_at: number
  }
}
```

#### Step 3: Implement the fetch and parse logic

Create the function that will be passed to `sendRequest()`. This function receives the `sendRequester` and `config` as parameters.

```typescript
const fetchAndParse = (sendRequester: HTTPSendRequester, config: Config): PriceData => {
  // 1. Construct the request
  const req = {
    url: config.apiUrl,
    method: "GET" as const,
  }

  // 2. Send the request using the provided sendRequester
  const resp = sendRequester.sendRequest(req).result()

  if (resp.statusCode !== 200) {
    throw new Error(`API returned status ${resp.statusCode}`)
  }

  // 3. Parse the raw JSON into our ExternalApiResponse type
  const bodyText = new TextDecoder().decode(resp.body)
  const externalResp = JSON.parse(bodyText) as ExternalApiResponse

  // 4. Transform into our internal PriceData type and return
  return {
    price: externalResp.ethereum.usd,
    lastUpdated: new Date(externalResp.ethereum.last_updated_at * 1000),
  }
}
```

#### Step 4: Call `sendRequest()` and aggregate results

In your `onCronTrigger` handler, call `httpClient.sendRequest()`. This returns a function that you call with `runtime.config`.

```typescript
const onCronTrigger = (runtime: Runtime<Config>): string => {
  const httpClient = new cre.capabilities.HTTPClient()

  // sendRequest returns a function that we call with runtime.config
  const result = httpClient
    .sendRequest(
      runtime,
      fetchAndParse,
      new cre.consensus.ConsensusAggregationByFields<PriceData>({
        price: cre.consensus.median<number>(),
        lastUpdated: cre.consensus.median<Date>(),
      })
    )(runtime.config) // Call the returned function with config
    .result()

  runtime.log(`Successfully fetched and aggregated price data: $${result.price} at ${result.lastUpdated.toISOString()}`)

  return `Price: ${result.price}`
}
```

#### Complete example

Here's the full workflow code:

```typescript
import { cre, type Runtime, type HTTPSendRequester, Runner } from "@chainlink/cre-sdk"
import { z } from "zod"

// Config schema
const configSchema = z.object({
  schedule: z.string(),
  apiUrl: z.string(),
})

type Config = z.infer<typeof configSchema>

// Types
type PriceData = {
  price: number
  lastUpdated: Date
}

type ExternalApiResponse = {
  ethereum: {
    usd: number
    last_updated_at: number
  }
}

// Fetch function receives sendRequester and config as parameters
const fetchAndParse = (sendRequester: HTTPSendRequester, config: Config): PriceData => {
  const req = {
    url: config.apiUrl,
    method: "GET" as const,
  }

  const resp = sendRequester.sendRequest(req).result()

  if (resp.statusCode !== 200) {
    throw new Error(`API returned status ${resp.statusCode}`)
  }

  const bodyText = new TextDecoder().decode(resp.body)
  const externalResp = JSON.parse(bodyText) as ExternalApiResponse

  return {
    price: externalResp.ethereum.usd,
    lastUpdated: new Date(externalResp.ethereum.last_updated_at * 1000),
  }
}

// Main workflow handler
const onCronTrigger = (runtime: Runtime<Config>): string => {
  const httpClient = new cre.capabilities.HTTPClient()

  const result = httpClient
    .sendRequest(
      runtime,
      fetchAndParse,
      new cre.consensus.ConsensusAggregationByFields<PriceData>({
        price: cre.consensus.median<number>(),
        lastUpdated: cre.consensus.median<Date>(),
      })
    )(runtime.config) // Call with config
    .result()

  runtime.log(`Successfully fetched price: $${result.price} at ${result.lastUpdated.toISOString()}`)

  return `Price: ${result.price}`
}

// Initialize workflow
const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## 2. The `runInNodeMode` Pattern (Low-Level)

For more complex scenarios, you can use the lower-level `runtime.runInNodeMode()` method directly. This gives you more control but requires more boilerplate code.

The pattern works like a "map-reduce" for the DON:

1. **Map**: You provide a function (e.g., `fetchPriceData`) that executes on every node.
2. **Reduce**: You provide a consensus aggregation to reduce the individual results into a single outcome. For a full list of supported consensus methods, see the [Consensus & Aggregation reference](/cre/reference/sdk/consensus-ts).

The example below is functionally identical to the `sendRequest` example above, but implemented using the low-level pattern.

```typescript
import { cre, type Runtime, type NodeRuntime, Runner } from "@chainlink/cre-sdk"
import { z } from "zod"

// Config and types (same as before)
const configSchema = z.object({
  schedule: z.string(),
  apiUrl: z.string(),
})

type Config = z.infer<typeof configSchema>

type PriceData = {
  price: number
  lastUpdated: Date
}

type ExternalApiResponse = {
  ethereum: {
    usd: number
    last_updated_at: number
  }
}

// fetchPriceData is a function that runs on each individual node
const fetchPriceData = (nodeRuntime: NodeRuntime<Config>): PriceData => {
  // 1. Create HTTP client and fetch raw data
  const httpClient = new cre.capabilities.HTTPClient()

  const req = {
    url: nodeRuntime.config.apiUrl,
    method: "GET" as const,
  }

  const resp = httpClient.sendRequest(nodeRuntime, req).result()

  if (resp.statusCode !== 200) {
    throw new Error(`API returned status ${resp.statusCode}`)
  }

  // 2. Parse and transform the response
  const bodyText = new TextDecoder().decode(resp.body)
  const externalResp = JSON.parse(bodyText) as ExternalApiResponse

  return {
    price: externalResp.ethereum.usd,
    lastUpdated: new Date(externalResp.ethereum.last_updated_at * 1000),
  }
}

// Main workflow handler
const onCronTrigger = (runtime: Runtime<Config>): string => {
  const result = runtime
    .runInNodeMode(
      fetchPriceData,
      new cre.consensus.ConsensusAggregationByFields<PriceData>({
        price: cre.consensus.median<number>(),
        lastUpdated: cre.consensus.median<Date>(),
      })
    )()
    .result()

  runtime.log(`Successfully fetched price: $${result.price} at ${result.lastUpdated.toISOString()}`)

  return `Price: ${result.price}`
}

// Initialize workflow (same as before)
const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Response helper functions

The SDK provides utility functions (`ok()`, `text()`, `json()`, `getHeader()`) to simplify working with HTTP responses. For full documentation and examples, see the [HTTP Client SDK Reference](/cre/reference/sdk/http-client-ts#helper-functions).

## Customizing your requests

The request object provides several fields to customize your HTTP call. See the [HTTP Client SDK Reference](/cre/reference/sdk/http-client-ts) for a full list of options, including:

- **Headers**: Custom HTTP headers
- **Body**: Request payload (for POST, PUT, etc.)
- **Timeout**: Request timeout in milliseconds
- **Cache settings**: Control response caching behavior

---

# Making POST Requests
Source: https://docs.chain.link/cre/guides/workflow/using-http-client/post-request-ts
Last Updated: 2025-12-16

This guide explains how to use the HTTP Client to send data to an external API using a `POST` request. Because POST requests typically create resources or trigger actions, this guide shows you how to ensure your request executes only once, even though multiple nodes in the DON run your workflow.


<Aside type="note" title="Single-Execution Pattern">
  By default, **all nodes in the DON execute HTTP requests**. For POST, PUT, PATCH, and DELETE operations, this would cause duplicate actions (like creating multiple resources or sending multiple emails).

  This guide shows you the **recommended pattern** using `cacheSettings` to ensure only one node makes the actual HTTP call. This is the standard approach for non-idempotent operations.
</Aside>

All HTTP requests are wrapped in a consensus mechanism. The SDK provides two ways to do this:

- **[High-level `sendRequest`](#1-the-high-level-sendrequest-pattern-recommended):** A high-level helper method that simplifies making requests. This is the recommended approach for most use cases.
- **[Low-level `runInNodeMode`](#2-the-low-level-runinnodemode-pattern):** The lower-level pattern for more complex scenarios.

## Choosing your approach

### Use High-Level `sendRequest` (Section 1) when:

- Making a **single HTTP POST request**
- Your logic is straightforward: make request → parse response → return result
- You want **simple, clean code** with minimal boilerplate

This is the recommended approach for most use cases.

### Use Low-Level `runInNodeMode` (Section 2) when:

- You need **to access secrets** (e.g., API keys, authentication tokens)
- You need **multiple HTTP requests** with logic between them
- You need **conditional execution** (if/else based on runtime conditions)
- You're **combining HTTP with other node-level operations**
- You need **custom retry logic** or complex error handling

If you're unsure, start with Section 1. You can always migrate to Section 2 later if your requirements become more complex.

For this example, we will use <a href="https://webhook.site/" target="_blank">**webhook.site**</a>, a free service that provides a unique URL to which you can send requests and see the results in real-time.

## Prerequisites

This guide assumes you have a basic understanding of CRE. If you are new, we strongly recommend completing the [Getting Started tutorial](/cre/getting-started/overview) first.


<Aside type="caution" title="Redirects are not supported">
  HTTP requests to URLs that return redirects (3xx status codes) will fail. Ensure the URL you provide is the final destination and does not redirect to another URL.
</Aside>

## 1. The High-Level `sendRequest` Pattern (recommended)

The high-level `sendRequest()` method is the simplest and recommended way to make `POST` requests. It automatically handles the `runInNodeMode` pattern for you.

### Step 1: Generate your unique webhook URL

1. Go to <a href="https://webhook.site/" target="_blank">**webhook.site**</a>.
2. Copy the unique URL provided for use in your configuration.

### Step 2: Configure your workflow

In your `config.json` file, add the webhook URL:

```json
{
  "webhookUrl": "https://webhook.site/<your-unique-id>",
  "schedule": "*/30 * * * * *"
}
```

### Step 3: Implement the POST request logic

#### 1. Understanding single-execution with `cacheSettings`

Before writing code, it's important to understand how to prevent duplicate POST requests. When your workflow runs, **all nodes in the DON execute your code**. For POST requests that create resources or trigger actions, this would cause duplicates.

**The solution**: Use `cacheSettings` in your HTTP request. This enables a shared cache across nodes:

1. The first node makes the HTTP request and stores the response in the cache
2. Other nodes check the cache first and reuse the cached response
3. Result: Only one actual HTTP call is made, while all nodes participate in consensus


<Aside type="note" title="When to use cacheSettings">
  Use `cacheSettings` for **all POST, PUT, PATCH, and DELETE requests** unless your API is explicitly designed to be idempotent (safe to call multiple times). This is the standard pattern.
</Aside>

**Key configuration:**

- `readFromCache: true` — Enables reading cached responses
- `maxAgeMs` — How long to accept cached responses (in milliseconds)

Now let's implement this pattern.

#### 2. Define your data types

In your `main.ts`, define the TypeScript types for your configuration and the data structures.

```typescript
import {
  cre,
  ok,
  consensusIdenticalAggregation,
  type Runtime,
  type HTTPSendRequester,
  Runner,
} from "@chainlink/cre-sdk"
import { z } from "zod"

// Config schema
const configSchema = z.object({
  webhookUrl: z.string(),
  schedule: z.string(),
})

type Config = z.infer<typeof configSchema>

// Data to be sent
type MyData = {
  message: string
  value: number
}

// Response for consensus
type PostResponse = {
  statusCode: number
}
```

#### 3. Create the data posting function

Create the function that will be passed to `sendRequest()`. It prepares the data, serializes it to JSON, and uses the `sendRequester` to send the `POST` request **with `cacheSettings`** to ensure single execution.

```typescript
const postData = (sendRequester: HTTPSendRequester, config: Config): PostResponse => {
  // 1. Prepare the data to be sent
  const dataToSend: MyData = {
    message: "Hello there!",
    value: 77,
  }

  // 2. Serialize the data to JSON and encode as bytes
  const bodyBytes = new TextEncoder().encode(JSON.stringify(dataToSend))

  // 3. Convert to base64 for the request
  const body = Buffer.from(bodyBytes).toString("base64")

  // 4. Construct the POST request with cacheSettings
  const req = {
    url: config.webhookUrl,
    method: "POST" as const,
    body,
    headers: {
      "Content-Type": "application/json",
    },
    cacheSettings: {
      readFromCache: true, // Enable reading from cache
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  }

  // 5. Send the request and wait for the response
  const resp = sendRequester.sendRequest(req).result()

  if (!ok(resp)) {
    throw new Error(`HTTP request failed with status: ${resp.statusCode}`)
  }

  return { statusCode: resp.statusCode }
}
```

#### 4. Call `sendRequest()` from your handler

In your main `onCronTrigger` handler, call `httpClient.sendRequest()`, which returns a function that you call with `runtime.config`.

```typescript
const onCronTrigger = (runtime: Runtime<Config>): string => {
  const httpClient = new cre.capabilities.HTTPClient()

  const result = httpClient
    .sendRequest(
      runtime,
      postData,
      consensusIdenticalAggregation<PostResponse>()
    )(runtime.config) // Call with config
    .result()

  runtime.log(`Successfully sent data to webhook. Status: ${result.statusCode}`)
  return "Success"
}
```

#### 5. Assemble the full workflow

Finally, add the `initWorkflow` and `main` functions.

```typescript
const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>({
    configSchema,
  })
  await runner.run(initWorkflow)
}

main()
```

#### The complete workflow file

```typescript
import {
  cre,
  ok,
  consensusIdenticalAggregation,
  type Runtime,
  type HTTPSendRequester,
  Runner,
} from "@chainlink/cre-sdk"
import { z } from "zod"

// Config schema
const configSchema = z.object({
  webhookUrl: z.string(),
  schedule: z.string(),
})

type Config = z.infer<typeof configSchema>

// Data to be sent
type MyData = {
  message: string
  value: number
}

// Response for consensus
type PostResponse = {
  statusCode: number
}

const postData = (sendRequester: HTTPSendRequester, config: Config): PostResponse => {
  // 1. Prepare the data to be sent
  const dataToSend: MyData = {
    message: "Hello there!",
    value: 77,
  }

  // 2. Serialize the data to JSON and encode as bytes
  const bodyBytes = new TextEncoder().encode(JSON.stringify(dataToSend))

  // 3. Convert to base64 for the request
  const body = Buffer.from(bodyBytes).toString("base64")

  // 4. Construct the POST request with cacheSettings
  const req = {
    url: config.webhookUrl,
    method: "POST" as const,
    body,
    headers: {
      "Content-Type": "application/json",
    },
    cacheSettings: {
      readFromCache: true, // Enable reading from cache
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  }

  // 5. Send the request and wait for the response
  const resp = sendRequester.sendRequest(req).result()

  if (!ok(resp)) {
    throw new Error(`HTTP request failed with status: ${resp.statusCode}`)
  }

  return { statusCode: resp.statusCode }
}

const onCronTrigger = (runtime: Runtime<Config>): string => {
  const httpClient = new cre.capabilities.HTTPClient()

  const result = httpClient
    .sendRequest(
      runtime,
      postData,
      consensusIdenticalAggregation<PostResponse>()
    )(runtime.config) // Call with config
    .result()

  runtime.log(`Successfully sent data to webhook. Status: ${result.statusCode}`)
  return "Success"
}

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>({
    configSchema,
  })
  await runner.run(initWorkflow)
}

main()
```

### Step 4: Run the simulation and verify

1. **Run the simulation**:

   ```bash
   cre workflow simulate my-workflow --target staging-settings
   ```

2. **Check webhook.site**:

   Open the webhook.site page with your unique URL. You should see a new request appear. Click on it to inspect the details, and you will see the JSON payload you sent.

   ```json
   {
     "message": "Hello there!",
     "value": 77
   }
   ```


<Aside type="note" title="Understanding the Caching Mechanism">
  The `cacheSettings` approach is a **best-effort mechanism** that works reliably in most scenarios. In rare cases, multiple requests may still occur. For more technical details, see the [HTTP Client reference](/cre/reference/sdk/http-client-ts#understanding-cachesettings-behavior).
</Aside>

## 2. The Low-Level `runInNodeMode` Pattern

For more complex scenarios, you can use the lower-level `runtime.runInNodeMode()` method directly. This pattern gives you access to the full `NodeRuntime`, which is essential when you need to use secrets.

<Aside type="note" title="Secrets require this pattern">
  The high-level `sendRequest()` method does **not** provide access to secrets. If you need to use API keys,
  authentication tokens, or any other secrets in your HTTP requests, you must use the `runInNodeMode` pattern.
</Aside>

### Example with secrets

Here's how to make a POST request with an API key from secrets:

```typescript
import { cre, ok, consensusIdenticalAggregation, type Runtime, type NodeRuntime, Runner } from "@chainlink/cre-sdk"
import { z } from "zod"

// Config and types
const configSchema = z.object({
  webhookUrl: z.string(),
  schedule: z.string(),
})

type Config = z.infer<typeof configSchema>

type MyData = {
  message: string
  value: number
}

type PostResponse = {
  statusCode: number
}

// Node-level function that runs on each node
const postData = (nodeRuntime: NodeRuntime<Config>): PostResponse => {
  // 1. Get the API key from secrets
  const secret = nodeRuntime.getSecret({ id: "API_KEY" }).result() // The secret name from your secrets.yaml

  // Use the secret value
  const apiKey = secret.value

  const httpClient = new cre.capabilities.HTTPClient()

  // 2. Prepare the data
  const dataToSend: MyData = {
    message: "Hello there!",
    value: 77,
  }

  // 3. Serialize to JSON and encode as bytes
  const bodyBytes = new TextEncoder().encode(JSON.stringify(dataToSend))

  // 4. Convert to base64 for the request
  const body = Buffer.from(bodyBytes).toString("base64")

  // 5. Construct the POST request with API key in header
  const req = {
    url: nodeRuntime.config.webhookUrl,
    method: "POST" as const,
    body,
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${apiKey}`, // Use the secret
    },
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000,
    },
  }

  // 6. Send the request
  const resp = httpClient.sendRequest(nodeRuntime, req).result()

  if (!ok(resp)) {
    throw new Error(`HTTP request failed with status: ${resp.statusCode}`)
  }

  return { statusCode: resp.statusCode }
}

const onCronTrigger = (runtime: Runtime<Config>): string => {
  const result = runtime.runInNodeMode(postData, consensusIdenticalAggregation<PostResponse>())().result()

  runtime.log(`Successfully sent data to webhook. Status: ${result.statusCode}`)
  return "Success"
}

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      new cre.capabilities.CronCapability().trigger({
        schedule: config.schedule,
      }),
      onCronTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>({
    configSchema,
  })
  await runner.run(initWorkflow)
}

main()
```

## Learn more

- **[HTTP Client SDK Reference](/cre/reference/sdk/http-client-ts)** — Complete API reference with all request options
- **[Secrets](/cre/guides/workflow/secrets)** — Learn how to securely use API keys and sensitive data
- **[GET Requests](/cre/guides/workflow/using-http-client/get-request-ts)** — Learn how to fetch data from APIs

---

# Submitting Reports via HTTP
Source: https://docs.chain.link/cre/guides/workflow/using-http-client/submitting-reports-http-ts
Last Updated: 2025-11-04

This guide shows how to send a cryptographically signed report (generated by your workflow) to an external HTTP API. You'll learn how to write a transformation function that formats the report for your specific API's requirements.

**What you'll learn:**

- How to use `sendReport()` to submit reports via HTTP
- How to write transformation functions for different API formats
- Best practices for report submission and deduplication


<Aside type="note" title="Need onchain submission instead?">
  This guide covers HTTP submission. For submitting reports to smart contracts, see [Submitting Reports Onchain](/cre/guides/workflow/using-evm-client/onchain-write/overview-ts).
</Aside>

## Prerequisites

- Familiarity with [making POST requests](/cre/guides/workflow/using-http-client/post-request)
- Familiarity with `runtime.report()` (covered [below](#generating-reports-for-http-submission))


<Aside type="caution" title="Redirects are not supported">
  HTTP requests to URLs that return redirects (3xx status codes) will fail. Ensure the URL you provide is the final destination and does not redirect to another URL.
</Aside>

## Quick start: Minimal example

Here's the simplest possible workflow that generates and submits a report via HTTP:

```typescript
import { ok, type ReportResponse, type RequestJson, type HTTPSendRequester } from "@chainlink/cre-sdk"

const formatReportSimple = (r: ReportResponse): RequestJson => {
  return {
    url: "https://api.example.com/reports",
    method: "POST",
    body: Buffer.from(r.rawReport).toString("base64"), // Send the raw report bytes (base64-encoded)
    headers: {
      "Content-Type": "application/octet-stream",
    },
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  }
}

const submitReport = (sendRequester: HTTPSendRequester, report: Report): { success: boolean } => {
  const response = sendRequester.sendReport(report, formatReportSimple).result()

  if (!ok(response)) {
    throw new Error(`API returned error: status=${response.statusCode}`)
  }

  return { success: true }
}
```

**What's happening here:**

1. `formatReportSimple` transforms the report into an HTTP request that your API understands
2. `sendRequester.sendReport()` calls your transformation function and sends the request
3. The SDK handles consensus and returns the result

The rest of this guide explains how this works and shows different formatting patterns for various API requirements.

## How it works

### The report structure

When you call `runtime.report()`, the SDK creates a `ReportResponse` containing:

```typescript
interface ReportResponse {
  rawReport: Uint8Array // Your encoded data + metadata
  reportContext: Uint8Array // Workflow execution context
  sigs: AttributedSignature[] // Cryptographic signatures from DON nodes
  configDigest: Uint8Array // DON configuration identifier
  seqNr: bigint // Sequence number
}
```

This structure contains everything your API might need:

- **`rawReport`**: The actual report data (always required)
- **`sigs`**: Cryptographic signatures from DON nodes (for verification)
- **`reportContext`**: Metadata about the workflow execution
- **`seqNr`**: Sequence number

### The transformation function

Your transformation function tells the SDK how to format the report for your API:

```typescript
;(reportResponse: ReportResponse) => RequestJson
```

**The SDK calls this function internally:**

1. You pass your transformation function to `sendReport()`
2. The SDK calls it with the generated `ReportResponse`
3. Your function returns a `RequestJson` formatted for your API
4. The SDK sends the request and handles consensus

**Why is this needed?** Different APIs expect different formats:

- Some want raw binary data
- Some want JSON with base64-encoded fields
- Some want signatures in headers, others in the body

The transformation function gives you complete control over the format.

## Formatting patterns

Here are common patterns for formatting reports. Choose the one that matches your API's requirements.

### Choosing the right pattern

| Pattern                                                                                                 | When to use                                               |
| ------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
| [**Pattern 1: Report in body**](#pattern-1-report-in-body-simplest)                                     | Your API accepts raw binary data and handles decoding     |
| [**Pattern 2: Report + signatures in body**](#pattern-2-report--signatures-in-body)                     | Your API needs everything concatenated in one binary blob |
| [**Pattern 3: Report in body, signatures in headers**](#pattern-3-report-in-body-signatures-in-headers) | Your API needs signatures separated for easier parsing    |
| [**Pattern 4: JSON-formatted report**](#pattern-4-json-formatted-report)                                | Your API only accepts JSON payloads                       |

### Pattern 1: Report in body (simplest)

Use this when your API accepts raw binary data:

```typescript
import type { ReportResponse, RequestJson } from "@chainlink/cre-sdk"

const formatReportSimple = (r: ReportResponse): RequestJson => {
  return {
    url: "https://api.example.com/reports",
    method: "POST",
    body: Buffer.from(r.rawReport).toString("base64"), // Just send the report (base64-encoded)
    headers: {
      "Content-Type": "application/octet-stream",
    },
    cacheSettings: {
      readFromCache: true, // Enable caching
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  }
}
```

### Pattern 2: Report + signatures in body

Use this when your API needs everything concatenated in one payload:

```typescript
const formatReportWithSignatures = (r: ReportResponse): RequestJson => {
  // Concatenate report, context, and all signatures
  const reportBytes = new Uint8Array(r.rawReport)
  const contextBytes = new Uint8Array(r.reportContext)

  let totalLength = reportBytes.length + contextBytes.length
  for (const sig of r.sigs) {
    totalLength += sig.signature.length
  }

  const body = new Uint8Array(totalLength)
  let offset = 0

  // Copy report
  body.set(reportBytes, offset)
  offset += reportBytes.length

  // Copy context
  body.set(contextBytes, offset)
  offset += contextBytes.length

  // Copy all signatures
  for (const sig of r.sigs) {
    body.set(new Uint8Array(sig.signature), offset)
    offset += sig.signature.length
  }

  return {
    url: "https://api.example.com/reports",
    method: "POST",
    body: Buffer.from(body).toString("base64"),
    headers: {
      "Content-Type": "application/octet-stream",
    },
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000,
    },
  }
}
```

### Pattern 3: Report in body, signatures in headers

Use this when your API needs signatures separated for easier parsing:

```typescript
const formatReportWithHeaderSigs = (r: ReportResponse): RequestJson => {
  const headers: { [key: string]: string } = {
    "Content-Type": "application/octet-stream",
  }

  // Add signatures to headers
  r.sigs.forEach((sig, i) => {
    const sigKey = `X-Signature-${i}`
    const signerKey = `X-Signer-ID-${i}`

    headers[sigKey] = Buffer.from(sig.signature).toString("base64")
    headers[signerKey] = sig.signerId.toString()
  })

  return {
    url: "https://api.example.com/reports",
    method: "POST",
    body: Buffer.from(r.rawReport).toString("base64"),
    headers,
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000,
    },
  }
}
```

### Pattern 4: JSON-formatted report

Use this when your API only accepts JSON payloads:

```typescript
interface ReportPayload {
  report: string
  context: string
  signatures: string[]
  configDigest: string
  seqNr: string
}

const formatReportAsJSON = (r: ReportResponse): RequestJson => {
  // Extract signatures
  const sigs = r.sigs.map((sig) => Buffer.from(sig.signature).toString("base64"))

  // Create JSON payload
  const payload: ReportPayload = {
    report: Buffer.from(r.rawReport).toString("base64"),
    context: Buffer.from(r.reportContext).toString("base64"),
    signatures: sigs,
    configDigest: Buffer.from(r.configDigest).toString("base64"),
    seqNr: r.seqNr.toString(),
  }

  const bodyBytes = new TextEncoder().encode(JSON.stringify(payload))

  return {
    url: "https://api.example.com/reports",
    method: "POST",
    body: Buffer.from(bodyBytes).toString("base64"),
    headers: {
      "Content-Type": "application/json",
    },
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000,
    },
  }
}
```

### Understanding `cacheSettings` for reports

You'll notice that all the patterns above include `cacheSettings`. This is critical for report submissions, just like it is for [POST requests](/cre/guides/workflow/using-http-client/post-request).

For a complete explanation of how `cacheSettings` works in general, see [Understanding `CacheSettings` behavior](/cre/reference/sdk/http-client-ts#understanding-cachesettings-behavior) in the HTTP Client reference.

**Why use `cacheSettings`?**

When a workflow executes, **all nodes in the DON** attempt to send the report to your API. Without caching, your API would receive multiple identical submissions (one from each node). `cacheSettings` prevents this by having the first node cache the response, which other nodes can reuse.

**Why are cache hits limited for reports?**

Unlike regular POST requests where caching can be very effective, **reports have a more limited cache effectiveness** due to signature variance:

1. Each DON node generates its own **unique cryptographic signature** for the report
2. These signatures are part of the `ReportResponse` structure
3. When nodes construct the HTTP request body (whether concatenating signatures or including them in headers), the signatures differ

**In practice:** Even though cache hits are limited, you should still include `cacheSettings` to prevent worst-case scenarios where all nodes hit your API simultaneously.

**The real solution: API-side deduplication**

Because caching alone cannot prevent all duplicate submissions, your receiving API **must implement its own deduplication logic**:

- Use the **hash of the report** (`keccak256(rawReport)`) as the unique identifier
- Store this hash when processing a report
- Reject any subsequent submissions with the same hash

This approach is reliable because the `rawReport` is identical across all nodes—only the signatures vary.

## Generating reports for HTTP submission

Before you can submit a report via HTTP, you need to generate it using `runtime.report()`. This creates a cryptographically signed report from your encoded data.

**Basic pattern:**

```typescript
import { hexToBase64, type Runtime } from "@chainlink/cre-sdk"
import { encodeAbiParameters, parseAbiParameters } from "viem"

// Step 1: Encode your data using Viem
const encodedValue = encodeAbiParameters(parseAbiParameters("uint256 value"), [123456789n])

// Step 2: Generate the signed report
const report = runtime
  .report({
    encodedPayload: hexToBase64(encodedValue),
    encoderName: "evm",
    signingAlgo: "ecdsa",
    hashingAlgo: "keccak256",
  })
  .result()

// Step 3: Submit via HTTP (covered in next section)
```

The `runtime.report()` method works the same way whether you're encoding a single value or a struct—just use Viem's `encodeAbiParameters()` with the appropriate ABI types. For detailed examples on encoding single values, structs, and complex types, see the [Writing Data Onchain](/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain) guide.

## Using `sendReport()` (recommended approach)

Use the high-level `httpClient.sendRequest()` pattern with `sendRequester.sendReport()`:

```typescript
import {
  cre,
  consensusIdenticalAggregation,
  ok,
  type Runtime,
  type HTTPSendRequester,
  type Report,
} from "@chainlink/cre-sdk"

interface SubmitResponse {
  success: boolean
}

const submitReportViaHTTP = (sendRequester: HTTPSendRequester, report: Report): SubmitResponse => {
  const response = sendRequester.sendReport(report, formatReportSimple).result()

  if (!ok(response)) {
    throw new Error(`API returned error: status=${response.statusCode}`)
  }

  runtime.log(`Report submitted successfully, status: ${response.statusCode}`)
  return { success: true }
}

// In your trigger callback
const onCronTrigger = (runtime: Runtime<Config>): MyResult => {
  const httpClient = new cre.capabilities.HTTPClient()

  // Assume 'report' was generated earlier in your workflow

  // Call the submission function
  const result = httpClient
    .sendRequest(
      runtime,
      (sendRequester: HTTPSendRequester) => submitReportViaHTTP(sendRequester, report),
      consensusIdenticalAggregation<SubmitResponse>()
    )()
    .result()

  return {}
}
```

## Complete working example

This example shows a workflow that:

1. Generates a report from a single value
2. Submits it to an HTTP API
3. Uses the simple "report in body" format

```typescript
import {
  cre,
  Runner,
  consensusIdenticalAggregation,
  hexToBase64,
  ok,
  type Runtime,
  type Report,
  type CronPayload,
  type HTTPSendRequester,
  type ReportResponse,
  type RequestJson,
} from "@chainlink/cre-sdk"
import { encodeAbiParameters, parseAbiParameters } from "viem"

interface Config {
  apiUrl: string
  schedule: string
}

interface SubmitResponse {
  success: boolean
}

type MyResult = Record<string, never>

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

// Transformation function: defines how the API expects the report
const formatReportForMyAPI = (r: ReportResponse): RequestJson => {
  return {
    url: "https://webhook.site/your-unique-id", // Replace with your API
    method: "POST",
    body: Buffer.from(r.rawReport).toString("base64"),
    headers: {
      "Content-Type": "application/octet-stream",
      "X-Report-SeqNr": r.seqNr.toString(),
    },
    cacheSettings: {
      readFromCache: true, // Prevent duplicate submissions
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  }
}

// Function that submits the report via HTTP
const submitReportViaHTTP = (sendRequester: HTTPSendRequester, report: Report): SubmitResponse => {
  runtime.log("Submitting report to API")

  const response = sendRequester.sendReport(report, formatReportForMyAPI).result()

  runtime.log(`Report submitted - status: ${response.statusCode}, bodyLength: ${response.body.length}`)

  if (!ok(response)) {
    const bodyText = new TextDecoder().decode(response.body)
    throw new Error(`API error: status=${response.statusCode}, body=${bodyText}`)
  }

  return { success: true }
}

const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): MyResult => {
  // Step 1: Generate a report (example: a single uint256 value)
  const myValue = 123456789n
  runtime.log(`Generating report with value: ${myValue}`)

  // Encode the value using Viem
  const encodedValue = encodeAbiParameters(parseAbiParameters("uint256 value"), [myValue])

  // Generate the report
  const reportResponse = runtime
    .report({
      encodedPayload: hexToBase64(encodedValue),
      encoderName: "evm",
      signingAlgo: "ecdsa",
      hashingAlgo: "keccak256",
    })
    .result()

  runtime.log("Report generated successfully")

  // Step 2: Submit the report via HTTP
  const httpClient = new cre.capabilities.HTTPClient()

  const submitResult = httpClient
    .sendRequest(
      runtime,
      (sendRequester: HTTPSendRequester) => submitReportViaHTTP(sendRequester, reportResponse),
      consensusIdenticalAggregation<SubmitResponse>()
    )()
    .result()

  runtime.log(`Workflow completed successfully, submitted: ${submitResult.success}`)
  return {}
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

### Configuration file (`config.json`)

```json
{
  "apiUrl": "https://webhook.site/your-unique-id",
  "schedule": "0 * * * *"
}
```

### Testing with webhook.site

1. Go to [webhook.site](https://webhook.site/) and get a unique URL
2. Update `config.json` with your webhook URL
3. Run the simulation:
   ```bash
   cre workflow simulate my-workflow --target staging-settings
   ```
4. Check webhook.site to see the report data received

## Advanced: Low-level pattern

For complex scenarios where you need more control, use `clientCapability.sendReport()` with `runtime.runInNodeMode()`:

```typescript
import { cre, consensusIdenticalAggregation, ok, type Runtime, type NodeRuntime, type Report } from "@chainlink/cre-sdk"

const onCronTrigger = (runtime: Runtime<Config>): MyResult => {
  // Assume 'report' was generated earlier

  const result = runtime
    .runInNodeMode((nodeRuntime: NodeRuntime<Config>) => {
      const httpClient = new cre.capabilities.HTTPClient()

      const response = httpClient.sendReport(nodeRuntime, report, formatReportSimple).result()

      if (!ok(response)) {
        throw new Error(`API error: ${response.statusCode}`)
      }

      return { success: true }
    }, consensusIdenticalAggregation<SubmitResponse>())()
    .result()

  return {}
}
```

## Best practices

1. **Always use `cacheSettings`**: Include caching in every transformation function to prevent worst-case duplicate submission scenarios
2. **Implement API-side deduplication**: Your receiving API must implement deduplication using the **hash of the report** (`keccak256(rawReport)`) to detect and reject duplicate submissions
3. **Verify signatures before processing**: Your API must verify the cryptographic signatures against DON public keys before trusting report data (see note below about signature verification)
4. **Match your API's format exactly**: Study your API's documentation to understand the expected format (binary, JSON, headers, etc.)
5. **Handle errors gracefully**: Check HTTP status codes and provide meaningful error messages


<Aside type="caution" title="Signature verification is your responsibility">
  Unlike onchain submissions (where the `KeystoneForwarder` contract verifies signatures), **HTTP submissions require your API to verify signatures** before trusting the report data.

  **Documentation coming soon**: "Verifying CRE Reports Offchain" guide.
</Aside>

## Troubleshooting

**"failed to send report" error**

- Verify your API URL is correct and accessible
- Check that your transformation function returns a valid `RequestJson`
- Ensure your API can handle binary data if you're sending raw bytes (base64-encoded)

**API returns 400/422 errors**

- Your report format likely doesn't match what your API expects
- Check if your API expects base64 encoding, JSON wrapping, or specific headers

## Learn more

- **[HTTP Client SDK Reference](/cre/reference/sdk/http-client-ts)** — Complete API reference including `sendReport()` and `ReportResponse`
- **[POST Requests](/cre/guides/workflow/using-http-client/post-request)** — Learn about HTTP request patterns and caching
- **[Writing Data Onchain](/cre/guides/workflow/using-evm-client/onchain-write/writing-data-onchain)** — Detailed guide on encoding single values, structs, and complex types using Viem
- **[Submitting Reports Onchain](/cre/guides/workflow/using-evm-client/onchain-write/submitting-reports-onchain)** — Alternative: Submit reports to smart contracts instead of HTTP

---

# Cron Trigger
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/cron-trigger-ts
Last Updated: 2025-11-04

The Cron trigger fires based on a time-based schedule, defined by a standard cron expression.

**Use case examples:**

- Periodically fetching data from an API.
- Regularly checking an onchain state.
- Regularly writing data to an onchain contract.

## Configuration and handler

You create a Cron trigger by calling the `CronCapability.trigger()` method and register it with a handler inside your `initWorkflow` function.

When you configure a Cron trigger, you must provide a `schedule` using a standard cron expression. The expression can contain 5 or 6 fields, where the optional 6th field represents seconds.

For help understanding or creating cron expressions, see <a href="https://crontab.guru/" target="_blank" rel="noopener noreferrer">crontab.guru</a> (note: this tool supports 5-field expressions; add a seconds field at the beginning for 6-field expressions).

**Examples:**

- Every 30 seconds (6 fields): `*/30 * * * * *`
- Every minute, at second 0 (6 fields): `0 * * * * *`
- Every hour, at the top of the hour (6 fields): `0 0 * * * *`
- Every 5 minutes from 08:00 to 08:59, Monday to Friday (5 fields): `*/5 8 * * 1-5`

### Timezone support

By default, cron expressions use UTC. To specify a different timezone, prefix your cron expression with `TZ=<timezone>`, where `<timezone>` is an <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones" target="_blank" rel="noopener noreferrer">IANA timezone identifier</a> (e.g., `America/New_York`, `Europe/London`, `Asia/Tokyo`).

**Examples with timezones:**

- Daily at midnight in New York: `TZ=America/New_York 0 0 * * *`
- Every Sunday at 8 PM in Tokyo: `TZ=Asia/Tokyo 0 20 * * 0`
- Every weekday at 9 AM in London: `TZ=Europe/London 0 9 * * 1-5`

The timezone-aware scheduler automatically handles daylight saving time transitions, ensuring your workflows run at the correct local time throughout the year.

<Aside type="note" title="Minimum Interval">
  You cannot schedule a trigger to fire more frequently than once every 30 seconds.
</Aside>

```typescript
import { cre, type Runtime, type CronPayload, Runner } from "@chainlink/cre-sdk"

type Config = {}

// Callback function that runs when the cron trigger fires
const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  if (payload.scheduledExecutionTime) {
    const seconds = payload.scheduledExecutionTime.seconds
    runtime.log(`Cron trigger fired at ${seconds}`)
  }
  // Your logic here...
  return "Trigger completed"
}

const initWorkflow = (config: Config) => {
  // Create the trigger - fires every 30 seconds in UTC
  const cronTrigger = new cre.capabilities.CronCapability().trigger({
    schedule: "*/30 * * * * *",
  })

  // Or use a timezone-aware schedule - fires daily at 9 AM Eastern Time
  // const cronTrigger = new cre.capabilities.CronCapability().trigger({
  //   schedule: "TZ=America/New_York 0 9 * * *",
  // })

  // Register a handler with the trigger and a callback function
  return [cre.handler(cronTrigger, onCronTrigger)]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Callback and payload

When a Cron trigger fires, it passes a `CronPayload` object to your callback function. This payload contains the scheduled execution time.

For the full type definition and all available fields, see the [Cron Trigger SDK Reference](/cre/reference/sdk/triggers/cron-trigger).

The payload parameter is optional in the callback function signature. If you don't need access to the scheduled execution time, you can omit it:

```typescript
// Simple callback without payload
const onCronTrigger = (runtime: Runtime<Config>): string => {
  runtime.log("Cron trigger fired")
  // Your logic here...
  return "Cron trigger completed"
}
```

If you need to access the scheduled execution time, include the `CronPayload` parameter:

```typescript
const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  if (payload.scheduledExecutionTime) {
    // Convert timestamp to JavaScript Date (timestamp has 'seconds' and 'nanos' fields)
    const scheduledTime = new Date(
      Number(payload.scheduledExecutionTime.seconds) * 1000 + payload.scheduledExecutionTime.nanos / 1000000
    )
    runtime.log(`Cron trigger fired at ${scheduledTime.toISOString()}`)
  }
  // Your logic here...
  return "Cron trigger completed"
}
```

## Testing cron triggers in simulation

To test your cron trigger during development, you can use the workflow simulator. The simulator executes cron triggers immediately when selected, allowing you to test your logic without waiting for the scheduled time.

For detailed instructions on simulating cron triggers, including interactive and non-interactive modes, see the [Cron Trigger section in the Simulating Workflows guide](/cre/guides/operations/simulating-workflows#cron-trigger).

---

# EVM Log Trigger
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/evm-log-trigger-ts
Last Updated: 2025-11-04

The EVM Log trigger fires when a specific log (event) is emitted by a smart contract on an EVM-compatible blockchain. This capability allows you to build powerful, event-driven workflows that react to onchain activity.

This guide explains the two key parts of working with log triggers:

- **[How to configure your workflow to listen for specific events](#configuring-your-trigger)**
- **[How to decode the event data your workflow receives](#decoding-the-event-payload)**

## Configuring your trigger

You create an EVM Log trigger by calling the `EVMClient.logTrigger()` method with a `FilterLogTriggerRequest` configuration. This configuration specifies which contract addresses and event topics to listen for.

<Aside type="note" title="Base64 Encoding Required">
  **All addresses and topic values must be base64 encoded** using the `hexToBase64()` helper function from the CRE SDK.
  While the workflow simulator accepts raw hex strings for convenience during development, **deployed workflows require
  base64 encoding**. Always use `hexToBase64()` on addresses and topic values to ensure your workflow works in both
  simulation and production.
</Aside>

### Basic configuration

The simplest configuration listens for **all events** from specific contract addresses:

```typescript
import { cre, getNetwork, type Runtime, type EVMLog, Runner, bytesToHex, hexToBase64 } from "@chainlink/cre-sdk"

type Config = {
  chainSelectorName: string
  contractAddress: string
}

// Callback function that runs when an event log is detected
const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  runtime.log(`Log detected from ${bytesToHex(log.address)}`)
  // Your logic here...
  return "Log processed"
}

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
      }),
      onLogTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

### Filtering by event type

To listen for **specific event types**, you need to provide the event's signature hash as the first topic (`Topics[0]`). You can compute this using viem's `keccak256` and `toBytes` functions:

```typescript
import { keccak256, toBytes } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // Compute the event signature hash for Transfer(address,address,uint256)
  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(transferEventHash)] }, // Listen only for Transfer events
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

### Filtering by indexed parameters

EVM events can have up to 3 `indexed` parameters (in addition to the event signature). You can filter on these indexed parameters by providing their values in the `topics` array.

**Understanding topic filtering:**

- **`addresses`**: The trigger fires if the event is emitted from **any** contract in this list (**OR** logic).
- **`topics`**: An event must match the conditions for **all** defined topic slots (**AND** logic between topics). Within a single topic, you can provide multiple values, and it will match if the event's topic is **any** of those values (**OR** logic within a topic).
- **Wildcarding topics**: To skip filtering on a specific topic position, omit it from the topics array or provide an empty values array `{ values: [] }`. For example, to filter on topic 1 and topic 3 but not topic 2, you would provide `[topic0, topic1, { values: [] }, topic3]`.

<Aside type="caution" title="Topic values must be padded to 32 bytes and base64 encoded">
  EVM logs always store indexed parameters as **32-byte values**. When filtering on topics 1, 2, or 3:

  1. **Pad your values to 32 bytes** using `padHex(value, { size: 32 })` (e.g., addresses are 20 bytes and must be padded)
  2. **Convert to base64** using `hexToBase64()`

  If you don't pad correctly, your filter won't match the actual log topics and the trigger will not fire.

  Topic 0 (the event signature from `keccak256`) is already 32 bytes and doesn't need padding.
</Aside>

#### Example 1: Filtering on a single indexed parameter

To trigger only on `Transfer` events where the `from` address is a specific value:

```typescript
import { keccak256, toBytes, padHex } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))
  const aliceAddress = "0xAlice..." as `0x${string}`

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(transferEventHash)] }, // Topic 0: Event signature (Transfer)
          { values: [hexToBase64(padHex(aliceAddress, { size: 32 }))] }, // Topic 1: from = Alice
        ],
      }),
      onLogTrigger
    ),
  ]
}
```


<Aside type="note" title="Indexed Parameters and Topics">
  Only parameters marked as `indexed` in the Solidity event definition can be filtered using topics. The event signature is always `Topics[0]`. Subsequent indexed parameters are `Topics[1]`, `Topics[2]`, and `Topics[3]`. Encoding different types:

  - **Addresses**: Cast as `` `0x${string}` ``, use `padHex(address, { size: 32 })` then `hexToBase64()`
  - **uint256**: Use `padHex(numberToHex(value), { size: 32 })` then `hexToBase64()`
  - **bytes32**: Ensure it's 32 bytes, then use `hexToBase64()` directly
</Aside>

#### Example 2: "AND" filtering

To trigger on `Transfer` events where `from` is Alice **AND** `to` is Bob:

```typescript
import { keccak256, toBytes, padHex } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))
  const aliceAddress = "0xAlice..." as `0x${string}`
  const bobAddress = "0xBob..." as `0x${string}`

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(transferEventHash)] }, // Topic 0: Event signature (Transfer)
          { values: [hexToBase64(padHex(aliceAddress, { size: 32 }))] }, // Topic 1: from = Alice
          { values: [hexToBase64(padHex(bobAddress, { size: 32 }))] }, // Topic 2: to = Bob
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

#### Example 3: "OR" filtering

To trigger on `Transfer` events where `from` is **either** Alice **OR** Charlie:

```typescript
import { keccak256, toBytes, padHex } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))
  const aliceAddress = "0xAlice..." as `0x${string}`
  const charlieAddress = "0xCharlie..." as `0x${string}`

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(transferEventHash)] }, // Topic 0: Event signature (Transfer)
          {
            values: [
              hexToBase64(padHex(aliceAddress, { size: 32 })),
              hexToBase64(padHex(charlieAddress, { size: 32 })),
            ],
          }, // Topic 1: from = Alice OR Charlie
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

#### Example 4: Multiple event types

To listen for **multiple event types** from a single contract, provide multiple event signature hashes in `Topics[0]`:

```typescript
import { keccak256, toBytes } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))
  const approvalEventHash = keccak256(toBytes("Approval(address,address,uint256)"))

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(transferEventHash), hexToBase64(approvalEventHash)] }, // Listen for Transfer OR Approval
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

#### Example 5: Multiple contracts

To listen for the **same event from multiple contracts**, provide multiple addresses:

```typescript
import { keccak256, toBytes } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64("0xTokenA..."), hexToBase64("0xTokenB..."), hexToBase64("0xTokenC...")],
        topics: [
          { values: [hexToBase64(transferEventHash)] }, // Listen for Transfer events from any of these contracts
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

#### Example 6: Filtering on uint256 indexed parameter

To filter on indexed `uint256` or other numeric types, convert them to a 32-byte hex value:

```typescript
import { keccak256, toBytes, numberToHex, padHex } from "viem"
import { hexToBase64 } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  // Example: event ValueChanged(address indexed user, uint256 indexed newValue)
  const eventHash = keccak256(toBytes("ValueChanged(address,uint256)"))
  const userAddress = padHex("0xUser..." as `0x${string}`, { size: 32 })
  const targetValue = padHex(numberToHex(12345), { size: 32 })

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.contractAddress)],
        topics: [
          { values: [hexToBase64(eventHash)] }, // Topic 0: Event signature
          { values: [hexToBase64(userAddress)] }, // Topic 1: user address
          { values: [hexToBase64(targetValue)] }, // Topic 2: newValue = 12345
        ],
      }),
      onLogTrigger
    ),
  ]
}
```

<Aside type="note" title="Converting Numbers to Topics">
  For indexed `uint256` parameters, use `numberToHex()` to convert the number to hex, then `padHex()` to ensure it's 32
  bytes, and finally `hexToBase64()` to encode it for the trigger configuration. For `bytes32` parameters, ensure
  they're already 32 bytes and apply `hexToBase64()` directly.
</Aside>

### Confidence level

You can set the block confirmation level by adding the `confidence` field to the trigger configuration:

```typescript
evmClient.logTrigger({
  addresses: [hexToBase64(config.contractAddress)],
  confidence: "CONFIDENCE_LEVEL_FINALIZED", // Wait for finalized blocks
})
```

See the [EVM Log Trigger reference](/cre/reference/sdk/triggers/evm-log-trigger-ts#configuration) for details on the available confidence levels.

## Decoding the event payload

Once your trigger is configured, your handler function receives an `EVMLog` object. For the full type definition and all available fields, see the [EVM Log Trigger SDK Reference](/cre/reference/sdk/triggers/evm-log-trigger-ts#payload).

This object contains:

| Field         | Description                                           |
| ------------- | ----------------------------------------------------- |
| `address`     | The contract address that emitted the event           |
| `topics`      | An array of indexed event parameters                  |
| `data`        | The non-indexed event parameters                      |
| `eventSig`    | The keccak256 hash of the event signature             |
| `blockNumber` | The block number where the event was emitted          |
| `blockHash`   | The block hash                                        |
| `txHash`      | The transaction hash                                  |
| `txIndex`     | The transaction index within the block                |
| `index`       | The log index within the block                        |
| `removed`     | Flag indicating if the log was removed during a reorg |

### Method 1: Manual topic extraction

The simplest approach is to manually extract values from the `topics` array. This is useful when you only need a few indexed parameters.

For example, to decode a `Transfer(address indexed from, address indexed to, uint256 value)` event:

```typescript
import { bytesToHex } from "@chainlink/cre-sdk"
import type { EVMLog, Runtime } from "@chainlink/cre-sdk"

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics

  if (topics.length < 3) {
    throw new Error("Log missing required topics")
  }

  // topics[0] is the event signature
  runtime.log(`Event signature: ${bytesToHex(topics[0])}`)

  // topics[1] is the first indexed parameter (from address for Transfer)
  // Addresses are 32 bytes, but the actual address is the last 20 bytes
  const fromAddress = bytesToHex(topics[1].slice(12))
  runtime.log(`From address: ${fromAddress}`)

  // topics[2] is the second indexed parameter (to address for Transfer)
  const toAddress = bytesToHex(topics[2].slice(12))
  runtime.log(`To address: ${toAddress}`)

  // For non-indexed parameters, you'll need to decode log.data using viem
  runtime.log(`Data length: ${log.data.length} bytes`)

  return "Log processed"
}
```

### Method 2: Using viem's `decodeEventLog`

For more complex events or when you need to decode non-indexed parameters, you can use viem's `decodeEventLog` function. First, define your event ABI:

```typescript
import { decodeEventLog, parseAbi } from "viem"
import { bytesToHex } from "@chainlink/cre-sdk"
import type { EVMLog, Runtime } from "@chainlink/cre-sdk"

// Define your event ABI
const eventAbi = parseAbi([
  "event Transfer(address indexed from, address indexed to, uint256 value)",
  "event Approval(address indexed owner, address indexed spender, uint256 value)",
])

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  // Convert topics and data to hex format for viem
  const topics = log.topics.map((topic) => bytesToHex(topic)) as [`0x${string}`, ...`0x${string}`[]]
  const data = bytesToHex(log.data)

  // Decode the event
  const decodedLog = decodeEventLog({
    abi: eventAbi,
    data,
    topics,
  })

  runtime.log(`Event name: ${decodedLog.eventName}`)

  if (decodedLog.eventName === "Transfer") {
    const { from, to, value } = decodedLog.args
    runtime.log(`Transfer from ${from} to ${to}, value: ${value.toString()}`)
  } else if (decodedLog.eventName === "Approval") {
    const { owner, spender, value } = decodedLog.args
    runtime.log(`Approval by ${owner} to ${spender}, value: ${value.toString()}`)
  }

  return "Log decoded"
}
```

<Aside type="note" title="Using Contract ABI Files">
  For complex workflows, consider defining your contract ABIs in separate TypeScript files (e.g.,
  `contracts/abi/MyContract.ts`) and importing them. This approach provides better type safety and reusability. See the
  [Reading an Onchain Value guide](/cre/getting-started/part-3-reading-onchain-value) for an example of this pattern.
</Aside>


<Aside type="note" title="Type Assertion for Topics">
  The type assertion

  ```
  as [`0x${string}`, ...`0x${string}`[]]
  ```

  tells TypeScript that `topics` is a non-empty array of
  hex strings (required by viem's `decodeEventLog`). This ensures the event signature is always present as the first
  element.
</Aside>

### Method 3: Manual decoding with viem utilities

If you need fine-grained control, you can manually decode specific fields using viem's utilities:

```typescript
import { bytesToHex } from "@chainlink/cre-sdk"
import { decodeAbiParameters, parseAbiParameters } from "viem"
import type { EVMLog, Runtime } from "@chainlink/cre-sdk"

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics

  // Manually extract indexed parameters
  const fromAddress = bytesToHex(topics[1].slice(12))
  const toAddress = bytesToHex(topics[2].slice(12))

  // Decode non-indexed parameters from log.data
  const decodedData = decodeAbiParameters(parseAbiParameters("uint256 value"), bytesToHex(log.data))
  const value = decodedData[0]

  runtime.log(`Transfer: ${fromAddress} -> ${toAddress}, value: ${value.toString()}`)

  return "Log decoded"
}
```

## Complete example

Here's a complete example that listens for ERC20 `Transfer` events and decodes them:

```typescript
import { cre, getNetwork, type Runtime, type EVMLog, Runner, bytesToHex, hexToBase64 } from "@chainlink/cre-sdk"
import { keccak256, toBytes, decodeEventLog, parseAbi } from "viem"

type Config = {
  chainSelectorName: string
  tokenAddress: string
}

const eventAbi = parseAbi(["event Transfer(address indexed from, address indexed to, uint256 value)"])

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics.map((topic) => bytesToHex(topic)) as [`0x${string}`, ...`0x${string}`[]]
  const data = bytesToHex(log.data)

  const decodedLog = decodeEventLog({
    abi: eventAbi,
    data,
    topics,
  })

  const { from, to, value } = decodedLog.args
  runtime.log(`Transfer detected: ${from} -> ${to}, amount: ${value.toString()}`)

  return `Processed transfer of ${value.toString()}`
}

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)
  const transferEventHash = keccak256(toBytes("Transfer(address,address,uint256)"))

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [hexToBase64(config.tokenAddress)],
        topics: [{ values: [hexToBase64(transferEventHash)] }],
        confidence: "CONFIDENCE_LEVEL_FINALIZED",
      }),
      onLogTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Testing log triggers in simulation

To test your EVM log trigger during development, you can use the workflow simulator with a transaction hash and event index. The simulator fetches the log from your configured RPC and passes it to your callback function.

For detailed instructions on simulating EVM log triggers, including interactive and non-interactive modes, see the [EVM Log Trigger section in the Simulating Workflows guide](/cre/guides/operations/simulating-workflows#evm-log-trigger).

---

# HTTP Trigger: Configuration & Handler
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/http-trigger/configuration-ts
Last Updated: 2025-11-10

The HTTP trigger fires when an external system makes an HTTP request to the trigger endpoint.

**Use case examples:**

- Integrating with existing web services or webhooks.
- Allowing an external system to initiate a workflow on demand.
- Creating a user-facing endpoint to run a specific piece of logic.

## Configuration and handler

You create an HTTP trigger by calling the `HTTPCapability.trigger()` method. Its configuration requires a set of authorized public keys to validate incoming request signatures.


<Aside type="note" title="Authorization required for deployment">
  When you deploy your workflow, HTTP triggers **must** include `authorizedKeys`. An empty configuration object `{}` is only valid for simulation and testing—deployed workflows will reject HTTP triggers without authorization keys.
</Aside>

```typescript
import { cre, type Runtime, type HTTPPayload, Runner } from "@chainlink/cre-sdk"

type Config = {
  authorizedEVMAddress: string
}

// Callback function that runs when an HTTP request is received
const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): string => {
  runtime.log(`HTTP trigger received: ${payload.input.length} bytes`)
  // Your logic here...
  return "Request processed"
}

const initWorkflow = (config: Config) => {
  const httpTrigger = new cre.capabilities.HTTPCapability()

  return [
    cre.handler(
      httpTrigger.trigger({
        authorizedKeys: [
          {
            type: "KEY_TYPE_ECDSA_EVM",
            publicKey: config.authorizedEVMAddress,
          },
        ],
      }),
      onHttpTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

**About authorized keys:**

- **`publicKey`**: An EVM address (e.g., `"0xb08E004bd2b5aFf1F5F950d141f449B1c05800eb"`) that is authorized to trigger the workflow
- **`type`**: Must be `"KEY_TYPE_ECDSA_EVM"` (currently the only supported authentication method)
- **Multiple keys**: You can include multiple authorized addresses in the array

When an HTTP request is made to trigger your workflow, CRE verifies that the request was signed by a private key corresponding to one of the authorized addresses.

## Callback and payload

The HTTP trigger passes an `HTTPPayload` to your callback. This object contains the request body (`input`) and the signing key (`key`) from the incoming HTTP request.

For the full type definition and all available fields, see the [HTTP Trigger SDK Reference](/cre/reference/sdk/triggers/http-trigger).

```typescript
type RequestData = {
  message: string
  value: number
}

const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): string => {
  // The payload.input is a Uint8Array.
  // You can decode it to a JSON object using the decodeJson helper.
  const requestData = decodeJson<RequestData>(payload.input)
  runtime.log(`Received HTTP request: ${JSON.stringify(requestData)}`)

  // Your logic here...
  // The value returned from your callback will be sent back as the HTTP response.
  return `Request processed: ${requestData.message}`
}
```

<Aside type="note" title="For local simulation only">
  During local simulation with `cre workflow simulate`, you can use an empty configuration `trigger({})` to test your
  workflow without setting up authorization keys. This is convenient for rapid development, but remember to add
  `authorizedKeys` before deploying. See [Testing in
  Simulation](/cre/guides/workflow/using-triggers/http-trigger/testing-in-simulation) for details.
</Aside>

---

# HTTP Trigger Overview
Source: https://docs.chain.link/cre/guides/workflow/using-triggers/http-trigger/overview-ts
Last Updated: 2025-11-10

The HTTP trigger allows external systems to initiate your workflow execution by making HTTP requests to a designated endpoint. This enables on-demand workflow execution, webhook integration, and API-driven automation.

## How HTTP triggers work

When you deploy a workflow with an HTTP trigger:

1. **External systems send HTTP POST requests** to a CRE gateway
2. **The request specifies your workflow ID** in the JSON-RPC body along with the input payload
3. **Requests must be cryptographically signed** using a private key corresponding to an authorized EVM address in the target workflow
4. **CRE validates the signature** against your configured `authorizedKeys`
5. **If authorized**, your workflow callback executes with the request payload

<Aside type="caution" title="Authorization required for deployed workflows">
  For **deployed workflows**, HTTP triggers use cryptographic signatures to ensure only authorized addresses can execute
  your workflow. During **local simulation**, you can use empty authorization configs to simplify testing—just remember
  to add `authorizedKeys` before deploying. Learn more: [Configuration &
  Handler](/cre/guides/workflow/using-triggers/http-trigger/configuration-ts#configuration-and-handler).
</Aside>

## When to use HTTP triggers

HTTP triggers are ideal for:

- **Webhook integration**: Receive events from external services (GitHub, payment processors, etc.)
- **On-demand execution**: Allow users or systems to trigger specific workflow logic when needed
- **API gateway patterns**: Create authenticated endpoints that execute blockchain operations
- **Event bridging**: Connect offchain systems events to workflows

## The complete HTTP trigger journey

This section provides everything you need to work with HTTP triggers:

1. **[Configuration & Handler](/cre/guides/workflow/using-triggers/http-trigger/configuration)** - Learn how to configure HTTP triggers in your workflow code and write handler functions to process incoming requests

2. **[Testing in Simulation](/cre/guides/workflow/using-triggers/http-trigger/testing-in-simulation)** - Test your HTTP trigger locally using the `cre workflow simulate` command before deploying

3. **[Triggering Deployed Workflows](/cre/guides/workflow/using-triggers/http-trigger/triggering-deployed-workflows)** - Understand the JSON-RPC format and JWT authentication required to trigger your deployed workflows

4. **[Testing with Local JWT Server](/cre/guides/workflow/using-triggers/http-trigger/local-testing-tool)** - Run a local proxy server that automatically generates JWT tokens and sends authenticated requests to the CRE gateway for testing your deployed workflows

## Quick comparison: Simulation vs Production

| Aspect             | Simulation                      | Production                                               |
| ------------------ | ------------------------------- | -------------------------------------------------------- |
| **Authorization**  | Optional (can use empty config) | Required (`authorizedKeys` must be configured)           |
| **Trigger method** | CLI with `--input` flag         | HTTP POST to gateway endpoint with JWT                   |
| **Endpoint**       | Local simulator                 | CRE gateway (`https://01.gateway.zone-a.cre.chain.link`) |
| **Use case**       | Development, testing, debugging | Live integrations, webhooks, production APIs             |

## Key concepts

### Authorization keys

Authorization keys are EVM addresses that are permitted to trigger your workflow. When you configure an HTTP trigger, you specify one or more `authorizedKeys`:

```typescript
authorizedKeys: [
  {
    type: "KEY_TYPE_ECDSA_EVM",
    publicKey: "0xYourEVMAddress",
  },
]
```

Only requests signed by the corresponding private keys will be accepted by the CRE gateway.

### Payload

The [HTTP trigger payload](/cre/reference/sdk/triggers/http-trigger-ts#payload) contains:

- **`input`**: The JSON data from the HTTP request body
- **`key`**: The authorized key that signed the request

Your callback function receives this payload and can process the input data to perform workflow logic.

## Next steps

Start by learning how to configure HTTP triggers and write handler functions:

- **[Configuration & Handler](/cre/guides/workflow/using-triggers/http-trigger/configuration)** - Set up your first HTTP trigger

Or explore the SDK reference for detailed API documentation:

- **[HTTP Trigger SDK Reference](/cre/reference/sdk/triggers/http-trigger)** - Complete API documentation

---

# Project Setup Commands
Source: https://docs.chain.link/cre/reference/cli/project-setup-ts
Last Updated: 2025-11-04

The project setup command `cre init` initializes new CRE projects or adds workflows to existing projects.

<Aside type="note" title="Global flags">
  All `cre` commands support [global flags](/cre/reference/cli#global-flags) like `--env`, `--target`, `--project-root`,
  and `--verbose`.
</Aside>

## `cre init`

<Aside type="note" title="Authentication required">
  Running this command requires you to be logged in with the CRE CLI. Run `cre whoami` in your terminal to verify you're
  logged in, or run `cre login` to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging
  in with the CLI](/cre/account/cli-login) for further details.
</Aside>

Initializes a new CRE project or adds a workflow to an existing project. The behavior depends on your current directory:

- **In a directory without a project**: Creates a new project with the first workflow
- **In an existing project directory**: Adds a new workflow to the existing project

**Usage:**

```bash
cre init [flags]
```

**Flags:**

| Flag                  | Description                        |
| --------------------- | ---------------------------------- |
| `-p, --project-name`  | Name for the new project           |
| `-t, --template-id`   | ID of the workflow template to use |
| `-w, --workflow-name` | Name for the new workflow          |

**Interactive mode (recommended):**

Running `cre init` without flags starts an interactive setup that guides you through the process:

1. **Project name** (only if creating a new project)
2. **Language** (Go or TypeScript)
3. **Workflow template** (example templates for the chosen language)
4. **Workflow name**

**Example:**

```bash
# Interactive setup
cre init
```

**Non-interactive mode:**

```bash
# Create a new project with initial workflow
cre init \
  --project-name my-cre-project \
  --workflow-name my-workflow \
  --template-id 1
```

<Aside type="note" title="Interactive vs. non-interactive">
  For most users, running `cre init` without flags (interactive mode) is the easiest way to set up a project. The CLI
  will guide you through the setup process with clear prompts.
</Aside>

For a detailed walkthrough, see [Part 1 of the Getting Started guide](/cre/getting-started/part-1-project-setup).

## Project initialization workflow

The typical project setup flow for TypeScript workflows:

1. **`cre init`** — Create a new project or add a workflow (interactive or with flags)
2. **Navigate to workflow directory** — `cd your-project/your-workflow`
3. **Install dependencies** — Run `bun install` to install the CRE SDK and dependencies
4. **Start development** — Write your workflow code

## Learn more

- [Part 1: Project Setup](/cre/getting-started/part-1-project-setup) — Step-by-step tutorial for initializing projects
- [Project Configuration](/cre/reference/project-configuration) — Understanding `project.yaml` and `workflow.yaml`

---

# Project Configuration
Source: https://docs.chain.link/cre/reference/project-configuration-ts
Last Updated: 2025-11-04

This page explains how to manage configuration within Chainlink Runtime Environment (CRE) projects. It covers the standard project structure, the roles and usage of the configuration files (`project.yaml` and `workflow.yaml`), and the concept of **targets** for handling environment-specific settings.

You will understand:

- The recommended directory structure for CRE projects and the significance of key files.
- How to define global settings in `project.yaml` and override them with workflow-specific settings in `workflow.yaml`.
- The purpose of targets and how they enable seamless switching between different operational environments.
- The process by which the CRE CLI resolves and merges target configurations.

## 1. Glossary

| Term           | Definition                                                                                                                                                                                         |
| -------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Project**    | A folder that groups one or more workflows plus shared files such as `project.yaml`, `.env`, and `.gitignore`.                                                                                     |
| **Workflow**   | A sub-folder that contains everything needed to run *one* workflow (code, dependencies, build artifacts, and the optional `workflow.yaml`).                                                        |
| **Config**     | Any YAML file that lets you adjust how a workflow behaves (timeouts, trigger frequency, etc.).                                                                                                     |
| **Settings**   | Values that describe the runtime environment. Project settings live in `project.yaml`; workflow settings live in `workflow.yaml`. If both define the same key, the workflow value wins.            |
| **Target**     | A named set of settings (e.g. `staging-settings`, `production-settings`). Switching the target switches networks, RPC URLs, contract addresses, and other environment-specific values in one step. |
| `secrets.yaml` | A project-level file that defines logical names for secrets and maps them to environment variables.                                                                                                |
| **ABI**        | Application Binary Interface - describes a smart contract's functions, events, and data structures. In TypeScript, ABIs are defined directly as TypeScript files using viem.                       |

## 2. Project structure

A typical CRE TypeScript project is organized as follows:

```text
myProject/
├── .env                    # Secret values (never commit to a Version Control System like Git)
├── .gitignore
├── project.yaml            # Global configuration
├── secrets.yaml            # Secret name declarations
├── contracts/              # Contract-related files
│   └── abi/                # TypeScript ABI definitions (.ts files)
│       ├── MessageEmitter.ts
│       ├── index.ts
│       └── …
├── workflow1/
│   ├── package.json        # NPM dependencies for this workflow
│   ├── tsconfig.json       # TypeScript configuration
│   ├── bun.lock            # Dependency lock file
│   ├── node_modules/       # Installed dependencies
│   ├── workflow.yaml       # Workflow-specific configuration (optional)
│   ├── main.ts             # Your workflow code
│   └── …
├── workflow2/
│   ├── package.json        # NPM dependencies for this workflow
│   ├── tsconfig.json       # TypeScript configuration
│   ├── bun.lock            # Dependency lock file
│   ├── node_modules/       # Installed dependencies
│   ├── workflow.yaml       # Workflow-specific configuration (optional)
│   ├── main.ts             # Your workflow code
│   └── …
└── …
```

- `project.yaml`: **Global settings**, shared by every workflow in the project.
- `workflow.yaml`: **Local settings** for a single workflow. Add this file only when the workflow needs overrides.
- `secrets.yaml`: **Secret declarations**, a manifest of logical secret names used across the project.
- `contracts/abi/`: **TypeScript ABI definitions**, where you place `.ts` files that export contract ABIs using viem's type system. Unlike Go, TypeScript doesn't require generating bindings—ABIs are used directly with viem.
- `package.json` / `tsconfig.json`: **Workflow-specific dependencies and TypeScript configuration**. Each workflow manages its own dependencies independently.
- `node_modules/`: **Installed dependencies** for the workflow (generated by `npm install` or similar).

## 3. Configuration files

### 3.1. Global configuration (`project.yaml`)

`project.yaml` holds everything that rarely changes across workflows, such as RPC endpoints.

```yaml
# project.yaml
staging-settings:
  rpcs:
    - chain-name: ethereum-testnet-sepolia
      url: https://ethereum-sepolia-rpc.publicnode.com

# You can define other targets for future use
production-settings:
  account:
    workflow-owner-address: "0x..." # Optional: For multi-sig wallets
  rpcs:
    - chain-name: ethereum-testnet-sepolia
      url: https://ethereum-sepolia-rpc.publicnode.com
    - chain-name: ethereum-mainnet
      url: https://mainnet.infura.io/v3/<YOUR-PROJECT-ID>
```

<Aside type="note" title="Available chain names">
  For a complete list of supported networks and their chain names, see [Supported
  Networks](/cre/guides/workflow/using-evm-client/supported-networks). For details on using chain selectors in your
  workflow code, see [Chain Selectors](/cre/reference/sdk/evm-client-ts#chain-selectors).
</Aside>

#### Configuration fields

| Field                            | Required | When to use           | Description                                                                                                                                                                                                                                                                                                                |
| -------------------------------- | -------- | --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `account.workflow-owner-address` | No       | Multi-sig only        | Multi-sig wallet address that owns the workflow. Required when using the `--unsigned` flag to generate unsigned transactions. For standard (non-multi-sig) deployments, omit this field—the CLI uses the address from `CRE_ETH_PRIVATE_KEY`. See [Using Multi-sig Wallets](/cre/guides/operations/using-multisig-wallets). |
| `rpcs`                           | Yes      | Always (if using EVM) | Array of RPC endpoints for chains your workflow interacts with. Each entry requires `chain-name` (e.g., `ethereum-mainnet`) and `url` (the RPC endpoint). **Required for both simulation and deployed workflows** that use EVM capabilities. The CLI pre-populates a public Sepolia RPC URL by default.                    |

<Aside type="caution" title="Missing RPC causes simulation failure">
  If your workflow uses EVM capabilities, you must configure RPC endpoints. Without them, the simulator cannot register
  the EVM capability and your workflow will fail.
</Aside>

### 3.2. Workflow configuration (`workflow.yaml`)

`workflow.yaml` captures details **unique to one workflow instance**, like its name and entry point file.

```yaml
# workflow.yaml
staging-settings:
  user-workflow:
    workflow-name: "my-por-workflow-staging"
  workflow-artifacts:
    workflow-path: "./main.ts" # Points to the TypeScript entry file
    config-path: "./config.staging.json"
    secrets-path: "" # Empty if not using secrets

production-settings:
  user-workflow:
    workflow-owner-address: "<address>" # Optional: For multi-sig wallets
    workflow-name: "my-por-workflow-production"
  workflow-artifacts:
    workflow-path: "./main.ts" # Points to the TypeScript entry file
    config-path: "./config.production.json"
    secrets-path: "" # e.g. "../secrets.yaml" if using secrets
```

#### Configuration fields

| Field                                  | Required | Description                                                                                                                                                                                                     |
| -------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `user-workflow.workflow-name`          | Yes      | The name of your workflow. This name is used to identify the workflow when deploying, activating, or managing it via CLI commands. Best practice: include environment suffix (e.g., `-staging`, `-production`). |
| `user-workflow.workflow-owner-address` | No       | Multi-sig wallet address (if applicable). Overrides the value from `project.yaml` for this specific workflow. See the `project.yaml` configuration table above for details.                                     |
| `workflow-artifacts.workflow-path`     | Yes      | Path to your workflow entry point. For TypeScript: `"./main.ts"` (or your entry file name). For Go: `"."` (current directory).                                                                                  |
| `workflow-artifacts.config-path`       | Yes      | Path to your workflow configuration JSON file (e.g., `"./config.staging.json"` or `"./config.production.json"`).                                                                                                |
| `workflow-artifacts.secrets-path`      | No       | Path to your secrets YAML file (e.g., `"../secrets.yaml"`). Use `""` (empty string) if not using secrets. See [Secrets Management](/cre/guides/workflow/secrets) for details.                                   |

<Aside type="note" title="When is workflow.yaml required?">
  Every workflow directory must have a `workflow.yaml` file that defines `workflow-name` and `workflow-artifacts`. You
  can optionally override project-level settings (like `workflow-owner-address`) for specific workflows.
</Aside>

<Aside type="note" title="TypeScript workflow-path">
  For TypeScript workflows, `workflow-path` must point to the specific TypeScript entry file (e.g., `"./main.ts"`),
  whereas Go workflows use `"."` to point to the workflow directory.
</Aside>

### 3.3. Secrets configuration (`secrets.yaml`)

`secrets.yaml` is an optional project-level file that maps logical secret names to environment variables. This allows you to decouple the secret names used in your code from the actual environment variable names.

```yaml
# secrets.yaml (at project root)
secretsNames:
  DATA_SOURCE_API_KEY:
    - DATA_SOURCE_API_KEY_ENV
```

To use secrets in your workflow, reference this file in your `workflow.yaml`:

```yaml
workflow-artifacts:
  secrets-path: "../secrets.yaml"
```

For simulation, secret values are loaded from your `.env` file or environment variables. For deployed workflows, secrets are managed through the Vault DON using `cre secrets` commands.

For complete details on using secrets in your workflows, see [Secrets Management](/cre/guides/workflow/secrets).

## 4. Targets

A target is a top-level key inside both `project.yaml` and `workflow.yaml`. It bundles all settings for a single environment or variant.

The CLI selects the active target using the `--target` flag. Example:

```sh
# Simulate using the 'staging-settings' target
cre workflow simulate my-workflow --target staging-settings
```

Alternatively, the `CRE_TARGET` environment variable can be used to specify the target. The CLI picks the active target in this order:

1. `--target <name>` flag
2. `CRE_TARGET=<name>` environment variable

### 4.1. Defining Multiple Targets

You can store many targets in one file. This is useful for managing different environments (like simulation vs. production) or for testing variations of a workflow.

```yaml
# In project.yaml

# Target for simulation and testing
staging-settings:
  rpcs:
    - chain-name: ethereum-testnet-sepolia
      url: https://ethereum-sepolia-rpc.publicnode.com

# Target for production deployment
production-settings:
  account:
    workflow-owner-address: "0x123..." # Optional: For multi-sig wallets
  rpcs:
    - chain-name: ethereum-mainnet
      url: https://mainnet.infura.io/v3/<YOUR-PROJECT-ID>
```

### 4.2. How Target Resolution Works

When you run a CLI command with a target, e.g., `--target staging-settings`:

1. Load the `staging-settings` target from `project.yaml`.
2. Load the same target from `workflow.yaml` (if the file exists and contains a matching target definition).
3. Merge the two objects.
   - Keys present in both files → value from `workflow.yaml` wins.
   - Keys present in only one file → that value is used.
4. Validate the final object before execution.

<Aside type="caution" title="Matching target names">
  If your workflow directory contains a `workflow.yaml`, the target you specify must exist in both `project.yaml` and
  `workflow.yaml`. A mismatch causes the CLI to abort with a clear error message.
</Aside>

---

# SDK Reference: Consensus & Aggregation
Source: https://docs.chain.link/cre/reference/sdk/consensus-ts
Last Updated: 2025-11-04

Aggregation is the process of taking many results from individual nodes and reducing them to a single, reliable value. This aggregated value is what the DON reaches consensus on. When you run code on individual nodes using [`runtime.runInNodeMode()`](/cre/reference/sdk/core-ts/#runtimeruninnodemode), you must provide an aggregation strategy to tell the DON how to produce this single, trustworthy outcome. This is achieved using a `ConsensusAggregation`.

## `ConsensusAggregation<T, U>`

This is a generic type passed as the second argument to `runtime.runInNodeMode()`. It defines the aggregation strategy and an optional default value to be used if the node-level execution fails.

There are two primary ways to specify an aggregation method:

1. [**Using built-in functions**](/cre/reference/sdk/consensus-ts#1-built-in-aggregation-functions): For simple types, use functions like [`consensusMedianAggregation()`](#consensusmedianaggregationt).
2. [**Using field-based aggregation**](/cre/reference/sdk/consensus-ts#2-field-based-aggregation-for-objects): For complex types (objects), use [`ConsensusAggregationByFields()`](#consensusaggregationbyfieldstfields).

## 1. Built-in aggregation functions

These functions are used for simple, single-value aggregations.

### `consensusMedianAggregation<T>()`

Computes the median of numeric results from all nodes.

**Supported Types (`T`):**
`number`, `bigint`, `Date`

**Usage:**

```typescript
import { consensusMedianAggregation, type Runtime, type NodeRuntime } from "@chainlink/cre-sdk"

const fetchPrice = (nodeRuntime: NodeRuntime<Config>): bigint => {
  // Fetch price from API
  return 100n
}

const onTrigger = (runtime: Runtime<Config>): string => {
  const price = runtime.runInNodeMode(fetchPrice, consensusMedianAggregation<bigint>())().result()

  runtime.log(`Median price: ${price}`)
  return "Success"
}
```

### `consensusIdenticalAggregation<T>()`

Ensures that a sufficient majority of nodes (a Byzantine Quorum) return the exact same value.

**Supported Types (`T`):**
Any primitive TypeScript type (`string`, `boolean`, `number`, `bigint`), or objects composed entirely of these types.

**Usage:**

```typescript
import { consensusIdenticalAggregation, type Runtime, type NodeRuntime } from "@chainlink/cre-sdk"

const fetchBlockHash = (nodeRuntime: NodeRuntime<Config>): string => {
  // Fetch block hash from RPC
  return "0xabc123..."
}

const onTrigger = (runtime: Runtime<Config>): string => {
  const blockHash = runtime.runInNodeMode(fetchBlockHash, consensusIdenticalAggregation<string>())().result()

  runtime.log(`Block hash: ${blockHash}`)
  return "Success"
}
```

### `consensusCommonPrefixAggregation<T>()`

Computes the longest common prefix from an array of values from all nodes. This is useful for finding the longest shared sequence at the beginning of a list.

**Supported Types (`T`):**
Any array of a type supported by `consensusIdenticalAggregation` (e.g., `string[]`, `number[]`).

**Usage:**

```typescript
import { consensusCommonPrefixAggregation, type Runtime, type NodeRuntime } from "@chainlink/cre-sdk"

const fetchBlockHeaders = (nodeRuntime: NodeRuntime<Config>): string[] => {
  // Fetch block headers for a chain fork
  return ["0xabc...", "0xdef...", "0x123..."]
}

const onTrigger = (runtime: Runtime<Config>): string => {
  const headers = runtime.runInNodeMode(fetchBlockHeaders, consensusCommonPrefixAggregation<string>())().result()

  runtime.log(`Common prefix length: ${headers.length}`)
  return "Success"
}
```

### `consensusCommonSuffixAggregation<T>()`

Computes the longest common suffix from an array of values from all nodes. This is useful for finding the longest shared sequence at the end of a list.

**Supported Types (`T`):**
Any array of a type supported by `consensusIdenticalAggregation` (e.g., `string[]`, `number[]`).

**Usage:**

```typescript
import { consensusCommonSuffixAggregation, type Runtime, type NodeRuntime } from "@chainlink/cre-sdk"

const fetchRecentTransactions = (nodeRuntime: NodeRuntime<Config>): string[] => {
  // Fetch recent transaction IDs
  return ["0x111...", "0x222...", "0x333..."]
}

const onTrigger = (runtime: Runtime<Config>): string => {
  const recentTxs = runtime
    .runInNodeMode(fetchRecentTransactions, consensusCommonSuffixAggregation<string>())()
    .result()

  runtime.log(`Common suffix length: ${recentTxs.length}`)
  return "Success"
}
```

## 2. Field-based aggregation for objects

For objects with multiple fields, the recommended approach is to use `ConsensusAggregationByFields`. This function allows you to specify different aggregation strategies for each field of your return type.

### `ConsensusAggregationByFields<T>(fields)`

Creates a consensus aggregation strategy by specifying how to aggregate each field of an object.

**Parameters:**

- `fields`: An object where each key corresponds to a field in your type `T`, and each value is a field aggregation function

**Field Aggregation Functions:**

| Function         | Description                                                 | Compatible Field Types          |
| ---------------- | ----------------------------------------------------------- | ------------------------------- |
| `median()`       | Computes the median of the field's value across all nodes   | `number`, `bigint`, `Date`      |
| `identical()`    | Ensures the field's value is identical across all nodes     | Primitives, objects             |
| `commonPrefix()` | Finds the longest common prefix for an array from all nodes | Arrays (`string[]`, `number[]`) |
| `commonSuffix()` | Finds the longest common suffix for an array from all nodes | Arrays                          |
| `ignore()`       | This field will be ignored during consensus                 | Any                             |

**Usage:**

```typescript
import {
  ConsensusAggregationByFields,
  median,
  identical,
  type Runtime,
  type HTTPSendRequester,
} from "@chainlink/cre-sdk"

type ReserveInfo = {
  lastUpdated: Date
  totalReserve: number
  source: string
}

const fetchReserveData = (sendRequester: HTTPSendRequester, config: Config): ReserveInfo => {
  const response = sendRequester.sendRequest({ url: config.apiUrl }).result()
  const data = JSON.parse(response.body.toString())

  return {
    lastUpdated: new Date(data.timestamp * 1000),
    totalReserve: data.reserve,
    source: data.source,
  }
}

const onTrigger = (runtime: Runtime<Config>): string => {
  const httpClient = new cre.capabilities.HTTPClient()

  const reserveInfo = httpClient
    .sendRequest(
      runtime,
      fetchReserveData,
      ConsensusAggregationByFields<ReserveInfo>({
        lastUpdated: median, // Use median for timestamp
        totalReserve: median, // Use median for reserve amount
        source: identical, // Ensure source is identical across nodes
      })
    )(runtime.config)
    .result()

  runtime.log(`Reserve: ${reserveInfo.totalReserve}`)
  return "Success"
}
```

<Aside type="note" title="Custom Data Feed Demo Example">
  The [Custom Data Feed demo](/cre/templates/running-demo-workflow-ts) uses `ConsensusAggregationByFields` with the
  `median` function to fetch and aggregate data from multiple nodes. This ensures the final value is reliable even if
  some nodes return slightly different results.
</Aside>

## Complete Example

Here's a complete example demonstrating both simple and field-based aggregation:

```typescript
import {
  cre,
  Runner,
  consensusMedianAggregation,
  ConsensusAggregationByFields,
  median,
  identical,
  type Runtime,
  type NodeRuntime,
  type HTTPSendRequester,
  type CronPayload,
} from "@chainlink/cre-sdk"

type Config = {
  apiUrl: string
}

type PriceData = {
  price: bigint
  timestamp: bigint
  source: string
}

// Simple aggregation example
const fetchSimplePrice = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()
  const response = httpClient.sendRequest(nodeRuntime, { url: nodeRuntime.config.apiUrl }).result()
  const data = JSON.parse(response.body.toString())
  return BigInt(data.price)
}

// Field-based aggregation example
const fetchPriceData = (sendRequester: HTTPSendRequester, config: Config): PriceData => {
  const response = sendRequester.sendRequest({ url: config.apiUrl }).result()
  const data = JSON.parse(response.body.toString())

  return {
    price: BigInt(data.price),
    timestamp: BigInt(data.timestamp),
    source: data.source,
  }
}

const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  // Example 1: Simple median aggregation
  const simplePrice = runtime.runInNodeMode(fetchSimplePrice, consensusMedianAggregation<bigint>())().result()

  runtime.log(`Simple median price: ${simplePrice}`)

  // Example 2: Field-based aggregation
  const httpClient = new cre.capabilities.HTTPClient()
  const priceData = httpClient
    .sendRequest(
      runtime,
      fetchPriceData,
      ConsensusAggregationByFields<PriceData>({
        price: median, // Median of price values
        timestamp: median, // Median of timestamps
        source: identical, // Must be identical across nodes
      })
    )(runtime.config)
    .result()

  runtime.log(`Aggregated price: ${priceData.price} from ${priceData.source}`)

  return "Price data aggregated successfully"
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: "0 */5 * * * *" }), onCronTrigger)]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Default Values

You can specify a default value to be used if node-level execution fails:

```typescript
const price = runtime.runInNodeMode(fetchPrice, consensusMedianAggregation<bigint>().withDefault(0n))().result()
```

If all nodes fail or consensus cannot be reached, the default value (`0n` in this example) will be used instead.

---

# SDK Reference: Core
Source: https://docs.chain.link/cre/reference/sdk/core-ts
Last Updated: 2025-11-04

This page provides a reference for the core data structures and functions of the CRE TypeScript SDK. These are the fundamental building blocks that every workflow uses, regardless of trigger types or capabilities.

## Key concepts and components

### `cre.handler()`

The `cre.handler()` function is the cornerstone of every workflow. It registers a handler that links a specific trigger to a callback function containing your workflow logic. It is typically called within your [`initWorkflow`](#initworkflow) function.

**Usage:**

```typescript
import { cre, type Runtime } from "@chainlink/cre-sdk"

const initWorkflow = (config: Config) => {
  return [
    cre.handler(
      // 1. A configured trigger, e.g., cron.trigger(...)
      // This determines WHEN the workflow runs
      triggerInstance,

      // 2. The callback function to execute when the trigger fires
      // This is WHERE your workflow logic lives
      myCallbackFunction
    ),
  ]
}
```

- **The Trigger**: An instance of a trigger capability (e.g., `cron.trigger(...)`). This defines the event that will start your workflow. See the [Triggers reference](/cre/reference/sdk/triggers) for details.
- **The Callback**: The function to be executed when the trigger fires. The signature of your callback function must match the output type of the trigger you are using.

<Aside type="note" title="Callback Function Signatures">
  Each trigger type requires a specific callback signature. See the [Triggers reference](/cre/reference/sdk/triggers)
  for the exact signature required for each trigger type.
</Aside>

### `Runtime` and `NodeRuntime`

These TypeScript interfaces provide access to capabilities and manage the execution context of your workflow. The key difference is who is responsible for creating a single, trusted result from the work of many nodes.

- **`Runtime<C>` ("DON Mode")**: Passed to your main trigger callback, this represents the **DON's (Decentralized Oracle Network) execution context**. It is used for operations that are already guaranteed to be Byzantine Fault Tolerant (BFT). When you use the `Runtime`, you ask the network to execute something, and CRE handles the underlying complexity to ensure you get back one final, secure, and trustworthy result. Common use cases include writing transactions to a blockchain with the EVM client or accessing secrets.

- **`NodeRuntime<C>` ("Node Mode")**: Represents an **individual node's execution context**. This is used when a BFT guarantee cannot be provided automatically (e.g., calling a third-party API). You tell each node to perform a task on its own, and each node returns its own individual answer. You are then responsible for telling the SDK how to combine them into a single, trusted result by providing a consensus and aggregation algorithm. It is used exclusively inside a [`runtime.runInNodeMode()`](#runtimeruninnodemode) block and is provided by that function—you do not receive this type directly in your handler's callback.

To learn more about how to aggregate results from `NodeRuntime`, see the [Consensus & Aggregation](/cre/reference/sdk/consensus) reference.

**Available Methods:**

Both `Runtime` and `NodeRuntime` provide:

- **`config`**: Access to your workflow's configuration
- **`now()`**: Returns the current `Date` object
- **`log(message: string)`**: Logs a message (accepts a single string argument)
- **`callCapability(...)`**: Internal method for calling capabilities (used by generated code)

`Runtime` additionally provides:

- **`runInNodeMode(...)`**: Execute code on individual nodes with consensus aggregation
- **`getSecret(...)`**: Access to workflow secrets
- **`report(...)`**: Generate cryptographically signed reports

### Understanding the `.result()` Pattern

All SDK capabilities in the TypeScript SDK use a two-step pattern for asynchronous operations:

**Step 1: Initiate the operation**

```typescript
const request = httpClient.sendRequest(runtime, { url: "https://api.example.com" })
```

**Step 2: Get the result**

```typescript
const response = request.result()
```

**Common usage:** These steps are often chained together for simplicity:

```typescript
import { cre, encodeCallMsg, LAST_FINALIZED_BLOCK_NUMBER, type Runtime } from "@chainlink/cre-sdk"
import { zeroAddress } from "viem"

const onCronTrigger = (runtime: Runtime<Config>): string => {
  const evmClient = new cre.capabilities.EVMClient(chainSelector)

  // Inline pattern: initiate and get result in one expression
  const contractCall = evmClient
    .callContract(runtime, {
      call: encodeCallMsg({
        from: zeroAddress,
        to: config.contractAddress,
        data: encodedCallData,
      }),
      blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
    })
    .result()

  return "Success"
}
```

#### Why this pattern exists

Traditional TypeScript `async/await` doesn't work with SDK capabilities in the WebAssembly environment where CRE workflows run. WASM execution is fundamentally synchronous—when you call a function, it runs to completion before anything else happens. The interaction between the WASM guest (your workflow) and the Go host (the CRE engine) uses simple, synchronous function calls.

The `.result()` pattern is a custom solution to this limitation. It simulates asynchronous behavior using a pair of synchronous calls:

1. The first call (e.g., `sendRequest()`) sends your request from the TypeScript code (compiled to WASM) to the CRE host
2. The `.result()` call blocks your WASM code and waits for the host to complete the async operation and return the response

This allows the host to handle I/O-bound tasks (like network requests) asynchronously without blocking the entire runtime, while providing a simple, blocking interface to your code inside the WASM module.

<Aside type="note" title="Using async/await in your own functions">
  While all SDK capabilities use the `.result()` pattern, you can still use `async/await` syntax in your own custom
  functions within your workflow if needed. However, since all external operations (HTTP requests, blockchain
  interactions, secrets access) must go through SDK capabilities, most workflow code will use the `.result()` pattern.
</Aside>

#### Preparing multiple operations

You can initiate multiple operations before calling `.result()` on any of them:

```typescript
// Initiate two operations
const request1 = httpClient.sendRequest(runtime, { url: "https://api1.example.com" })
const request2 = httpClient.sendRequest(runtime, { url: "https://api2.example.com" })

// Get results as needed
const response1 = request1.result()
const response2 = request2.result()
```

This pattern allows you to prepare operations and then collect their results in the order you need them.

#### Operations that use `.result()`

The `.result()` pattern applies to all SDK capabilities that perform asynchronous work:

- **HTTP requests**: `httpClient.sendRequest(...).result()`
- **EVM contract calls (read)**: `evmClient.callContract(...).result()`
- **EVM contract calls (write)**: `evmClient.writeReport(...).result()`
- **Secrets retrieval**: `runtime.getSecret(...).result()`
- **Node-level execution**: `runtime.runInNodeMode(...)().result()`
- **Report generation**: `runtime.report(...).result()`

## Workflow entry points

Your workflow code requires two specific functions to serve as entry points for compilation and execution.

### `main()`

This is the entry point of your workflow. You must define this async function to create a WASM runner and start your workflow.

**Required Pattern:**

```typescript
import { Runner } from "@chainlink/cre-sdk"
import { z } from "zod"

// Define your config schema with Zod
const configSchema = z.object({
  schedule: z.string(),
  apiUrl: z.string(),
})

type Config = z.infer<typeof configSchema>

export async function main() {
  // Create the runner with your config schema
  const runner = await Runner.newRunner<Config>({ configSchema })

  // Run your workflow initialization function
  await runner.run(initWorkflow)
}

main()
```

**Key points:**

- Must be an `async` function
- Must call `Runner.newRunner<Config>()` with an optional `configSchema` parameter for validation
- Must call `runner.run(initWorkflow)` to execute your workflow
- Must invoke `main()` at the end of your file

### `initWorkflow`

This is the second required entry point. The CRE runner calls this function to initialize your workflow and register all its handlers.

**Required Signature:**

```typescript
import { cre, type Runtime } from "@chainlink/cre-sdk"

function initWorkflow(config: Config): Array<HandlerEntry<Config, any, any, any>>
```

**Parameters:**

- `config`: Your workflow's configuration object (validated against your Zod schema if provided)

**Returns:**

- An array of handlers created with `cre.handler()`

<Aside type="note" title="Using secrets">
  If your workflow uses secrets, you can add a second parameter `secretsProvider: SecretsProvider` to access the
  `getSecret()` method. See the [Secrets guide](/cre/guides/workflow/secrets) for details.
</Aside>

**Example:**

```typescript
import { cre, type Runtime, type CronPayload } from "@chainlink/cre-sdk"

// Callback function executed by the handler
const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  runtime.log("Workflow triggered!")
  return "complete"
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}
```

## `runtime.runInNodeMode()`

As explained in the [`Runtime` and `NodeRuntime`](#runtime-and-noderuntime) section, this method is the bridge between the DON-level execution context (`Runtime`) and the individual node-level context (`NodeRuntime`). It allows you to execute code on individual nodes and then aggregate their results back into a single, trusted outcome.

**Signature:**

```typescript
runtime.runInNodeMode<TArgs extends unknown[], TOutput>(
  fn: (nodeRuntime: NodeRuntime<C>, ...args: TArgs) => TOutput,
  consensusAggregation: ConsensusAggregation<TOutput, true>,
  unwrapOptions?: UnwrapOptions<TOutput>
): (...args: TArgs) => { result: () => TOutput }
```

**Parameters:**

- `fn`: A function that receives a `NodeRuntime` and executes on each individual node
- `consensusAggregation`: An aggregation function (e.g., `consensusMedianAggregation<bigint>()`)
- `unwrapOptions`: Optional configuration for how to unwrap complex return types

**Returns:**

A function that, when called with any additional arguments, returns an object with a `.result()` method.

**Example:**

This example uses `runInNodeMode` to fetch data from an API on each node, and then uses the DON-level `Runtime` to write the aggregated result onchain.

```typescript
import {
  cre,
  consensusMedianAggregation,
  type Runtime,
  type NodeRuntime,
} from "@chainlink/cre-sdk"

const fetchPrice = (nodeRuntime: NodeRuntime<Config>): bigint => {
  const httpClient = new cre.capabilities.HTTPClient()
  // Fetch price from API using nodeRuntime
  return fetchOffchainPrice(nodeRuntime)
}

const onTrigger = (runtime: Runtime<Config>, ...): string => {
  // 1. Run code on individual nodes using runInNodeMode
  // The fetchPrice function receives a NodeRuntime
  const price = runtime
    .runInNodeMode(
      fetchPrice,
      consensusMedianAggregation<bigint>()
    )()
    .result()

  // 2. Now, back in the DON context, use the top-level runtime
  // to perform an action that requires consensus, like an onchain write
  const tx = evmClient
    .writeReport(runtime, { /* ... */ })
    .result()

  return "success"
}
```

<Aside type="note" title="Notice the double function call">
  The pattern `runInNodeMode(fn, aggregation)()` requires calling the returned function immediately with `()` before
  calling `.result()`. This is because `runInNodeMode` returns a function that can accept additional arguments.
</Aside>

---

# SDK Reference: EVM Client
Source: https://docs.chain.link/cre/reference/sdk/evm-client-ts
Last Updated: 2025-11-04

This page provides a reference for the `EVMClient`, the low-level tool for all interactions with EVM-compatible blockchains. The client includes a comprehensive set of read, write, and utility methods for building chain-aware workflows.

## Client instantiation

To use the client, you must instantiate it with the numeric `ChainSelector` ID for the blockchain you intend to interact with.

```typescript
import { cre, getNetwork } from "@chainlink/cre-sdk"

// Get network information by chain selector name
const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})

if (!network) {
  throw new Error("Network not found")
}

// Instantiate a client for Ethereum Sepolia
const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)
```

<Aside type="note" title="What is a Chain Selector?">
  A Chain Selector is a unique identifier for a blockchain network. It is used to select the correct blockchain client
  for the workflow. See the [Chain Selectors](#chain-selectors) section below for a complete list of supported networks
  and usage examples.
</Aside>

### Using `getNetwork()` helper

The `getNetwork()` utility function allows you to look up network information by chain selector name or numeric ID. This is the recommended approach as it provides type-safe access to chain metadata and validates network configurations.

```typescript
import { getNetwork } from "@chainlink/cre-sdk"

const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})

// Access network properties
console.log(network.chainSelector.selector) // 16015286601757825753n (bigint)
console.log(network.chainSelector.name) // "ethereum-testnet-sepolia"
console.log(network.family) // "evm"
```

## Read & query methods

These methods are used to read data from the blockchain without creating a transaction.

### `callContract()`

Executes a `view` or `pure` function on a smart contract. This method returns an object with a `.result()` method that blocks until the call completes.

**Signature:**

```typescript
callContract(
  runtime: Runtime<unknown>,
  input: CallContractRequest | CallContractRequestJson
): { result: () => CallContractReply }
```

#### `CallContractRequest` / `CallContractRequestJson`

This is the main input object for the `callContract()` method.

| Field         | Type          | Required | Description                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| ------------- | ------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `call`        | `CallMsgJson` | Yes      | Contains the actual details of the function call you want to make.                                                                                                                                                                                                                                                                                                                                                                                    |
| `blockNumber` | `BigIntJson`  | No       | The block number to query. Accepts:<br />• `LATEST_BLOCK_NUMBER` (default): the most recent block<br />• `LAST_FINALIZED_BLOCK_NUMBER`: a finalized block<br />• **`BigIntJson` object**: an explicit block height (see [Custom Block Depths](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths) for conversion details).<br /><br />See [Finality and Confidence Levels](/cre/concepts/finality-ts) for finality strategies. |

#### `CallMsg` / `CallMsgJson`

This struct contains the core details of your onchain call.

| Field  | Type     | Required | Description                                                                                                                                      |
| ------ | -------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |
| `from` | `string` | Yes      | The 20-byte address of the sender (hex string). For `view`/`pure` functions, use `zeroAddress` from viem as the sender doesn't typically matter. |
| `to`   | `string` | Yes      | The 20-byte address of the target contract (hex string).                                                                                         |
| `data` | `string` | Yes      | The ABI-encoded function call data (hex string), including the function selector and arguments.                                                  |

#### `CallContractReply`

This is the object returned by `.result()` when the `callContract()` method successfully completes.

| Field  | Type         | Description                                         |
| ------ | ------------ | --------------------------------------------------- |
| `data` | `Uint8Array` | The ABI-encoded data returned by the contract call. |

#### Usage example

```typescript
import { cre, getNetwork, encodeCallMsg, bytesToHex, LAST_FINALIZED_BLOCK_NUMBER } from "@chainlink/cre-sdk"
import { encodeFunctionData, decodeFunctionResult, zeroAddress } from "viem"
import { Storage } from "../contracts/abi"

// Get network and instantiate client
const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})
const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

// Encode the contract call data
const callData = encodeFunctionData({
  abi: Storage,
  functionName: "get",
})

// Call the contract
const contractCall = evmClient
  .callContract(runtime, {
    call: encodeCallMsg({
      from: zeroAddress, // Required by encodeCallMsg. For view/pure functions, the sender doesn't matter.
      to: "0x1234567890123456789012345678901234567890",
      data: callData,
    }),
    blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
  })
  .result()

// Decode the result
const value = decodeFunctionResult({
  abi: Storage,
  functionName: "get",
  data: bytesToHex(contractCall.data),
})
```

***

### `filterLogs()`

Queries historical event logs that match a specific set of filter criteria. This method returns an object with a `.result()` method that blocks until the query completes.

**Signature:**

```typescript
filterLogs(
  runtime: Runtime<unknown>,
  input: FilterLogsRequest | FilterLogsRequestJson
): { result: () => FilterLogsReply }
```

#### `FilterLogsRequest` / `FilterLogsRequestJson`

| Field         | Type              | Required | Description                                                                                  |
| ------------- | ----------------- | -------- | -------------------------------------------------------------------------------------------- |
| `filterQuery` | `FilterQueryJson` | Yes      | A struct defining the filters for the log query, such as block range, addresses, and topics. |

#### `FilterLogsReply`

| Field  | Type    | Description                                          |
| ------ | ------- | ---------------------------------------------------- |
| `logs` | `Log[]` | An array of log objects that match the filter query. |

#### `Log` type

| Field         | Type           | Description                                                  |
| ------------- | -------------- | ------------------------------------------------------------ |
| `address`     | `Uint8Array`   | The 20-byte address of the contract that emitted the log.    |
| `topics`      | `Uint8Array[]` | An array of indexed log fields (32-byte arrays).             |
| `data`        | `Uint8Array`   | Non-indexed log data.                                        |
| `blockNumber` | `bigint`       | The block number where the log was emitted (optional).       |
| `txHash`      | `Uint8Array`   | The 32-byte transaction hash.                                |
| `txIndex`     | `number`       | The index of the transaction within the block.               |
| `blockHash`   | `Uint8Array`   | The 32-byte block hash.                                      |
| `index`       | `number`       | The index of the log within the transaction.                 |
| `removed`     | `boolean`      | `true` if the log was removed due to a chain reorganization. |

***

### `balanceAt()`

Retrieves the native token balance for a specific account. This method returns an object with a `.result()` method that blocks until the balance is retrieved.

**Signature:**

```typescript
balanceAt(
  runtime: Runtime<unknown>,
  input: BalanceAtRequest | BalanceAtRequestJson
): { result: () => BalanceAtReply }
```

#### `BalanceAtRequest` / `BalanceAtRequestJson`

| Field         | Type         | Required | Description                                                                                                                                                                                                                                                                                                                       |
| ------------- | ------------ | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `account`     | `string`     | Yes      | The 20-byte address of the account to query (hex string).                                                                                                                                                                                                                                                                         |
| `blockNumber` | `BigIntJson` | No       | The block number to query. Accepts `LATEST_BLOCK_NUMBER` (default), `LAST_FINALIZED_BLOCK_NUMBER`, or a `BigIntJson` object for an explicit block height (see [Custom Block Depths](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths)). See [Finality and Confidence Levels](/cre/concepts/finality-ts). |

#### `BalanceAtReply`

| Field     | Type     | Description                        |
| --------- | -------- | ---------------------------------- |
| `balance` | `bigint` | The balance of the account in Wei. |

***

### `estimateGas()`

Estimates the gas required to execute a specific transaction. This method returns an object with a `.result()` method that blocks until the estimation completes.

**Signature:**

```typescript
estimateGas(
  runtime: Runtime<unknown>,
  input: EstimateGasRequest | EstimateGasRequestJson
): { result: () => EstimateGasReply }
```

#### `EstimateGasRequest` / `EstimateGasRequestJson`

| Field | Type          | Required | Description                                             |
| ----- | ------------- | -------- | ------------------------------------------------------- |
| `msg` | `CallMsgJson` | Yes      | The transaction message to simulate for gas estimation. |

#### `EstimateGasReply`

| Field | Type     | Description                               |
| ----- | -------- | ----------------------------------------- |
| `gas` | `bigint` | The estimated amount of gas in gas units. |

***

### `getTransactionByHash()`

Retrieves a transaction by its hash. This method returns an object with a `.result()` method that blocks until the transaction is retrieved.

**Signature:**

```typescript
getTransactionByHash(
  runtime: Runtime<unknown>,
  input: GetTransactionByHashRequest | GetTransactionByHashRequestJson
): { result: () => GetTransactionByHashReply }
```

#### `GetTransactionByHashRequest` / `GetTransactionByHashRequestJson`

| Field  | Type     | Required | Description                                                  |
| ------ | -------- | -------- | ------------------------------------------------------------ |
| `hash` | `string` | Yes      | The 32-byte hash of the transaction to look up (hex string). |

#### `GetTransactionByHashReply`

| Field         | Type          | Description                       |
| ------------- | ------------- | --------------------------------- |
| `transaction` | `Transaction` | The transaction object, if found. |

#### `Transaction` type

| Field      | Type         | Description                           |
| ---------- | ------------ | ------------------------------------- |
| `nonce`    | `bigint`     | The nonce of the transaction.         |
| `gas`      | `bigint`     | The gas limit.                        |
| `to`       | `Uint8Array` | The 20-byte address of the recipient. |
| `data`     | `Uint8Array` | The transaction data payload.         |
| `hash`     | `Uint8Array` | The 32-byte transaction hash.         |
| `value`    | `bigint`     | The value transferred in Wei.         |
| `gasPrice` | `bigint`     | The gas price in Wei.                 |

***

### `getTransactionReceipt()`

Fetches the receipt for a transaction given its hash. This method returns an object with a `.result()` method that blocks until the receipt is retrieved.

**Signature:**

```typescript
getTransactionReceipt(
  runtime: Runtime<unknown>,
  input: GetTransactionReceiptRequest | GetTransactionReceiptRequestJson
): { result: () => GetTransactionReceiptReply }
```

#### `GetTransactionReceiptRequest` / `GetTransactionReceiptRequestJson`

| Field  | Type     | Required | Description                                       |
| ------ | -------- | -------- | ------------------------------------------------- |
| `hash` | `string` | Yes      | The 32-byte hash of the transaction (hex string). |

#### `GetTransactionReceiptReply`

| Field     | Type      | Description                               |
| --------- | --------- | ----------------------------------------- |
| `receipt` | `Receipt` | The transaction receipt object, if found. |

#### `Receipt` type

| Field               | Type         | Description                                                     |
| ------------------- | ------------ | --------------------------------------------------------------- |
| `status`            | `bigint`     | Transaction status: `1` for success, `0` for failure.           |
| `gasUsed`           | `bigint`     | The amount of gas used by the transaction.                      |
| `txIndex`           | `bigint`     | The index of the transaction within the block.                  |
| `blockHash`         | `Uint8Array` | The 32-byte block hash.                                         |
| `logs`              | `Log[]`      | An array of log objects emitted by the transaction.             |
| `txHash`            | `Uint8Array` | The 32-byte transaction hash.                                   |
| `effectiveGasPrice` | `bigint`     | The actual gas price paid per gas unit (optional).              |
| `blockNumber`       | `bigint`     | The block number where the transaction was included (optional). |
| `contractAddress`   | `Uint8Array` | The address of the deployed contract (if applicable).           |

***

### `headerByNumber()`

Retrieves a block header by its number. This method returns an object with a `.result()` method that blocks until the header is retrieved.

**Signature:**

```typescript
headerByNumber(
  runtime: Runtime<unknown>,
  input: HeaderByNumberRequest | HeaderByNumberRequestJson
): { result: () => HeaderByNumberReply }
```

#### `HeaderByNumberRequest` / `HeaderByNumberRequestJson`

| Field         | Type         | Required | Description                                                                                                                                                                                                                                                                                                                                                           |
| ------------- | ------------ | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `blockNumber` | `BigIntJson` | No       | The number of the block to retrieve. Accepts `undefined` for `latest` (default), `LATEST_BLOCK_NUMBER`, `LAST_FINALIZED_BLOCK_NUMBER`, or a `BigIntJson` object for an explicit block height (see [Custom Block Depths](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths)). See [Finality and Confidence Levels](/cre/concepts/finality-ts). |

#### `HeaderByNumberReply`

| Field    | Type     | Description                        |
| -------- | -------- | ---------------------------------- |
| `header` | `Header` | The block header object, if found. |

#### `Header` type

| Field         | Type         | Description                           |
| ------------- | ------------ | ------------------------------------- |
| `timestamp`   | `bigint`     | The Unix timestamp of the block.      |
| `blockNumber` | `bigint`     | The block number (optional).          |
| `hash`        | `Uint8Array` | The 32-byte block hash.               |
| `parentHash`  | `Uint8Array` | The 32-byte hash of the parent block. |

***

## Write methods

### `writeReport()`

Executes a state-changing transaction by submitting a cryptographically signed report to a designated receiver contract. This is the primary method for writing data onchain in CRE workflows.

**Signature:**

```typescript
writeReport(
  runtime: Runtime<unknown>,
  input: WriteCreReportRequest | WriteCreReportRequestJson
): { result: () => WriteReportReply }
```

#### `WriteCreReportRequest` / `WriteCreReportRequestJson`

| Field       | Type            | Required | Description                                                        |
| ----------- | --------------- | -------- | ------------------------------------------------------------------ |
| `receiver`  | `string`        | Yes      | The 20-byte address of the receiver contract to call (hex string). |
| `report`    | `Report`        | Yes      | The report object generated by `runtime.report()`.                 |
| `gasConfig` | `GasConfigJson` | No       | Gas limit configuration for the transaction.                       |

#### `GasConfig` / `GasConfigJson`

| Field      | Type     | Required | Description                        |
| ---------- | -------- | -------- | ---------------------------------- |
| `gasLimit` | `string` | Yes      | The gas limit for the transaction. |

#### `WriteReportReply`

| Field                             | Type                              | Description                                                                                                                                                |
| --------------------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `txStatus`                        | `TxStatus`                        | The final status of the transaction: `TX_STATUS_SUCCESS`, `TX_STATUS_REVERTED`, or `TX_STATUS_FATAL`.                                                      |
| `receiverContractExecutionStatus` | `ReceiverContractExecutionStatus` | The status of the receiver contract's execution: `RECEIVER_CONTRACT_EXECUTION_STATUS_SUCCESS` or `RECEIVER_CONTRACT_EXECUTION_STATUS_REVERTED` (optional). |
| `txHash`                          | `Uint8Array`                      | The 32-byte transaction hash of the onchain submission (optional).                                                                                         |
| `transactionFee`                  | `bigint`                          | The total fee paid for the transaction in Wei (optional).                                                                                                  |
| `errorMessage`                    | `string`                          | An error message if the transaction failed (optional).                                                                                                     |

#### `TxStatus` enum

- `TX_STATUS_SUCCESS`: The transaction was successful.
- `TX_STATUS_REVERTED`: The transaction reverted.
- `TX_STATUS_FATAL`: A fatal error occurred.

#### Usage example

```typescript
import { cre, getNetwork, hexToBase64, bytesToHex } from "@chainlink/cre-sdk"
import { encodeFunctionData } from "viem"
import { ReserveManager } from "../contracts/abi"

// Encode the contract call data
const callData = encodeFunctionData({
  abi: ReserveManager,
  functionName: "updateReserves",
  args: [
    {
      totalMinted: 100n,
      totalReserve: 50n,
    },
  ],
})

// Generate a signed report
const reportResponse = runtime
  .report({
    encodedPayload: hexToBase64(callData),
    encoderName: "evm",
    signingAlgo: "ecdsa",
    hashingAlgo: "keccak256",
  })
  .result()

// Submit the report onchain
const writeResult = evmClient
  .writeReport(runtime, {
    receiver: "0x1234567890123456789012345678901234567890",
    report: reportResponse,
    gasConfig: {
      gasLimit: "1000000",
    },
  })
  .result()

runtime.log(`Transaction hash: ${bytesToHex(writeResult.txHash || new Uint8Array(32))}`)
```

***

## Log tracking methods

These methods manage stateful log tracking subscriptions.

### `registerLogTracking()`

Creates a persistent filter to track specific logs over time. This is a "fire-and-forget" operation that returns an empty result.

**Signature:**

```typescript
registerLogTracking(
  runtime: Runtime<unknown>,
  input: RegisterLogTrackingRequest | RegisterLogTrackingRequestJson
): { result: () => Empty }
```

#### `RegisterLogTrackingRequest` / `RegisterLogTrackingRequestJson`

| Field    | Type           | Required | Description                                                                                |
| -------- | -------------- | -------- | ------------------------------------------------------------------------------------------ |
| `filter` | `LPFilterJson` | Yes      | A struct defining the persistent log filter, including rate limits and retention policies. |

***

### `unregisterLogTracking()`

Removes a previously registered log tracking filter. This is a "fire-and-forget" operation that returns an empty result.

**Signature:**

```typescript
unregisterLogTracking(
  runtime: Runtime<unknown>,
  input: UnregisterLogTrackingRequest | UnregisterLogTrackingRequestJson
): { result: () => Empty }
```

#### `UnregisterLogTrackingRequest` / `UnregisterLogTrackingRequestJson`

| Field        | Type     | Description                              |
| ------------ | -------- | ---------------------------------------- |
| `filterName` | `string` | The unique name of the filter to remove. |

***

## Helper functions

The TypeScript SDK provides several helper functions for working with the EVM Client.

### `encodeCallMsg()`

Encodes a call message payload into a `CallMsgJson`, expected by the EVM capability.

**Signature:**

```typescript
function encodeCallMsg(payload: EncodeCallMsgPayload): CallMsgJson
```

**Parameters:**

```typescript
interface EncodeCallMsgPayload {
  from: Address // viem Address type (0x-prefixed hex string)
  to: Address
  data: Hex // viem Hex type (0x-prefixed hex string)
}
```

**Usage:**

```typescript
import { encodeCallMsg } from "@chainlink/cre-sdk"
import { zeroAddress } from "viem"

const callMsg = encodeCallMsg({
  from: zeroAddress,
  to: "0x1234567890123456789012345678901234567890",
  data: "0xabcdef",
})
```

***

### `bytesToHex()`

Converts a `Uint8Array` to a `0x`-prefixed hex string.

**Signature:**

```typescript
function bytesToHex(bytes: Uint8Array): Hex
```

***

### `hexToBase64()`

Converts a `0x`-prefixed hex string to a base64-encoded string. This is useful for preparing data for protobuf structures.

**Signature:**

```typescript
function hexToBase64(hex: Hex): string
```

***

### `protoBigIntToBigint()`

Converts a protobuf `BigInt` (returned by SDK methods like `headerByNumber`) to a native JavaScript `bigint`. Use this when you need to perform arithmetic on block numbers or other numeric values returned from the blockchain.

**Signature:**

```typescript
function protoBigIntToBigint(pb: ProtoBigInt): bigint
```

**Parameters:**

- `pb`: A protobuf `BigInt` object with `absVal` (Uint8Array) and `sign` (bigint) fields

**Returns:**

A native JavaScript `bigint` value.

**Usage:**

```typescript
import { protoBigIntToBigint } from "@chainlink/cre-sdk"

// Get the latest block number from the blockchain
const latestHeader = evmClient.headerByNumber(runtime, {}).result()

// Convert the protobuf BigInt to a native bigint for arithmetic
const latestBlockNum = protoBigIntToBigint(latestHeader.header.blockNumber)
const customBlock = latestBlockNum - 500n // Now you can do arithmetic

console.log(`Latest block: ${latestBlockNum}`)
console.log(`Custom block (500 blocks ago): ${customBlock}`)
```

See [Custom Block Depths](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths) for a complete example.

***

### `blockNumber()`

Converts a native `bigint`, `number`, or `string` to the protobuf `BigInt` JSON format required by SDK methods. This is a convenience alias for `bigintToProtoBigInt`. Use this when specifying an explicit block height for contract calls or other blockchain queries.

**Signature:**

```typescript
function blockNumber(n: number | bigint | string): BigIntJson
```

**Parameters:**

- `n`: The block number as a native `bigint`, `number`, or `string`

**Returns:**

A `BigIntJson` object in the protobuf format expected by SDK methods.

**Usage:**

```typescript
import { blockNumber, encodeCallMsg } from "@chainlink/cre-sdk"
import { zeroAddress } from "viem"

// Read from a specific historical block
const historicalBlock = 9767655n
const contractCall = evmClient
  .callContract(runtime, {
    call: encodeCallMsg({
      from: zeroAddress,
      to: contractAddress,
      data: callData,
    }),
    blockNumber: blockNumber(historicalBlock),
  })
  .result()
```

See [Custom Block Depths](/cre/guides/workflow/using-evm-client/onchain-read-ts#custom-block-depths) for a complete example.

***

### `prepareReportRequest()`

Prepares a report request with default EVM encoding parameters for use with `runtime.report()`. This helper simplifies report generation by automatically setting the standard encoding configuration (`evm`, `ecdsa`, `keccak256`) required for EVM-based workflows.

**Signature:**

```typescript
function prepareReportRequest(
  hexEncodedPayload: Hex,
  reportEncoder?: Exclude<ReportRequestJson, "encodedPayload">
): ReportRequestJson
```

**Parameters:**

- `hexEncodedPayload`: The hex-encoded payload (typically from `encodeFunctionData()`) to be signed by the DON
- `reportEncoder`: Optional. Custom report encoder configuration. Defaults to `EVM_DEFAULT_REPORT_ENCODER` (`{ encoderName: 'evm', signingAlgo: 'ecdsa', hashingAlgo: 'keccak256' }`)

**Returns:**

A `ReportRequestJson` object ready to pass to `runtime.report()`.

**Usage:**

```typescript
import { prepareReportRequest, type Runtime } from "@chainlink/cre-sdk"
import { encodeFunctionData } from "viem"
import { MyContractABI } from "./abi"

// Encode the function call data
const callData = encodeFunctionData({
  abi: MyContractABI,
  functionName: "updateValue",
  args: [42n],
})

// Generate a signed report using the helper (simplest approach)
const report = runtime.report(prepareReportRequest(callData)).result()

// The helper automatically sets:
// - encodedPayload: hexToBase64(callData)
// - encoderName: "evm"
// - signingAlgo: "ecdsa"
// - hashingAlgo: "keccak256"
```

**Equivalent manual approach:**

```typescript
import { hexToBase64, type Runtime } from "@chainlink/cre-sdk"

// Without the helper, you must specify all parameters manually
const report = runtime
  .report({
    encodedPayload: hexToBase64(callData),
    encoderName: "evm",
    signingAlgo: "ecdsa",
    hashingAlgo: "keccak256",
  })
  .result()
```


<Aside type="tip" title="When to use this helper">
  Use `prepareReportRequest()` for all EVM write operations to reduce boilerplate and ensure consistent encoding parameters. It's particularly useful in workflows with multiple write operations, as it eliminates repetitive configuration.
</Aside>

***

### `LAST_FINALIZED_BLOCK_NUMBER`

A constant representing the last finalized block number for use in `callContract()` and similar methods.

```typescript
const LAST_FINALIZED_BLOCK_NUMBER = {
  absVal: Buffer.from([3]).toString("base64"), // 3 for finalized block
  sign: "-1",
}
```

**Usage:**

```typescript
import { LAST_FINALIZED_BLOCK_NUMBER } from "@chainlink/cre-sdk"

const contractCall = evmClient
  .callContract(runtime, {
    call: encodeCallMsg({ from: zeroAddress, to: contractAddress, data: callData }),
    blockNumber: LAST_FINALIZED_BLOCK_NUMBER,
  })
  .result()
```

***

### `LATEST_BLOCK_NUMBER`

A constant representing the latest mined block number.

```typescript
const LATEST_BLOCK_NUMBER = {
  absVal: Buffer.from([2]).toString("base64"), // 2 for the latest block
  sign: "-1",
}
```

## Chain Selectors

A **chain selector** is a unique identifier for a blockchain network used throughout the CRE platform. In TypeScript, you work with chain selectors primarily through the `getNetwork()` helper function.

<Aside type="note" title="Chain Selector lookup">
  The `getNetwork()` function provides a type-safe way to look up network information by chain selector name. This
  returns an object containing the numeric chain selector (as a `bigint`) along with other network metadata.
</Aside>

### Using `getNetwork()`

The recommended way to work with chain selectors in TypeScript:

```typescript
import { getNetwork } from "@chainlink/cre-sdk"

const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})

if (!network) {
  throw new Error("Network not found")
}

// Access the numeric chain selector (bigint)
const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)
// network.chainSelector.selector is 16015286601757825753n (bigint)
```

### Supported networks

The `EVMClient` class includes a static `SUPPORTED_CHAINS` constant with all available chain selectors:

```typescript
cre.capabilities.EVMClient.SUPPORTED_CHAINS
// Returns:
// {
//   'avalanche-mainnet': 6433500567565415381n,
//   'avalanche-testnet-fuji': 14767482510784806043n,
//   'ethereum-testnet-sepolia': 16015286601757825753n,
//   // ... and more
// }
```

### Chain selector reference

This table shows the chain selector names and their numeric IDs. In your configuration files (`project.yaml`, `config.staging.json`, `config.production.json`, etc.), you always use the **String Name**. The numeric ID is used internally by the SDK and is returned by the `getNetwork()` helper function.

<Aside type="note" title="Related resources">
  - **Forwarder addresses**: For network-specific forwarder contract addresses, see [Supported
    Networks](/cre/guides/workflow/using-evm-client/supported-networks) - **RPC configuration**: For configuring RPC
    endpoints in `project.yaml`, see [Project Configuration](/cre/reference/project-configuration)
</Aside>

| Chain             | String Name                         | Numeric ID           |
| ----------------- | ----------------------------------- | -------------------- |
| Arbitrum One      | ethereum-mainnet-arbitrum-1         | 4949039107694359620  |
| Arbitrum Sepolia  | ethereum-testnet-sepolia-arbitrum-1 | 3478487238524512106  |
| Avalanche Mainnet | avalanche-mainnet                   | 6433500567565415381  |
| Avalanche Fuji    | avalanche-testnet-fuji              | 14767482510784806043 |
| Base Mainnet      | ethereum-mainnet-base-1             | 15971525489660198786 |
| Base Sepolia      | ethereum-testnet-sepolia-base-1     | 10344971235874465080 |
| BNB Chain Mainnet | binance_smart_chain-mainnet       | 11344663589394136015 |
| BNB Chain Testnet | binance_smart_chain-testnet       | 5142893604156789321  |
| Ethereum Mainnet  | ethereum-mainnet                    | 5009297550715157269  |
| Ethereum Sepolia  | ethereum-testnet-sepolia            | 16015286601757825753 |
| OP Mainnet        | ethereum-mainnet-optimism-1         | 3734403246176062136  |
| OP Sepolia        | ethereum-testnet-sepolia-optimism-1 | 5224473277236331295  |
| Polygon Mainnet   | polygon-mainnet                     | 4051577828743386545  |
| Polygon Amoy      | polygon-testnet-amoy                | 16281711391670634445 |

### Usage in configuration files

**In your `project.yaml` file (RPC configuration):**

```yaml
local-simulation:
  rpcs:
    - chain-name: ethereum-testnet-sepolia # String name for RPC endpoint
      url: https://your-rpc-url.com
```

**In your workflow's `config.json` file (workflow-specific settings):**

Your workflow configuration can include chain selector names as part of your custom config structure. For example:

```json
{
  "schedule": "*/30 * * * * *",
  "evms": [
    {
      "storageAddress": "0x1234...",
      "chainName": "ethereum-testnet-sepolia" // Use string name in your config
    }
  ]
}
```

**In your workflow TypeScript code:**

```typescript
// Recommended: Use getNetwork() with the string name from your config
const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: config.evms[0].chainName, // Read from your config
  isTestnet: true,
})

if (!network) {
  throw new Error(`Network not found: ${config.evms[0].chainName}`)
}

const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)
```

---

# SDK Reference: HTTP Client
Source: https://docs.chain.link/cre/reference/sdk/http-client-ts
Last Updated: 2025-12-16

This page provides a reference for making offchain HTTP requests using the `HTTPClient`. This is a "node-level" action capability, meaning it executes on each individual node in the DON.

Because it operates at the node level, all HTTP requests are wrapped in a consensus mechanism to provide a single, reliable result to your workflow. The TypeScript SDK provides two ways to use the `HTTPClient`:

- **[High-level (recommended)](#high-level-sendrequest-recommended):** Automatically wraps your request in the `runtime.runInNodeMode()` pattern with consensus aggregation.
- **[Low-level](#low-level-sendrequest):** Requires manual wrapping in a `runtime.runInNodeMode()` block for complex scenarios.

For complete step-by-step examples, see the [GET requests](/cre/guides/workflow/using-http-client/get-request) and [POST requests](/cre/guides/workflow/using-http-client/post-request) guides.

## High-level `sendRequest()` (recommended)

The high-level `sendRequest()` method is the recommended approach for making HTTP requests. It simplifies the process by automatically handling the `runtime.runInNodeMode()` pattern and consensus aggregation for you.

**Signature:**

```typescript
sendRequest<TArgs extends unknown[], TOutput>(
  runtime: Runtime<unknown>,
  fn: (sendRequester: SendRequester, ...args: TArgs) => TOutput,
  consensusAggregation: ConsensusAggregation<TOutput, true>,
  unwrapOptions?: TOutput extends PrimitiveTypes ? never : UnwrapOptions<TOutput>
): (...args: TArgs) => { result: () => TOutput }
```

**Parameters:**

- `runtime`: The top-level `Runtime` from your workflow callback.
- `fn`: A function containing your core fetching and parsing logic. It receives a `SendRequester` instance and any additional arguments you provide.
- `consensusAggregation`: The [consensus aggregation method](/cre/reference/sdk/consensus) (e.g., `consensusMedianAggregation()`, `ConsensusAggregationByFields()`).
- `unwrapOptions`: Optional. Unwrapping configuration for complex types. Not needed for primitive types or flat objects.

**Returns:**

A curried function that accepts your custom arguments and returns an object with a `.result()` method. Calling `.result()` blocks until the consensus is reached and returns the aggregated result.

**Example:**

```typescript
import { cre, consensusMedianAggregation, type Runtime, type HTTPSendRequester } from "@chainlink/cre-sdk"

interface Config {
  apiUrl: string
}

// Your fetching and parsing logic
const fetchPrice = (sendRequester: HTTPSendRequester, url: string): number => {
  const response = sendRequester.sendRequest({ url }).result()

  if (response.statusCode !== 200) {
    throw new Error(`HTTP request failed with status: ${response.statusCode}`)
  }

  const responseText = new TextDecoder().decode(response.body)
  const data = JSON.parse(responseText)

  return data.price
}

// In your workflow
const workflow = (runtime: Runtime<Config>) => {
  const httpClient = new cre.capabilities.HTTPClient()

  // Call the high-level sendRequest with your custom function
  const price = httpClient
    .sendRequest(runtime, fetchPrice, consensusMedianAggregation<number>())(runtime.config.apiUrl)
    .result()

  runtime.log(`Aggregated price: ${price}`)

  return price
}
```

### Using `SendRequester`

The `SendRequester` helper class is provided to your function by the high-level `sendRequest()` method. It provides a simplified interface for making HTTP requests within the node-level execution context.

**Method:**

```typescript
sendRequest(input: Request | RequestJson): { result: () => Response }
```

**Parameters:**

- `input`: A `Request` or `RequestJson` object defining the API call.

**Returns:**

An object with a `.result()` method that blocks until the HTTP request completes and returns the `Response`.

### Using `sendReport()`

The `SendRequester` class also provides a `sendReport()` method for submitting reports via HTTP. This is useful when you need to send a cryptographically signed report to an external API endpoint.

**Method:**

```typescript
sendReport(
  report: Report,
  fn: (reportResponse: ReportResponse) => Request | RequestJson
): { result: () => Response }
```

**Parameters:**

- `report`: A `Report` object generated by `runtime.report()`.
- `fn`: A function that converts the inner `ReportResponse` to a `Request` or `RequestJson` object.

**Returns:**

An object with a `.result()` method that blocks until the HTTP request completes and returns the `Response`.

<Aside type="note" title="Caching limitation">
  Note that caching is limited for `sendReport()` as reports may contain different sets of signatures on different
  nodes, leading to cache misses.
</Aside>

**Example:**

```typescript
import { type HTTPSendRequester, type Report } from "@chainlink/cre-sdk"

const submitReport = (sendRequester: HTTPSendRequester, report: Report): string => {
  const response = sendRequester
    .sendReport(report, (reportResponse) => ({
      url: "https://api.example.com/submit-report",
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: Buffer.from(JSON.stringify({ report: reportResponse })).toString("base64"),
    }))
    .result()

  if (response.statusCode !== 200) {
    throw new Error(`Failed to submit report: ${response.statusCode}`)
  }

  return "Report submitted successfully"
}
```

## Low-level `sendRequest()`

The low-level `sendRequest()` method requires manual wrapping in a `runtime.runInNodeMode()` block. It provides more flexibility for complex scenarios but requires more boilerplate code.

**Signature:**

```typescript
sendRequest(
  runtime: NodeRuntime<unknown>,
  input: Request | RequestJson
): { result: () => Response }
```

**Parameters:**

- `runtime`: A `NodeRuntime` instance. This is provided by the `runtime.runInNodeMode()` function.
- `input`: A `Request` or `RequestJson` object defining the API call.

**Returns:**

An object with a `.result()` method that blocks until the HTTP request completes and returns the `Response`.

**Example:**

```typescript
import { cre, consensusMedianAggregation, type Runtime, type NodeRuntime } from "@chainlink/cre-sdk"

// Low-level usage with manual node mode
const fetchPrice = (nodeRuntime: NodeRuntime<Config>): number => {
  const httpClient = new cre.capabilities.HTTPClient()

  const response = httpClient
    .sendRequest(nodeRuntime, {
      url: nodeRuntime.config.apiUrl,
      method: "GET",
    })
    .result()

  if (response.statusCode !== 200) {
    throw new Error(`HTTP request failed with status: ${response.statusCode}`)
  }

  const responseText = new TextDecoder().decode(response.body)
  const data = JSON.parse(responseText)

  return data.price
}

// In your workflow
const workflow = (runtime: Runtime<Config>) => {
  const price = runtime.runInNodeMode(fetchPrice, consensusMedianAggregation<number>())().result()

  runtime.log(`Aggregated price: ${price}`)

  return price
}
```

### Low-level `sendReport()`

The `ClientCapability` class also provides a low-level `sendReport()` method for submitting reports via HTTP when using manual node mode wrapping.

**Signature:**

```typescript
sendReport(
  runtime: NodeRuntime<unknown>,
  report: Report,
  fn: (reportResponse: ReportResponse) => Request | RequestJson
): { result: () => Response }
```

**Parameters:**

- `runtime`: A `NodeRuntime` instance. This is provided by the `runtime.runInNodeMode()` function.
- `report`: A `Report` object generated by `runtime.report()`.
- `fn`: A function that converts the inner `ReportResponse` to a `Request` or `RequestJson` object.

**Returns:**

An object with a `.result()` method that blocks until the HTTP request completes and returns the `Response`.

## Helper Functions

The SDK provides utility functions to simplify working with HTTP responses. These functions are exported from `@chainlink/cre-sdk` and can be used to check status codes, decode text, and parse JSON responses.

### `ok()`

Checks if an HTTP response indicates success (status code 200-299).

**Signature:**

```typescript
ok(response: Response): boolean
```

**Parameters:**

- `response`: The HTTP response object.

**Returns:**

`true` if the status code is in the 200-299 range, `false` otherwise.

**Example:**

```typescript
import { ok } from "@chainlink/cre-sdk"

const response = sendRequester.sendRequest({ url: apiUrl }).result()

if (!ok(response)) {
  throw new Error(`HTTP request failed with status: ${response.statusCode}`)
}
```

### `text()`

Decodes the response body as UTF-8 text and automatically trims whitespace.

**Signature:**

```typescript
text(response: Response): string
```

**Parameters:**

- `response`: The HTTP response object.

**Returns:**

The response body decoded as a UTF-8 string with leading and trailing whitespace removed.

**Example:**

```typescript
import { text } from "@chainlink/cre-sdk"

const response = sendRequester.sendRequest({ url: apiUrl }).result()
const responseText = text(response) // Automatically trimmed
```

### `json()`

Parses the response body as JSON.

**Signature:**

```typescript
json(response: Response): unknown
```

**Parameters:**

- `response`: The HTTP response object.

**Returns:**

The parsed JSON object (type `unknown`, should be cast to your expected type).

**Example:**

```typescript
import { json } from "@chainlink/cre-sdk"

interface ApiResponse {
  price: number
  timestamp: number
}

const response = sendRequester.sendRequest({ url: apiUrl }).result()
const data = json(response) as ApiResponse
```

### `getHeader()`

Retrieves a specific HTTP header value from the response (case-insensitive).

**Signature:**

```typescript
getHeader(response: Response, name: string): string | undefined
```

**Parameters:**

- `response`: The HTTP response object.
- `name`: The header name to retrieve (case-insensitive).

**Returns:**

The header value as a string, or `undefined` if the header is not found.

**Example:**

```typescript
import { getHeader } from "@chainlink/cre-sdk"

const response = sendRequester.sendRequest({ url: apiUrl }).result()

const contentType = getHeader(response, "content-type")
const rateLimit = getHeader(response, "X-Rate-Limit-Remaining")
```

**Using all helpers together:**

```typescript
import { ok, json, getHeader } from "@chainlink/cre-sdk"

const fetchPrice = (sendRequester: HTTPSendRequester, url: string): number => {
  const response = sendRequester.sendRequest({ url }).result()

  if (!ok(response)) {
    throw new Error(`HTTP request failed with status: ${response.statusCode}`)
  }

  // Check content type
  const contentType = getHeader(response, "content-type")
  if (!contentType?.includes("application/json")) {
    throw new Error("Expected JSON response")
  }

  const data = json(response) as { price: number }
  return data.price
}
```

## Associated Types

### `Request` / `RequestJson`

Defines the parameters for an outgoing HTTP request.


<Aside type="caution" title="Redirects are not supported">
  HTTP requests to URLs that return redirects (3xx status codes) will fail. Ensure the URL you provide is the final destination and does not redirect to another URL.
</Aside>

| Field           | Type                                              | Description                                                     |
| --------------- | ------------------------------------------------- | --------------------------------------------------------------- |
| `url`           | `string`                                          | The URL of the API endpoint.                                    |
| `method`        | `string` (optional)                               | The HTTP method (e.g., `"GET"`, `"POST"`). Defaults to `"GET"`. |
| `headers`       | `{ [key: string]: string }` (optional)            | Optional HTTP headers.                                          |
| `body`          | `string` (base64-encoded) (optional)              | Optional raw request body (must be base64-encoded).             |
| `timeoutMs`     | `number` (optional)                               | Optional request timeout in milliseconds.                       |
| `cacheSettings` | `CacheSettings` \| `CacheSettingsJson` (optional) | Optional caching behavior for the request.                      |

### `CacheSettings` / `CacheSettingsJson`

Defines caching behavior for the request. This is particularly useful for preventing duplicate execution of non-idempotent requests (POST, PUT, PATCH, DELETE).

| Field           | Type      | Description                                                                                                                                                                                                                     |
| --------------- | --------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `readFromCache` | `boolean` | If `true`, attempt to read a cached response for the request. When combined with a non-zero `maxAgeMs`, this enables cache reading. If `false`, the request will not read from cache but may still store a response if enabled. |
| `maxAgeMs`      | `number`  | Maximum age of a cached response in milliseconds that this workflow will accept. If `0` or not set, the request will not attempt to read from cache. Max value is 600000 ms (10 minutes).                                       |

#### Understanding `CacheSettings` behavior

When you make HTTP requests in CRE, **all nodes in the DON execute the request by default**. For read-only operations (like GET), this is fine—consensus ensures a reliable result. However, for **non-idempotent operations** (POST, PUT, PATCH, DELETE), multiple executions can cause problems:

- Creating duplicate resources (e.g., multiple user accounts)
- Triggering duplicate actions (e.g., sending multiple emails)
- Unintended side effects (e.g., incrementing counters multiple times)

**`CacheSettings` provides a solution** by enabling a shared cache across all nodes in the DON:

1. **Node 1** makes the HTTP request and stores the response in the shared cache
2. **Nodes 2, 3, etc.** check the cache first and reuse the cached response if it exists

**Important considerations:**

- **Best effort mechanism**: The caching works reliably in most scenarios, but is not guaranteed to prevent all duplicates. For example, gateway availability (network issues or deployments) can affect routing to different gateway instances.
- **Request matching**: Caching only works when all nodes construct **identical requests** (same URL, headers, and body). Ensure your workflow generates deterministic request payloads.
- **Understanding `maxAgeMs`**: This controls how stale your workflow will accept a cached response to be:
  - The cache system stores responses for up to 10 minutes (600000 ms) by default (system-wide TTL)
  - `maxAgeMs` lets your workflow specify: "I'll only use cached data if it's fresher than X milliseconds"
  - Setting `maxAgeMs` to `0` or not providing it forces a fresh fetch every time (but still stores if `readFromCache` is true)
  - For POST/PUT/PATCH/DELETE operations: Set this slightly longer than your workflow's expected execution time (e.g., `60000` for 60 seconds)
  - For GET operations where you want to reuse data: Set this to your desired cache duration

**Example with caching:**

```typescript
const bodyBytes = new TextEncoder().encode(JSON.stringify({ name: "Resource" }))
const body = Buffer.from(bodyBytes).toString("base64")

const response = sendRequester
  .sendRequest({
    url: "https://api.example.com/create-resource",
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body,
    cacheSettings: {
      readFromCache: true,
      maxAgeMs: 60000, // Accept cached responses up to 60 seconds old
    },
  })
  .result()
```

For practical examples, see the [POST request guide](/cre/guides/workflow/using-http-client/post-request).

### `Response`

The result of the HTTP call from a single node (before consensus aggregation).

| Field        | Type                              | Description                |
| ------------ | --------------------------------- | -------------------------- |
| `statusCode` | `number`                          | The HTTP status code.      |
| `headers`    | `{ [key: string]: string }`       | The HTTP response headers. |
| `body`       | `Uint8Array` \| `string` (base64) | The raw response body.     |

**Example parsing response body:**

```typescript
import { text, json } from "@chainlink/cre-sdk"

const response = sendRequester.sendRequest({ url: apiUrl }).result()

// Parse as text using helper
const responseText = text(response)

// Parse as JSON using helper
const data = json(response)

// Or manually with TextDecoder
const manualText = new TextDecoder().decode(response.body)
const manualData = JSON.parse(manualText)
```

## Usage Patterns

### Simple GET request

```typescript
import { cre, consensusMedianAggregation, ok, json, type HTTPSendRequester } from "@chainlink/cre-sdk"

const fetchData = (sendRequester: HTTPSendRequester, url: string): number => {
  const response = sendRequester.sendRequest({ url }).result()

  if (!ok(response)) {
    throw new Error(`HTTP request failed with status: ${response.statusCode}`)
  }

  const data = json(response) as { value: number }
  return data.value
}

// In your workflow
const httpClient = new cre.capabilities.HTTPClient()
const result = httpClient.sendRequest(runtime, fetchData, consensusMedianAggregation<number>())(apiUrl).result()
```

### POST request with caching

```typescript
import { cre, consensusIdenticalAggregation, ok, json, type HTTPSendRequester } from "@chainlink/cre-sdk"

const createResource = (sendRequester: HTTPSendRequester, payload: { name: string }): { id: string } => {
  // Encode the body as base64
  const bodyBytes = new TextEncoder().encode(JSON.stringify(payload))
  const body = Buffer.from(bodyBytes).toString("base64")

  const response = sendRequester
    .sendRequest({
      url: "https://api.example.com/resources",
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body,
      cacheSettings: {
        readFromCache: true,
        maxAgeMs: 60000, // 60 seconds
      },
    })
    .result()

  if (!ok(response)) {
    throw new Error(`Failed to create resource: ${response.statusCode}`)
  }

  const data = json(response) as { id: string }
  return { id: data.id }
}

// In your workflow
const httpClient = new cre.capabilities.HTTPClient()
const resource = httpClient
  .sendRequest(runtime, createResource, consensusIdenticalAggregation<{ id: string }>())({ name: "My Resource" })
  .result()
```

### Complex object aggregation

For complex objects with multiple fields, use `ConsensusAggregationByFields()`:

```typescript
import {
  cre,
  ConsensusAggregationByFields,
  median,
  identical,
  ok,
  json,
  type HTTPSendRequester,
} from "@chainlink/cre-sdk"

interface ReserveInfo {
  lastUpdated: Date
  totalReserve: number
  status: string
}

const fetchReserveInfo = (sendRequester: HTTPSendRequester, url: string): ReserveInfo => {
  const response = sendRequester.sendRequest({ url }).result()

  if (!ok(response)) {
    throw new Error(`HTTP request failed with status: ${response.statusCode}`)
  }

  const data = json(response) as { timestamp: number; reserve: number; status: string }

  return {
    lastUpdated: new Date(data.timestamp),
    totalReserve: data.reserve,
    status: data.status,
  }
}

// In your workflow
const httpClient = new cre.capabilities.HTTPClient()
const reserveInfo = httpClient
  .sendRequest(
    runtime,
    fetchReserveInfo,
    ConsensusAggregationByFields<ReserveInfo>({
      lastUpdated: median,
      totalReserve: median,
      status: identical,
    })
  )(apiUrl)
  .result()
```

---

# SDK Reference
Source: https://docs.chain.link/cre/reference/sdk/overview-ts
Last Updated: 2025-11-04

<Aside type="note" title="Required SDK Version: v1.0.1">
  The CRE CLI automatically includes version `v1.0.1` of the `@chainlink/cre-sdk` package when you initialize a new
  TypeScript workflow with `cre init`.
</Aside>

This section provides a detailed technical reference for the public interfaces of the CRE TypeScript SDK. Use this reference for quick lookups of specific functions, types, and method signatures.

## How to read this section

The SDK Reference is broken down into several pages, each corresponding to a core part of the SDK's functionality:

- **[Core SDK](/cre/reference/sdk/core)**: Covers the fundamental building blocks of any workflow, including `cre.handler`, `Runtime`, and the `.result()` pattern for promise resolution.
- **[Triggers](/cre/reference/sdk/triggers)**: Details the configuration and payload structures for all available trigger types (`Cron`, `HTTP`, `EVM Log`).
- **[EVM Client](/cre/reference/sdk/evm-client)**: Provides a reference for the `cre.capabilities.EVMClient`, the primary tool for all EVM interactions, including reads and writes.
- **[HTTP Client](/cre/reference/sdk/http-client)**: Provides a reference for the `cre.capabilities.HTTPClient`, used for making offchain API requests from individual nodes.
- **[Consensus & Aggregation](/cre/reference/sdk/consensus)**: Describes how to use aggregators like `consensusMedianAggregation` and `ConsensusAggregationByFields` with `runtime.runInNodeMode()` to process and consolidate data from multiple nodes.

## Package Structure

The TypeScript SDK is distributed as a single npm package `@chainlink/cre-sdk` that exports all necessary functionality:

```typescript
import {
  cre, // Main SDK namespace with capabilities
  Runner, // Workflow runner
  type Runtime, // Runtime interface
  type NodeRuntime, // Node-level runtime interface
  consensusMedianAggregation, // Consensus aggregators
  getNetwork, // Chain selector utilities
  bytesToHex, // Data conversion utilities
  hexToBase64,
  // ... and more
} from "@chainlink/cre-sdk"
```

## Understanding TypeScript Types

The TypeScript SDK uses Protocol Buffers for type definitions, which generates two type representations for each message:

- **`Type`** (e.g., `CallMsg`): Runtime types using `Uint8Array`, protobuf `BigInt`, and protobuf `Message` objects. These are efficient for in-memory processing.
- **`TypeJson`** (e.g., `CallMsgJson`): JSON-serializable types using `string`, `number`, and plain objects. These are used for serialization and I/O operations.

**As a developer, you always use JSON types.** The SDK automatically converts between representations when crossing WASM boundaries.

```typescript
// You write (JSON types - convenient):
evmClient.callContract(runtime, {
  call: {
    from: "0x742d35Cc6634C0532925a3b844Bc9e7595f0bEb", // string (hex)
    to: "0x123...",
    data: "0xabc...",
  },
  blockNumber: "12345", // string representation
})

// SDK handles internally (native types - efficient):
// - Converts strings to Uint8Array
// - Processes with protobuf types
// - Converts results back to JSON types for your callback
```

In this reference documentation, type tables show **JSON types only**, as these are what you'll use when writing workflows.

## Working with Contract ABIs

The TypeScript SDK integrates with <a href="https://viem.sh/" target="_blank" rel="noopener noreferrer">viem</a> for type-safe contract interactions. You define ABIs directly in TypeScript files and use viem's functions for encoding and decoding. For detailed examples and best practices, see the [Onchain Read guide](/cre/guides/workflow/using-evm-client/onchain-read).

## Runtime Environment

TypeScript workflows are compiled to WebAssembly (WASM) using Javy and QuickJS. This compilation process has implications for library compatibility and available Node.js APIs. See [TypeScript Runtime Environment](/cre/concepts/typescript-wasm-runtime) for details on the compilation pipeline, QuickJS compatibility, and how to verify library compatibility.

## Runtime Requirements

- **Bun**: >= 1.2.21
- **TypeScript**: >= 5.9
- **Dependencies**: `viem` (`^2.34.0`), `zod` (`^3.25.76`)

---

# SDK Reference: Cron Trigger
Source: https://docs.chain.link/cre/reference/sdk/triggers/cron-trigger-ts
Last Updated: 2025-11-04

The Cron Trigger fires at a specified schedule using standard cron expressions. It is ideal for workflows that need to run at regular intervals.

## Creating the trigger

```typescript
import { cre } from "@chainlink/cre-sdk"

const cron = new cre.capabilities.CronCapability()
const trigger = cron.trigger({ schedule: "0 */10 * * * *" }) // Every 10 minutes
```

## Configuration

The `trigger()` method accepts a configuration object with the following field:

| Field      | Type   | Description                                                                                                                                   |
| ---------- | ------ | --------------------------------------------------------------------------------------------------------------------------------------------- |
| `schedule` | string | A standard cron expression with 5 or 6 fields, where the optional 6th field represents seconds. **Note:** The minimum interval is 30 seconds. |

**Examples:**

```typescript
// Every 30 seconds (minimum interval)
cron.trigger({ schedule: "*/30 * * * * *" })

// Every 10 minutes
cron.trigger({ schedule: "0 */10 * * * *" })

// Every day at midnight
cron.trigger({ schedule: "0 0 * * *" })

// Every Monday at 9 AM
cron.trigger({ schedule: "0 9 * * 1" })
```

## Payload

The payload passed to your callback function contains the scheduled execution time.

| Field                    | Type        | Description                               |
| ------------------------ | ----------- | ----------------------------------------- |
| `scheduledExecutionTime` | `Timestamp` | The time the execution was scheduled for. |

The `Timestamp` has the following structure:

```typescript
{
  seconds: bigint // Seconds since Unix epoch
  nanos: number // Nanoseconds component
}
```

## Callback Function

Your callback function for cron triggers must conform to this signature:

```typescript
import { type Runtime, type CronPayload } from "@chainlink/cre-sdk"

const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): YourReturnType => {
  // Your workflow logic here
  return result
}
```

**Parameters:**

- `runtime`: The runtime object used to invoke capabilities and access configuration
- `payload`: The cron payload containing the scheduled execution time

**Example:**

```typescript
import { type Runtime, type CronPayload } from "@chainlink/cre-sdk"

type Config = {
  schedule: string
  message: string
}

const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  if (!payload.scheduledExecutionTime) {
    throw new Error("Scheduled execution time is required")
  }

  const executionTime = new Date(Number(payload.scheduledExecutionTime.seconds) * 1000)
  runtime.log(`Workflow triggered at: ${executionTime.toISOString()}`)

  return runtime.config.message
}
```

## Complete Example

```typescript
import { cre, Runner, type Runtime, type CronPayload } from "@chainlink/cre-sdk"

type Config = {
  schedule: string
}

const onCronTrigger = (runtime: Runtime<Config>, payload: CronPayload): string => {
  runtime.log("Cron workflow triggered!")
  return "Success"
}

const initWorkflow = (config: Config) => {
  const cron = new cre.capabilities.CronCapability()

  return [cre.handler(cron.trigger({ schedule: config.schedule }), onCronTrigger)]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

---

# SDK Reference: EVM Log Trigger
Source: https://docs.chain.link/cre/reference/sdk/triggers/evm-log-trigger-ts
Last Updated: 2025-11-04

The EVM Log Trigger fires when a specific log (event) is emitted by an onchain smart contract.

## Creating the trigger

```typescript
import { cre } from "@chainlink/cre-sdk"

// Create an EVMClient instance with a chain selector
const network = getNetwork({
  chainFamily: "evm",
  chainSelectorName: "ethereum-testnet-sepolia",
  isTestnet: true,
})

const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

// Basic log trigger for a contract address
const trigger = evmClient.logTrigger({
  addresses: ["0x123...abc"],
})

// With topics for event filtering
const trigger = evmClient.logTrigger({
  addresses: ["0x123...abc"],
  topics: [
    {
      values: [
        // Keccak256 hash of "Transfer(address,address,uint256)"
        "0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef",
      ],
    },
  ],
})
```

## Configuration

The `logTrigger()` method accepts a configuration object with the following fields:

| Field        | Type            | Description                                                                                                                                                                                                                                                                                                                                                                                               |
| ------------ | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `addresses`  | `string[]`      | A list of contract addresses to monitor (as hex strings, e.g., `"0x..."`). At least one address is required.                                                                                                                                                                                                                                                                                              |
| `topics`     | `TopicValues[]` | Optional. A fixed 4-element array to filter event topics. The first element contains event signatures, and the next three elements contain indexed argument values. An empty array element acts as a wildcard.                                                                                                                                                                                            |
| `confidence` | `string`        | Optional. The block confirmation level to monitor. Can be: <ul><li>**`"CONFIDENCE_LEVEL_LATEST"`**: The most recent block (fastest but least secure).</li><li>**`"CONFIDENCE_LEVEL_SAFE"` (default)**: A block unlikely to be reorged but not yet irreversible.</li><li>**`"CONFIDENCE_LEVEL_FINALIZED"`**: A block considered irreversible (safest, but requires waiting longer for finality).</li></ul> |

<Aside type="note" title="Finality details">
  For details on how each confidence level maps to specific chains and estimated wait times, see [Finality and
  Confidence Levels](/cre/concepts/finality).
</Aside>

### `TopicValues`

The `topics` array uses a special format for filtering events:

| Field    | Type       | Description                                         |
| -------- | ---------- | --------------------------------------------------- |
| `values` | `string[]` | Array of possible values for a topic (hex strings). |

**Topic array structure:**

- **`topics[0]`**: Event signatures (keccak256 hash of the event name and indexed arg types). Must have at least one value.
- **`topics[1]`**: Optional. Values for the first indexed argument. Can be empty (wildcard).
- **`topics[2]`**: Optional. Values for the second indexed argument. Can be empty (wildcard).
- **`topics[3]`**: Optional. Values for the third indexed argument. Can be empty (wildcard).

**Example:**

```typescript
const trigger = evmClient.logTrigger({
  addresses: ["0x1234567890abcdef..."],
  topics: [
    // Topic 0: Event signature (Transfer event)
    {
      values: ["0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef"],
    },
    // Topic 1: From address (indexed parameter 1)
    {
      values: ["0x000000000000000000000000abcdef..."],
    },
    // Topic 2: Empty (wildcard for any "to" address)
    {
      values: [],
    },
  ],
  confidence: "CONFIDENCE_LEVEL_FINALIZED",
})
```

<Aside type="note" title="Simplified configuration">
  In the demo workflow and for simple use cases, you can omit `topics` and `confidence`. The trigger will fire for any
  event from the specified addresses using the default "SAFE" confirmation level.
</Aside>

## Payload

The payload passed to your callback function is an `EVMLog` object containing the log data.

| Field         | Type           | Description                                                    |
| ------------- | -------------- | -------------------------------------------------------------- |
| `address`     | `Uint8Array`   | Address of the contract that emitted the log (20 bytes).       |
| `topics`      | `Uint8Array[]` | Indexed log fields, including event signature (32 bytes each). |
| `data`        | `Uint8Array`   | ABI-encoded non-indexed log data.                              |
| `txHash`      | `Uint8Array`   | Hash of the transaction (32 bytes).                            |
| `blockHash`   | `Uint8Array`   | Hash of the block (32 bytes).                                  |
| `blockNumber` | `bigint`       | The block number containing the log (optional).                |
| `txIndex`     | `number`       | Index of the transaction within the block.                     |
| `index`       | `number`       | Index of the log within the block.                             |
| `eventSig`    | `Uint8Array`   | Keccak256 hash of the event signature (32 bytes).              |
| `removed`     | `boolean`      | True if the log was removed during a reorg.                    |

**Working with log data:**

```typescript
import { bytesToHex } from "@chainlink/cre-sdk"

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  // Convert addresses and hashes to hex
  const contractAddress = bytesToHex(log.address)
  const txHash = bytesToHex(log.txHash)

  // Access topics (first topic is typically the event signature)
  const eventSignature = bytesToHex(log.topics[0])
  const firstIndexedParam = bytesToHex(log.topics[1])

  runtime.log(`Event from ${contractAddress}`)
  runtime.log(`Transaction: ${txHash}`)

  return "Success"
}
```

## Callback Function

Your callback function for EVM log triggers must conform to this signature:

```typescript
import { type Runtime, type EVMLog } from "@chainlink/cre-sdk"

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): YourReturnType => {
  // Your workflow logic here
  return result
}
```

**Parameters:**

- `runtime`: The runtime object used to invoke capabilities and access configuration
- `log`: The EVM log payload containing all event data

**Example:**

```typescript
import { bytesToHex, type Runtime, type EVMLog } from "@chainlink/cre-sdk"

type Config = {
  contractAddress: string
}

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics

  if (topics.length < 3) {
    runtime.log(`Log payload does not contain enough topics: ${topics.length}`)
    throw new Error("Insufficient topics in log")
  }

  // Extract indexed parameters from topics
  // topics[0] is the event signature
  // topics[1], topics[2], etc. are indexed event parameters

  const eventSig = bytesToHex(topics[0])
  runtime.log(`Event signature: ${eventSig}`)

  // Access block information
  runtime.log(`Block number: ${log.blockNumber}`)
  runtime.log(`Transaction index: ${log.txIndex}`)

  return "Event processed successfully"
}
```

## Complete Example

```typescript
import { cre, bytesToHex, getNetwork, Runner, type Runtime, type EVMLog } from "@chainlink/cre-sdk"

type Config = {
  chainSelectorName: string
  contractAddress: string
}

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics

  if (topics.length < 2) {
    throw new Error("Log missing required topics")
  }

  runtime.log(`Processing log from ${bytesToHex(log.address)}`)
  runtime.log(`Event signature: ${bytesToHex(topics[0])}`)

  // Decode the log data based on your event ABI
  // For this example, we just log the raw data
  runtime.log(`Data length: ${log.data.length} bytes`)

  return "Log processed"
}

const initWorkflow = (config: Config) => {
  const network = getNetwork({
    chainFamily: "evm",
    chainSelectorName: config.chainSelectorName,
    isTestnet: true,
  })

  if (!network) {
    throw new Error(`Network not found: ${config.chainSelectorName}`)
  }

  const evmClient = new cre.capabilities.EVMClient(network.chainSelector.selector)

  return [
    cre.handler(
      evmClient.logTrigger({
        addresses: [config.contractAddress],
      }),
      onLogTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Decoding Log Data

For production workflows, you'll typically want to decode the log data based on the event's ABI. The TypeScript SDK uses [viem](https://viem.sh/) for ABI encoding/decoding:

```typescript
import { bytesToHex, type Runtime, type EVMLog } from "@chainlink/cre-sdk"

const onLogTrigger = (runtime: Runtime<Config>, log: EVMLog): string => {
  const topics = log.topics

  // topics[0] is the event signature
  // topics[1], topics[2], topics[3] are indexed event parameters

  // Example: Extract an address from topic 1 (last 20 bytes of 32-byte topic)
  const addressFromTopic = bytesToHex(topics[1].slice(12))
  runtime.log(`Address parameter: ${addressFromTopic}`)

  // For non-indexed parameters, you would decode log.data according to the ABI
  // The demo workflow uses viem for contract interactions and ABI handling

  return "Log decoded"
}
```

---

# SDK Reference: HTTP Trigger
Source: https://docs.chain.link/cre/reference/sdk/triggers/http-trigger-ts
Last Updated: 2025-11-04

The HTTP Trigger fires when an HTTP request is made to the workflow's designated endpoint. This allows you to start workflows from external systems.

## Creating the trigger

```typescript
import { cre } from "@chainlink/cre-sdk"

const http = new cre.capabilities.HTTPCapability()

// Basic trigger (no authorization)
const trigger = http.trigger({})

// Or with authorized keys for signature validation
const trigger = http.trigger({
  authorizedKeys: [
    {
      type: "KEY_TYPE_ECDSA_EVM",
      publicKey: "0x...",
    },
  ],
})
```

## Configuration

The `trigger()` method accepts a configuration object with the following field:

| Field            | Type              | Description                                                                                       |
| ---------------- | ----------------- | ------------------------------------------------------------------------------------------------- |
| `authorizedKeys` | `AuthorizedKey[]` | **Required for deployment.** A list of EVM addresses authorized to trigger the workflow via HTTP. |

### `AuthorizedKey`

Defines an EVM address authorized to trigger the workflow.

| Field       | Type     | Description                                                                                                |
| ----------- | -------- | ---------------------------------------------------------------------------------------------------------- |
| `type`      | `string` | The type of the key. Must be `"KEY_TYPE_ECDSA_EVM"` (currently the only supported authentication method).  |
| `publicKey` | `string` | An EVM address (e.g., `"0xb08E004bd2b5aFf1F5F950d141f449B1c05800eb"`) authorized to trigger this workflow. |

**Example:**

```typescript
const config = {
  authorizedKeys: [
    {
      type: "KEY_TYPE_ECDSA_EVM",
      publicKey: "0xb08E004bd2b5aFf1F5F950d141f449B1c05800eb",
    },
  ],
}
```


<Aside type="caution" title="Authorization required for deployment">
  When you deploy your workflow, you **must** include `authorizedKeys` in your HTTP trigger configuration. An empty configuration object `{}` is only valid for local simulation with `cre workflow simulate`—deployed workflows will reject HTTP triggers without authorization keys.
</Aside>

## Payload

The payload passed to your callback function contains the HTTP request data.

| Field   | Type                       | Description                                                                    |
| ------- | -------------------------- | ------------------------------------------------------------------------------ |
| `input` | `Uint8Array`               | The JSON input from the HTTP request body as raw bytes.                        |
| `key`   | `AuthorizedKey` (optional) | The EVM address that signed the request (matches one of the `authorizedKeys`). |

**Working with the `input` field:**

The `input` field is a `Uint8Array` containing the raw bytes of the HTTP request body. The SDK provides a `decodeJson` helper to parse it:

```typescript
import { decodeJson } from "@chainlink/cre-sdk"

// Parse as JSON (recommended)
const inputData = decodeJson(payload.input)

// Or convert to string manually
const inputString = payload.input.toString()

// Or parse manually
const inputJson = JSON.parse(payload.input.toString())
```

<Aside type="note" title="Using decodeJson">
  The `decodeJson` helper uses `TextDecoder` for proper UTF-8 decoding and is the recommended way to parse JSON from
  HTTP payloads.
</Aside>

## Callback Function

Your callback function for HTTP triggers must conform to this signature:

```typescript
import { type Runtime, type HTTPPayload } from "@chainlink/cre-sdk"

const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): YourReturnType => {
  // Your workflow logic here
  return result
}
```

**Parameters:**

- `runtime`: The runtime object used to invoke capabilities and access configuration
- `payload`: The HTTP payload containing the request input and signing key

**Example:**

```typescript
import { decodeJson, type Runtime, type HTTPPayload } from "@chainlink/cre-sdk"

type Config = {
  apiUrl: string
}

const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): string => {
  // Check if there's input data
  if (!payload.input || payload.input.length === 0) {
    runtime.log("HTTP trigger payload is empty")
    return "No input provided"
  }

  // Parse the input as JSON
  try {
    const inputData = decodeJson(payload.input)
    runtime.log(`Received HTTP trigger: ${JSON.stringify(inputData)}`)

    // Access parsed data
    runtime.log(`Processing request with key: ${inputData.key}`)
  } catch (error) {
    runtime.log("Failed to parse input as JSON")
    return "Invalid JSON input"
  }

  return "Success"
}
```

## Complete Example

```typescript
import { cre, decodeJson, Runner, type Runtime, type HTTPPayload } from "@chainlink/cre-sdk"

type Config = {
  publicKey: string
}

const onHttpTrigger = (runtime: Runtime<Config>, payload: HTTPPayload): string => {
  if (!payload.input || payload.input.length === 0) {
    return "Empty request"
  }

  const inputData = decodeJson(payload.input)
  runtime.log(`Processing HTTP request: ${JSON.stringify(inputData)}`)

  return "Request processed successfully"
}

const initWorkflow = (config: Config) => {
  const http = new cre.capabilities.HTTPCapability()

  return [
    cre.handler(
      http.trigger({
        authorizedKeys: [
          {
            type: "KEY_TYPE_ECDSA_EVM",
            publicKey: config.publicKey,
          },
        ],
      }),
      onHttpTrigger
    ),
  ]
}

export async function main() {
  const runner = await Runner.newRunner<Config>()
  await runner.run(initWorkflow)
}

main()
```

## Testing HTTP Triggers

When simulating workflows with HTTP triggers, the CLI will prompt you to provide JSON input:

```bash
$ cre workflow simulate my-workflow --target local-simulation

# Select the HTTP trigger when prompted
# Then enter your JSON input:
{"key": "value", "number": 123}
```

The input you provide will be converted to bytes and passed to your callback function as `payload.input`.

---

# SDK Reference: Triggers
Source: https://docs.chain.link/cre/reference/sdk/triggers/overview-ts
Last Updated: 2025-11-04

This section provides a reference for the built-in trigger capabilities of the CRE TypeScript SDK. Each trigger type has its own configuration, payload structure, and required callback signature.

- **[Cron Trigger](/cre/reference/sdk/triggers/cron-trigger)**: Fires at a specified schedule using standard cron expressions.
- **[HTTP Trigger](/cre/reference/sdk/triggers/http-trigger)**: Fires when an HTTP request is made to the workflow's designated endpoint.
- **[EVM Log Trigger](/cre/reference/sdk/triggers/evm-log-trigger)**: Fires when a specific log (event) is emitted by an onchain smart contract.

---

# Running a Demo Workflow
Source: https://docs.chain.link/cre/templates/running-demo-workflow-ts
Last Updated: 2025-11-04

This guide walks you through the core developer loop of CRE: initializing a project from a template and running it locally using the [simulator](/cre/guides/operations/simulating-workflows). By the end, you will have run the Custom Data Feed demo workflow and tested its two distinct behaviors: a **proactive** path where it fetches data from an API to write a result onchain, and a **reactive** path where it listens for onchain events to trigger new actions.

## What you'll do

- **Initialize a project**: Use the `cre init` command to scaffold a complete project from the Custom Data Feed template.
- **Configure your private key**: Add your funded Sepolia private key to the `.env` file.
- **Install dependencies**: Use `bun install` to set up your workflow's TypeScript dependencies.
- **Run the simulation**: Use `cre workflow simulate` to execute the end-to-end workflow and observe its output.

## 1. Prerequisites

Before you begin, ensure you have the necessary tools installed:

- **CRE CLI**: You must have the CRE CLI installed. See [Install the CLI](/cre/getting-started/cli-installation) for instructions.
- **CRE account & authentication**: You must have a CRE account and be logged in with the CLI. Run cre whoami in your terminal to verify you're logged in, or run cre login to authenticate. See [Creating Your Account](/cre/account/creating-account) and [Logging in with the CLI](/cre/account/cli-login) for instructions.
- **Bun**: You must have Bun version 1.2.21 or higher installed. Check your version with bun --version. See [Install Bun](https://bun.sh/docs/installation) for instructions.
- **Sepolia Testnet Account**: You need a private key for an account funded with Sepolia ETH. This is required because the demo workflow performs a write transaction. Go to <a href="https://faucets.chain.link" target="blank">faucets.chain.link</a> to get some Sepolia ETH.

## 2. Initialize the demo project

The `cre init` command scaffolds a new project from a template. The CLI will prompt you for configuration details during initialization.

1. **In your terminal, navigate to where you want your project created.**

2. **Run the init command:**

   ```bash
   cre init
   ```

3. **Provide the following details when prompted:**
   - **Project name**: demo (this becomes your project directory name)
   - **Language**: Select `Typescript` and press Enter.
   - **Pick a workflow template**: Select `Custom data feed: Typescript updating on-chain data periodically using offchain API data`
   - **Sepolia RPC URL**: Press Enter to use the default public RPC (`https://ethereum-sepolia-rpc.publicnode.com`), or provide your own Sepolia RPC URL.
   - **Workflow name**: custom-data-feed

**Result:** The CLI creates a new `demo` directory with all the necessary files and folders, including:

- A `custom-data-feed` subdirectory containing your workflow code
- A `contracts/abi/` directory with TypeScript ABI definitions for the demo contracts
- A pre-configured `project.yaml` with your RPC URL already set

<Aside type="note" title="RPC URL is pre-configured">
  The CLI now asks for your RPC URL during initialization and automatically adds it to `project.yaml`. You can change it
  later if needed.
</Aside>

## 3. Configure your private key

The demo workflow needs your funded Sepolia private key to sign and broadcast transactions.

**Open the `.env` file** in your project root (`demo`) and replace the placeholder private key with your actual funded Sepolia private key:

<Aside type="caution" title="Use the Raw Key">
  Your private key must be the 64-character hexadecimal string. Do **not** include the `0x` prefix.
</Aside>

<Aside type="caution" title="Never Commit .env Files">
  The `.gitignore` file included in the project already prevents `.env` files from being committed to version control.
  **Never** share your private keys or commit them to Git.
</Aside>

<Aside type="note" title="Best Practices for Security">
  Using a plaintext `.env` file is convenient for initial testing, but for better security, we recommend using a
  dedicated secrets manager. See our guide on [Managing Secrets with 1Password
  CLI](/cre/guides/workflow/secrets/managing-secrets-1password) to learn how to inject secrets securely at runtime.
</Aside>

## 4. Install dependencies

Your workflow needs to install its TypeScript dependencies. Run the `bun install` command with the `--cwd` flag to install dependencies in the workflow directory:

```bash
bun install --cwd ./demo/custom-data-feed
```

<Aside type="note" title="Why install in the workflow directory?">
  Each CRE TypeScript workflow is a standalone npm package with its own `package.json` and dependencies. This allows you
  to manage dependencies independently for each workflow in your project. The `--cwd` flag tells Bun to install
  dependencies in the specified workflow directory.
</Aside>

## 5. Run the simulation

Now you are ready to compile and run the workflow. The single `main.ts` file you are about to simulate is cleverly designed to demonstrate two distinct, powerful workflow patterns. We will run each one separately.

- **Path A: End-to-End Custom Data Feed**: Triggered by a CRON schedule, this workflow performs the full offchain to onchain data feed check and writes the result to the blockchain.
- **Path B: Reactive Event Handling**: Triggered by an onchain event log, this workflow demonstrates how to use data from one event to react and query another contract.

1. **Navigate into your project directory:**

   ```bash
   cd demo
   ```

2. **Run the `simulate` command**:

   ```bash
   cre workflow simulate custom-data-feed --broadcast --target staging-settings
   ```

   <Aside type="note" title="How configuration is discovered">
     The CLI automatically discovers your workflow configuration from the `workflow.yaml` file in the `custom-data-feed`
     directory. This file specifies paths to your workflow code (`main.ts`) and config files (`config.staging.json`,
     `config.production.json`). The template already configures these paths correctly.
   </Aside>

   <Aside type="note" title="Onchain Writes are Dry Runs by Default">
     The `--broadcast` flag is included here because this workflow performs an onchain write. By default, the `simulate`
     command performs a dry run and will not broadcast the transaction without this flag. For more details, see the `cre
       workflow simulate` [reference](/cre/reference/cli/workflow#cre-workflow-simulate).
   </Aside>

   You will first see a `Workflow compiled` message, followed by the trigger selection menu.

***

### Path A: The end-to-end Custom Data Feed workflow

This path executes the core functionality of the demo: fetching offchain reserve data and writing the result onchain. It is initiated by the CRON trigger.

#### **Running with the CRON Trigger**

1. At the prompt, select the `cron-trigger` by pressing `1` and then `Enter`.

   ```bash
   Workflow compiled

   🚀 Workflow simulation ready. Please select a trigger:
   1. cron-trigger@1.0.0 Trigger
   2. evm:ChainSelector:16015286601757825753@1.0.0 LogTrigger

   Enter your choice (1-2): 1

   2025-10-31T16:57:45Z [SIMULATION] Simulator Initialized

   2025-10-31T16:57:45Z [SIMULATION] Running trigger trigger=cron-trigger@1.0.0
   2025-10-31T16:57:45Z [USER LOG] Running CronTrigger
   2025-10-31T16:57:45Z [USER LOG] fetching por url https://api.real-time-reserves.verinumus.io/v1/chainlink/proof-of-reserves/TrueUSD
   2025-10-31T16:57:46Z [USER LOG] ReserveInfo {
     "lastUpdated": "2025-10-31T21:57:35.528Z",
     "totalReserve": 494515082.75
   }
   2025-10-31T16:57:46Z [USER LOG] TotalSupply 1000000000000000000000000
   2025-10-31T16:57:46Z [USER LOG] TotalReserveScaled 494515082750000009035382784
   2025-10-31T16:57:46Z [USER LOG] NativeTokenBalance 0
   2025-10-31T16:57:46Z [USER LOG] Updating reserves totalSupply 1000000000000000000000000 totalReserveScaled 494515082750000009035382784
   2025-10-31T16:58:01Z [USER LOG] Write report transaction succeeded at txHash: 0x9fbbeee645704d020bef000d35de52f10ccb91d8a3793f9523a4dc5155cef109

   Workflow Simulation Result:
   "494515082.75"

   2025-10-31T16:58:01Z [SIMULATION] Execution finished signal received
   2025-10-31T16:58:01Z [SIMULATION] Skipping WorkflowEngineV2
   ```

#### **Verifying the result onchain**

1. **Check the Transaction**: Copy the `txHash` from the logs in your terminal and paste it into the search bar on <a href="https://sepolia.etherscan.io/" target="_blank" rel="noopener noreferrer">Sepolia Etherscan</a>. You will see the full details of the transaction your workflow submitted.

   <Aside type="note" title="What are you seeing on a blockchain explorer?">
     You'll notice the transaction's `to` address is not the ReserveManager contract you intended to call. Instead,
     it's to the Chainlink **Forwarder** contract. Your workflow sends the transaction to the Forwarder, which verifies
     the cryptographic signatures and then delivers your data to the ReserveManager contract by calling its
     `onReport()` function.

     This is a core security pattern in CRE that ensures only verified, consensus-approved data reaches your smart
     contracts. To learn more, see the [Onchain Write
     guide](/cre/guides/workflow/using-evm-client/onchain-write/overview).
   </Aside>

2. **Check the Contract State**: While your transaction went to the Forwarder, the underlying ReserveManager contract's state was still updated. You can verify this change directly on Etherscan in two ways:

   **Option A: Read the contract state**

   - Navigate to the ReserveManager contract address used in the demo: <a href="https://sepolia.etherscan.io/address/0x073671aE6EAa2468c203fDE3a79dEe0836adF032" target="_blank" rel="noopener noreferrer">`0x073671aE6EAa2468c203fDE3a79dEe0836adF032`</a>.
   - Go to the `Read Contract` tab.
   - Check the values for `lastTotalMinted` and `lastTotalReserve`. They should now reflect the data your workflow just wrote to the chain.

   **Option B: Check the transaction events**

   - Go to your transaction on Etherscan (using the `txHash` from your logs).
   - Click on the `Logs` tab.
   - You'll see events emitted during the transaction, including the event from the ReserveManager contract confirming the data update.

This completes the end-to-end loop: triggering a workflow, fetching data, and verifiably writing the result to a public blockchain.

***

### Path B: The reactive event handler

This path demonstrates a more advanced, reactive pattern. It uses an onchain event (a log) as a trigger, inspects the data within that event, and uses that data to make an onchain read call. This path does not write any data onchain.

#### **Running with the log trigger**

1. From the trigger menu, select the EVM Log Trigger by pressing `2` and then `Enter`.

2. When prompted, provide the following details for a real <a href="https://sepolia.etherscan.io/tx/0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e#eventlog" target="_blank" rel="noopener noreferrer">transaction on the Sepolia testnet</a> that emitted a `MessageEmitted` event:
   - **Transaction hash**: 0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e
   - **Event index**: 0

3. The simulator will find the onchain event and use its payload to run the workflow. The final log will show the message it retrieved from the `MessageEmitter` contract.

   ```bash
   Workflow compiled

   🚀 Workflow simulation ready. Please select a trigger:
   1. cron-trigger@1.0.0 Trigger
   2. evm:ChainSelector:16015286601757825753@1.0.0 LogTrigger

   Enter your choice (1-2): 2


   🔗 EVM Trigger Configuration:
   Please provide the transaction hash and event index for the EVM log event.
   Enter transaction hash (0x...): 0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e
   Enter event index (0-based): 0
   Fetching transaction receipt for transaction 0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e...
   Found log event at index 0: contract=0x1d598672486ecB50685Da5497390571Ac4E93FDc, topics=3
   Created EVM trigger log for transaction 0x420721d7d00130a03c5b525b2dbfd42550906ddb3075e8377f9bb5d1a5992f8e, event 0
   2025-10-31T17:05:17Z [SIMULATION] Simulator Initialized

   2025-10-31T17:05:17Z [SIMULATION] Running trigger trigger=evm:ChainSelector:16015286601757825753@1.0.0
   2025-10-31T17:05:17Z [USER LOG] Running LogTrigger
   2025-10-31T17:05:17Z [USER LOG] Emitter 0x6b6d462be56d0630579fdd1a1247140b5d51a8c6
   2025-10-31T17:05:18Z [USER LOG] Message retrieved from the contract this is a test message

   Workflow Simulation Result:
   "this is a test message"

   2025-10-31T17:05:18Z [SIMULATION] Execution finished signal received
   2025-10-31T17:05:18Z [SIMULATION] Skipping WorkflowEngineV2
   ```

#### **How it works: An event-driven pattern**

What you just witnessed is a powerful event-driven capability. The workflow didn't just react to an event; it used the information *inside* that event to drive its next action. Here's how it works:

1. **You provide the event's "coordinates"**: By giving the simulator a transaction hash and a log index, you point it to a specific `MessageEmitted` event on the blockchain.

2. **The workflow receives the event data**: The simulator passes the raw log data into the `onLogTrigger` callback function in `main.ts`. This log contains a list of `topics`, which are indexed fields from the Solidity event.

3. **The workflow extracts the emitter's address**: The code for `onLogTrigger` knows that for a `MessageEmitted` event, `topics[1]` holds the address of the entity that emitted the message. It extracts this address.

4. **The workflow makes a new onchain call**: This is the key step. The workflow now takes the `emitter` address it just extracted from the event and uses it as an argument to call the `getLastMessage` function on the `MessageEmitter` contract. It is effectively asking, "What was the last message from the specific emitter involved in the event that triggered me?"

5. **The workflow logs the result**: Finally, it logs the message content it received from its `getLastMessage` call and finishes.

This pattern showcases how you can build sophisticated, interconnected institutional-grade smart contracts that react to onchain activity in real-time.

## 6. Exploring the code

The `main.ts` file is a great example of how a single workflow can contain multiple, independent handlers to perform different tasks. Here is a high-level tour of the code to show how the two paths you just tested are implemented.

- **`main()` and `Runner`**: The entry point of the workflow. It creates a new `Runner` instance with your config schema using `Runner.newRunner()`, then calls `runner.run()` with the `initWorkflow` function. This is the standard pattern for initializing CRE workflows in the TypeScript SDK.

- **`initWorkflow`**: This function initializes the trigger capabilities and returns an array of two handlers: one for the cron trigger and one for the EVM log trigger. Each handler is created using `cre.handler()`, which pairs a trigger configuration with a callback function.

- **`onCronTrigger`**: The entry point for **Path A**. It's a lightweight callback that immediately delegates to the shared `doPOR` function, demonstrating how you can reuse core logic.

- **`onLogTrigger`**: This is the self-contained entry point for **Path B**. It contains its own unique logic to handle the reactive pattern: it's triggered by an event, extracts data from that event's topics, and uses that data to make a new onchain query. It does **not** call `doPOR`.

- **`doPOR`**: This is the engine for **Path A**. It contains the core business logic for the Custom Data Feed workflow, orchestrating the sequence of helper functions to fetch API data, read contract state, and finally write the result back onchain.

- **`fetchReserveInfo`, `getTotalSupply`, `fetchNativeTokenBalance`, `updateReserves`, and `getLastMessage`**: These are the helper functions that execute the specific steps for the Custom Data Feed and reactive event handling workflows. They contain the calls to the SDK capabilities (`HTTPClient`, `EVMClient`) and use viem for ABI encoding/decoding.

## 7. Key TypeScript SDK features in this demo

This demo showcases several important patterns and features of the TypeScript SDK:

- **Runner Pattern**: The workflow uses the `Runner.newRunner()` pattern to initialize the workflow with a config schema and run it.
- **Zod Schema Validation**: The workflow uses Zod to define and validate the configuration schema, ensuring type safety at runtime.
- **Multiple Trigger Handlers**: A single workflow can register multiple handlers for different trigger types using `cre.handler()`.
- **Viem Integration**: All EVM interactions use viem's `encodeFunctionData` and `decodeFunctionResult` for type-safe contract calls.
- **Manual ABI Management**: TypeScript workflows use manually defined ABI constants from the `contracts/abi/` directory.
- **Consensus Aggregation**: The HTTP capability uses `ConsensusAggregationByFields` to aggregate offchain data from multiple nodes.
- **Two-Step Write Pattern**: The workflow uses `runtime.report()` to generate a signed report, then `evmClient.writeReport()` to submit it onchain.
- **Helper Functions**: The SDK provides utilities like `getNetwork()`, `encodeCallMsg()`, `bytesToHex()`, and `hexToBase64()` for common tasks.

## Next steps

You have successfully run the Custom Data Feed demo workflow. To understand how the different pieces of the `main.ts` file work, explore these detailed guides:

- **How are the Cron and EVM Log events handled?** Learn how to use different event sources to start your workflow in the **[Using Triggers](/cre/guides/workflow/using-triggers/overview)** guides.
- **How does it fetch API data?** The demo uses the `HTTPClient` to fetch offchain reserve data. Learn more in the **[API Interactions](/cre/guides/workflow/using-http-client)** guide.
- **How does it read from or write to the blockchain?** It uses the `EVMClient` for all onchain interactions. See the [**EVM Chain Interactions**](/cre/guides/workflow/using-evm-client/overview) guides for details.
- **How does it work with contract ABIs?** The demo uses pre-defined TypeScript ABI files with viem. Learn more in the **[Onchain Read](/cre/guides/workflow/using-evm-client/onchain-read)** guide.
